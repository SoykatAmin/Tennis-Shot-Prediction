{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.data import *\n",
    "from src.models import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-18T20:23:46.148260Z",
     "iopub.status.busy": "2025-12-18T20:23:46.147924Z",
     "iopub.status.idle": "2025-12-18T20:23:46.485289Z",
     "shell.execute_reply": "2025-12-18T20:23:46.483364Z",
     "shell.execute_reply.started": "2025-12-18T20:23:46.148232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- CONFIGURAZIONE GLOBALE ---\n",
    "SEQ_LEN = 30       # Fixed sequence length\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the path\n",
    "save_path = 'tennis_shot_forecasting.pth'\n",
    "\n",
    "# Seed everything to avoid randomness\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Call it immediately\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T20:23:43.638735Z",
     "iopub.status.busy": "2025-12-18T20:23:43.638289Z",
     "iopub.status.idle": "2025-12-18T20:23:43.665844Z",
     "shell.execute_reply": "2025-12-18T20:23:43.664893Z",
     "shell.execute_reply.started": "2025-12-18T20:23:43.638700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def train_singlehead_baseline(dataset, epochs=5, batch_size=64, lr=1e-3, device='cuda'):\n",
    "    print(f\"--- STARTING SINGLE-HEAD BASELINE (Vocab: {len(dataset.unified_vocab)}) ---\")\n",
    "    \n",
    "    # 1. IDENTIFY SERVE TOKENS ONCE\n",
    "    serve_token_ids = set()\n",
    "    for key, idx in dataset.unified_vocab.items():\n",
    "        if key.lower().startswith('serve') or key.startswith('S_'):\n",
    "            serve_token_ids.add(idx)\n",
    "            \n",
    "    print(f\"Training will ignore {len(serve_token_ids)} serve tokens as targets.\")\n",
    "\n",
    "    # --- MODIFICATION: 80 / 15 / 5 SPLIT ---\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.80 * total_len)\n",
    "    val_len   = int(0.15 * total_len)\n",
    "    test_len  = total_len - train_len - val_len # Remaining 5%\n",
    "    \n",
    "    print(f\"Data Split -> Train: {train_len}, Val: {val_len}, Test: {test_len}\")\n",
    "\n",
    "    # Use a generator for strict reproducibility\n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    train_ds, val_ds, test_ds = torch.utils.data.random_split(\n",
    "        dataset, [train_len, val_len, test_len], generator=gen\n",
    "    )\n",
    "    \n",
    "    # Train on 80%, Validate (for print stats) on 15%\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = SimpleUnifiedBaseline(\n",
    "        vocab_size=len(dataset.unified_vocab),\n",
    "        context_dim=10\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            full_seq = batch['x_seq'].to(device)\n",
    "            ctx = batch['context'].to(device)\n",
    "            \n",
    "            x_in = full_seq[:, :-1] \n",
    "            y_target = full_seq[:, 1:]\n",
    "            \n",
    "            # Mask serves\n",
    "            y_target_masked = y_target.clone()\n",
    "            for s_id in serve_token_ids:\n",
    "                y_target_masked[y_target == s_id] = 0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_in, ctx)\n",
    "            \n",
    "            loss = criterion(logits.reshape(-1, len(dataset.unified_vocab)), \n",
    "                             y_target_masked.reshape(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation loop (using the 15% validation set)\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                full_seq = batch['x_seq'].to(device)\n",
    "                ctx = batch['context'].to(device)\n",
    "                x_in = full_seq[:, :-1]\n",
    "                y_target = full_seq[:, 1:]\n",
    "                \n",
    "                logits = model(x_in, ctx)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                \n",
    "                mask = (y_target != 0)\n",
    "                if mask.sum() > 0:\n",
    "                    correct += (preds[mask] == y_target[mask]).sum().item()\n",
    "                    total += mask.sum().item()\n",
    "        \n",
    "        acc = (correct / total * 100) if total > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Val Acc (15% split): {acc:.2f}%\")\n",
    "\n",
    "    # Return model AND the held-out test set\n",
    "    return model, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T20:33:24.692734Z",
     "iopub.status.busy": "2025-12-18T20:33:24.692336Z",
     "iopub.status.idle": "2025-12-18T20:34:38.697282Z",
     "shell.execute_reply": "2025-12-18T20:34:38.696268Z",
     "shell.execute_reply.started": "2025-12-18T20:33:24.692704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_path = '/kaggle/input/wta-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-w-points-2020s.csv',\n",
    "    base_path + 'charting-w-points-2010s.csv',\n",
    "    base_path + 'charting-w-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/wta-matches/charting-w-matches.csv'\n",
    "'''\n",
    "\n",
    "base_path = '/kaggle/input/atp-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-m-points-2020s.csv',\n",
    "    base_path + 'charting-m-points-2010s.csv',\n",
    "    base_path + 'charting-m-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "'''\n",
    "\n",
    "atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "wta_path = '/kaggle/input/wta-players/wta_players.csv'\n",
    "\n",
    "datasetSingle = MCPTennisDataset(point_files, matches_path, atp_path, wta_path, max_seq_len=SEQ_LEN) \n",
    "\n",
    "\n",
    "if 'datasetSingle' not in globals():\n",
    "    print(\"Please ensure 'dataset' is loaded.\")\n",
    "else:\n",
    "    baselineSingleHead, test_ds = train_singlehead_baseline(\n",
    "        datasetSingle, \n",
    "        epochs=10, \n",
    "        batch_size=512, \n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # 2. PREPARE TEST DATA\n",
    "    # Create loader specifically for the 5% test set\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Extract indices from the subset (needed for Part 2 & 3 of evaluation to look up match IDs)\n",
    "    test_indices = test_ds.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_surface_distribution(dataset):\n",
    "    print(\"--- SURFACE DISTRIBUTION ANALYSIS ---\")\n",
    "    \n",
    "    # 1. Identify Unique Matches in the Dataset\n",
    "    # sample_match_ids contains the match_id for every single rally sequence in the data\n",
    "    if not hasattr(dataset, 'sample_match_ids'):\n",
    "        print(\"Error: Dataset does not track sample_match_ids.\")\n",
    "        return\n",
    "\n",
    "    unique_match_ids = set(dataset.sample_match_ids)\n",
    "    print(f\"Total Unique Matches with valid rallies: {len(unique_match_ids):,}\")\n",
    "\n",
    "    # 2. Count Surfaces\n",
    "    # We look up the surface for each unique match using the match_meta dictionary\n",
    "    surface_counts = Counter()\n",
    "    \n",
    "    for m_id in unique_match_ids:\n",
    "        # Default to 'Hard' if metadata is missing (same logic as in your Dataset class)\n",
    "        meta = dataset.match_meta.get(m_id, {'surface': 'Hard'})\n",
    "        surface = meta.get('surface', 'Hard')\n",
    "        \n",
    "        # Normalize labels just in case\n",
    "        if 'Clay' in surface: surface = 'Clay'\n",
    "        elif 'Grass' in surface: surface = 'Grass'\n",
    "        else: surface = 'Hard'\n",
    "        \n",
    "        surface_counts[surface] += 1\n",
    "\n",
    "    # 3. Create DataFrame for Display\n",
    "    df_surf = pd.DataFrame.from_dict(surface_counts, orient='index', columns=['Count'])\n",
    "    df_surf.index.name = 'Surface'\n",
    "    df_surf = df_surf.sort_values('Count', ascending=False)\n",
    "    \n",
    "    # Add Percentage column\n",
    "    total_matches = df_surf['Count'].sum()\n",
    "    df_surf['Percentage'] = (df_surf['Count'] / total_matches * 100).round(2)\n",
    "    \n",
    "    print(\"\\n[Match Counts by Surface]\")\n",
    "    print(df_surf)\n",
    "\n",
    "    # 4. Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # Custom colors for tennis surfaces\n",
    "    colors = {'Hard': '#1f77b4', 'Clay': '#d62728', 'Grass': '#2ca02c'}\n",
    "    palette = [colors.get(s, 'grey') for s in df_surf.index]\n",
    "    \n",
    "    sns.barplot(x=df_surf.index, y=df_surf['Count'], palette=palette)\n",
    "    plt.title(f'Distribution of Surfaces (N={total_matches} Matches)')\n",
    "    plt.ylabel('Number of Matches')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add text labels\n",
    "    for i, v in enumerate(df_surf['Count']):\n",
    "        plt.text(i, v + (0.01*v), str(v), ha='center', fontweight='bold')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# --- EXECUTE ---\n",
    "if 'datasetSingle' in globals():\n",
    "    check_surface_distribution(datasetSingle)\n",
    "else:\n",
    "    print(\"Please ensure 'datasetSingle' is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate the map using the dataset attached to your model\n",
    "universal_map = get_universal_decoder_map(datasetSingle)\n",
    "\n",
    "# 2. Initialize the Adapter\n",
    "adapter = UnifiedAdapter(\n",
    "    model=baselineSingleHead, \n",
    "    device=DEVICE, \n",
    "    dataset=datasetSingle, \n",
    "    uni_map=universal_map  # <--- Pass the robust map here\n",
    ")\n",
    "\n",
    "# 3. Run Evaluation\n",
    "evaluator = TennisEvaluator(adapter, test_loader, test_indices)\n",
    "evaluator.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:30:52.252934Z",
     "iopub.status.busy": "2025-12-18T12:30:52.252648Z",
     "iopub.status.idle": "2025-12-18T12:31:57.895945Z",
     "shell.execute_reply": "2025-12-18T12:31:57.895369Z",
     "shell.execute_reply.started": "2025-12-18T12:30:52.252914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# --- 1. SHARED CONFIG & HELPERS ---\n",
    "EVAL_SHOT_VOCAB = {'<pad>': 0, 'f': 1, 'b': 2, 'r': 3, 'v': 4, 'o': 5, 's': 6, 'u': 7, 'l': 8, 'm': 9, 'z': 10}\n",
    "EVAL_DIR_VOCAB  = {'<pad>': 0, '0': 0, '1': 1, '2': 2, '3': 3}\n",
    "EVAL_DEPTH_VOCAB = {'<pad>': 0, '0': 0, '7': 1, '8': 2, '9': 3}\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_fast_decoder_map(dataset):\n",
    "    \"\"\"\n",
    "    Creates a direct lookup table: Unified_ID -> (Type_ID, Dir_ID, Depth_ID)\n",
    "    Robustly handles Serves, Shots, Lets, and Specials.\n",
    "    \"\"\"\n",
    "    uni_map = {}\n",
    "    serve_id = EVAL_SHOT_VOCAB.get('s', 0)\n",
    "    \n",
    "    for uid, key in dataset.inv_unified_vocab.items():\n",
    "        # 1. Handle Padding/Unk\n",
    "        if uid <= 1: \n",
    "            uni_map[uid] = (0,0,0)\n",
    "            continue\n",
    "            \n",
    "        parts = key.split('_')\n",
    "        \n",
    "        # 2. Handle Serves\n",
    "        if parts[0] == 'Serve':\n",
    "            uni_map[uid] = (serve_id, 0, 0)\n",
    "            \n",
    "        # 3. Handle Specials/Lets (e.g., \"LET_1\", \"SPECIAL_S\")\n",
    "        # These are not shots, so we map them to (0,0,0) or a specific ID if needed.\n",
    "        # For this evaluation, we treat them as 'No Shot' (0).\n",
    "        elif parts[0] in ['LET', 'SPECIAL']:\n",
    "            uni_map[uid] = (0, 0, 0)\n",
    "            \n",
    "        # 4. Handle Standard Shots (e.g., \"f_2_8_...\")\n",
    "        else:\n",
    "            # We expect at least Type, Dir, Depth.\n",
    "            # If the parser produced a weird key like \"f_1\" (len 2), handle gracefully.\n",
    "            if len(parts) < 3:\n",
    "                t = EVAL_SHOT_VOCAB.get(parts[0], 0)\n",
    "                d = EVAL_DIR_VOCAB.get(parts[1], 0) if len(parts) > 1 else 0\n",
    "                uni_map[uid] = (t, d, 0)\n",
    "            else:\n",
    "                uni_map[uid] = (\n",
    "                    EVAL_SHOT_VOCAB.get(parts[0], 0), \n",
    "                    EVAL_DIR_VOCAB.get(parts[1], 0), \n",
    "                    EVAL_DEPTH_VOCAB.get(parts[2], 0)\n",
    "                )\n",
    "    return uni_map\n",
    "\n",
    "def decode_unified_predictions(preds, dataset):\n",
    "    # Wrapper to use the fast map for list processing\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    types, dirs, depths = [], [], []\n",
    "    for p in preds:\n",
    "        t, d, dep = uni_map.get(p, (0,0,0))\n",
    "        types.append(t); dirs.append(d); depths.append(dep)\n",
    "    return types, dirs, depths\n",
    "\n",
    "# --- 2. THE ADAPTED EVALUATION FUNCTION ---\n",
    "def run_full_evaluation(model, dataset, loader, test_indices, \n",
    "                        live_samples=5000, \n",
    "                        length_matches=2000, \n",
    "                        freq_matches=2000, \n",
    "                        era_matches=50, \n",
    "                        speed_samples=10000):\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Starting Full Evaluation on {len(test_indices)} test samples...\")\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    # Pre-calculate Match-to-Index Map\n",
    "    match_map = {}\n",
    "    for idx in test_indices:\n",
    "        mid = dataset.sample_match_ids[idx]\n",
    "        match_map.setdefault(mid, []).append(idx)\n",
    "    unique_matches = list(match_map.keys())\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 1: OVERALL TACTICAL METRICS (Updated)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 1: OVERALL TACTICAL METRICS \\n\" + \"=\"*40)\n",
    "    all_preds_unified, all_targets_unified = [], []\n",
    "    \n",
    "    print(\"Running Evaluation on TEST SET (Unified Model)...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_seq = batch['x_seq'].to(DEVICE)\n",
    "            x_c = batch['context'].to(DEVICE)\n",
    "            y = batch['y_target'].to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x_seq, x_c)\n",
    "            \n",
    "            # Mask out padding (0) for cleaner metrics\n",
    "            mask = y.view(-1) != 0\n",
    "            all_preds_unified.extend(logits.argmax(-1).view(-1)[mask].cpu().numpy())\n",
    "            all_targets_unified.extend(y.view(-1)[mask].cpu().numpy())\n",
    "\n",
    "    print(\"Decoding predictions...\")\n",
    "    pred_t, pred_d, pred_dp = decode_unified_predictions(all_preds_unified, dataset)\n",
    "    targ_t, targ_d, targ_dp = decode_unified_predictions(all_targets_unified, dataset)\n",
    "\n",
    "    # 1. Direction Report\n",
    "    print(\"\\n=== DIRECTION REPORT (1=Right, 2=Center, 3=Left) ===\")\n",
    "    # Filter out Pad and '0' (assuming 0 is undefined/unknown direction)\n",
    "    d_labels = [k for k,v in EVAL_DIR_VOCAB.items() if v in np.unique(targ_d) and k not in ['<pad>', '0']]\n",
    "    d_indices = [EVAL_DIR_VOCAB[k] for k in d_labels]\n",
    "    print(classification_report(targ_d, pred_d, labels=d_indices, target_names=d_labels, zero_division=0))\n",
    "    \n",
    "    # 2. Depth Report\n",
    "    print(\"\\n=== DEPTH REPORT (7=Shallow, 8=Mid, 9=Deep) ===\")\n",
    "    dp_labels = [k for k,v in EVAL_DEPTH_VOCAB.items() if v in np.unique(targ_dp) and k not in ['<pad>', '0']]\n",
    "    dp_indices = [EVAL_DEPTH_VOCAB[k] for k in dp_labels]\n",
    "    print(classification_report(targ_dp, pred_dp, labels=dp_indices, target_names=dp_labels, zero_division=0))\n",
    "    \n",
    "    # 3. Shot Type Report\n",
    "    print(\"\\n=== SHOT TYPE REPORT ===\")\n",
    "    tp_labels = [k for k,v in EVAL_SHOT_VOCAB.items() if v in np.unique(targ_t) and k not in ['<pad>']]\n",
    "    tp_indices = [EVAL_SHOT_VOCAB[k] for k in tp_labels]\n",
    "    print(classification_report(targ_t, pred_t, labels=tp_indices, target_names=tp_labels, zero_division=0))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 2: DETAILED LIVE SAMPLES\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 2: LIVE SAMPLES (Showing {live_samples} Cases) \\n\" + \"=\"*40)\n",
    "    \n",
    "    # Setup for pretty printing\n",
    "    inv_dir = {v:k for k,v in EVAL_DIR_VOCAB.items()}\n",
    "    inv_typ = {v:k for k,v in EVAL_SHOT_VOCAB.items()}\n",
    "    \n",
    "    selected_indices = random.sample(test_indices, min(live_samples * 2, len(test_indices))) # Sample extra to account for skips\n",
    "    results_buffer = {3: [], 2:[], 1:[], 0:[]}\n",
    "    \n",
    "    printed_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices:\n",
    "            if printed_count >= live_samples: break\n",
    "            \n",
    "            sample = dataset[idx]\n",
    "            non_zeros = (sample['x_seq'] != 0).nonzero(as_tuple=True)[0]\n",
    "            if len(non_zeros) < 2: continue\n",
    "            \n",
    "            # Predict a random point in the sequence\n",
    "            valid_indices = non_zeros.tolist()\n",
    "            t = random.choice(valid_indices)\n",
    "            \n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(x_seq, x_c)\n",
    "            \n",
    "            # --- Build History String ---\n",
    "            start_idx = valid_indices[0]\n",
    "            history_str = \"\"\n",
    "            for i in range(start_idx, t + 1):\n",
    "                uid = sample['x_seq'][i].item()\n",
    "                typ, d, dep = uni_map.get(uid, (0,0,0))\n",
    "                z_in = inv_dir.get(d, '?')\n",
    "                t_in = inv_typ.get(typ, '?')\n",
    "                \n",
    "                if i == start_idx:\n",
    "                    history_str += f\"[Serve {z_in}] \" if t_in == 's' else f\"[{t_in}{z_in}] \"\n",
    "                else:\n",
    "                    history_str += f\"-> {t_in}{z_in} \"\n",
    "            \n",
    "            # --- Get Prediction ---\n",
    "            probs = torch.softmax(logits[0, t], dim=0)\n",
    "            pred_uid = probs.argmax().item()\n",
    "            conf = probs.max().item() * 100\n",
    "            \n",
    "            pred_t, pred_d, pred_dp = uni_map.get(pred_uid, (0,0,0))\n",
    "            \n",
    "            true_uid = sample['y_target'][t].item()\n",
    "            true_t, true_d, true_dp = uni_map.get(true_uid, (0,0,0))\n",
    "            \n",
    "            # Skip uninformative cases (pad vs pad)\n",
    "            if true_t == 0: continue\n",
    "\n",
    "            s_pred_d = inv_dir.get(pred_d, '?'); s_pred_t = inv_typ.get(pred_t, '?')\n",
    "            s_true_d = inv_dir.get(true_d, '?'); s_true_t = inv_typ.get(true_t, '?')\n",
    "            \n",
    "            check_d = \"✅\" if pred_d == true_d else \"❌\"\n",
    "            check_t = \"✅\" if pred_t == true_t else \"❌\"\n",
    "            check_dp = \"✅\" if pred_dp == true_dp else \"❌\"\n",
    "            \n",
    "            def d_lbl(x):\n",
    "                if x == 1: return \"Short\"\n",
    "                if x == 2: return \"Deep\"\n",
    "                if x == 3: return \"V.Deep\"\n",
    "                return \"N/A\"\n",
    "            \n",
    "            score = (1 if pred_d == true_d else 0) + (1 if pred_t == true_t else 0) + (1 if pred_dp == true_dp else 0)\n",
    "            \n",
    "            m_id = dataset.sample_match_ids[idx]\n",
    "            p1 = dataset.match_meta.get(m_id, {}).get('p1_name', 'Unknown')\n",
    "\n",
    "            out = []\n",
    "            out.append(f\"\\nMatch {m_id} ({p1}):\")\n",
    "            out.append(f\"  History:    {history_str}\")\n",
    "            out.append(f\"  Prediction: {s_pred_t} to {s_pred_d} ({d_lbl(pred_dp)}) | Conf: {conf:.0f}%\")\n",
    "            out.append(f\"  ACTUAL:     {s_true_t} to {s_true_d} ({d_lbl(true_dp)}) | {check_t} Type {check_d} Dir {check_dp} Dep\")\n",
    "            \n",
    "            results_buffer[score].append(\"\\n\".join(out))\n",
    "            printed_count += 1\n",
    "\n",
    "    print_flag = False\n",
    "    \n",
    "    for s in [3,2,1,0]:\n",
    "        items = results_buffer[s]\n",
    "        if items:\n",
    "            print(f\"\\n{'='*20} {s}/3 CORRECT ({len(items)} cases) {'='*20}\")\n",
    "            if print_flag:\n",
    "                for item in items: \n",
    "                    print(item)\n",
    "                \n",
    "    # ==============================================================================\n",
    "    # PART 3: GRANULAR ACCURACY VS RALLY LENGTH\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 3: GRANULAR ACCURACY VS RALLY LENGTH \\n\" + \"=\"*40)\n",
    "    \n",
    "    # 3.1 Calculate Baselines (Weighted Random Probability)\n",
    "    print(\"Calculating dataset baselines...\")\n",
    "    all_d, all_dp, all_tp = [], [], []\n",
    "    \n",
    "    for i in test_indices:\n",
    "        y_seq = dataset[i]['y_target']\n",
    "        for uid in y_seq:\n",
    "            if uid.item() <= 1: continue\n",
    "            t, d, dep = uni_map[uid.item()]\n",
    "            all_tp.append(t)\n",
    "            all_d.append(d)\n",
    "            if dep != 0: all_dp.append(dep)\n",
    "\n",
    "    def calc_baseline(data_list):\n",
    "        if not data_list: return 0.33\n",
    "        counts = Counter(data_list)\n",
    "        total = sum(counts.values())\n",
    "        return sum([(c/total)**2 for c in counts.values()])\n",
    "\n",
    "    base_d = calc_baseline(all_d)\n",
    "    base_dp = calc_baseline(all_dp)\n",
    "    base_tp = calc_baseline(all_tp)\n",
    "    \n",
    "    base_pair_avg = (base_d*base_dp + base_d*base_tp + base_tp*base_dp) / 3\n",
    "    base_whole = base_d * base_dp * base_tp\n",
    "    \n",
    "    print(f\"Baselines -> Dir: {base_d:.2f}, Depth: {base_dp:.2f}, Type: {base_tp:.2f}, Whole: {base_whole:.4f}\")\n",
    "    \n",
    "    # 3.2 Analysis Loop\n",
    "    # We sample matches to get coherent rally structures\n",
    "    test_match_ids = [dataset.sample_match_ids[i] for i in test_indices]\n",
    "    unique_matches_p3 = sorted(list(set(test_match_ids)))\n",
    "    selected_matches_p3 = random.sample(unique_matches_p3, min(length_matches, len(unique_matches_p3)))\n",
    "    selected_indices_p3 = [i for i in test_indices if dataset.sample_match_ids[i] in selected_matches_p3]\n",
    "    \n",
    "    print(f\"Analyzing {len(selected_indices_p3)} points from {len(selected_matches_p3)} matches...\")\n",
    "\n",
    "    results_p3 = []\n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices_p3:\n",
    "            sample = dataset[idx]\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "\n",
    "            logits = model(x_seq, x_c)\n",
    "            preds = logits.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            seq_len = x_seq.shape[1]\n",
    "            for t in range(seq_len):\n",
    "                if x_seq[0, t] == 0: continue\n",
    "                \n",
    "                # Calculate absolute shot number\n",
    "                history_so_far = x_seq[0, :t+1]\n",
    "                true_shot_count = (history_so_far != 0).sum().item()\n",
    "                shot_num = true_shot_count + 1\n",
    "\n",
    "                p_uid = preds[t].item(); t_uid = y[t].item()\n",
    "                if t_uid <= 1: continue \n",
    "                \n",
    "                p_t, p_d, p_dp = uni_map.get(p_uid, (0,0,0))\n",
    "                t_t, t_d, t_dp = uni_map.get(t_uid, (0,0,0))\n",
    "\n",
    "                # --- LOGIC SPLIT ---\n",
    "                # 1. ALWAYS valid tasks\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Direction', 'Type': 'Single', 'Accuracy': 1 if p_d == t_d else 0})\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Type', 'Type': 'Single', 'Accuracy': 1 if p_t == t_t else 0})\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Type', 'Type': 'Pair', 'Accuracy': 1 if (p_d==t_d and p_t==t_t) else 0})\n",
    "                \n",
    "                # 2. DEPTH-DEPENDENT tasks (Only count if target depth is NOT 0)\n",
    "                if t_dp != 0:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Depth', 'Type': 'Single', 'Accuracy': 1 if p_dp == t_dp else 0})\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Depth', 'Type': 'Pair', 'Accuracy': 1 if (p_d==t_d and p_dp==t_dp) else 0})\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Type + Depth', 'Type': 'Pair', 'Accuracy': 1 if (p_t==t_t and p_dp==t_dp) else 0})\n",
    "                \n",
    "                # 3. Whole Shot\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Whole Shot', 'Type': 'Whole', 'Accuracy': 1 if p_uid == t_uid else 0})\n",
    "\n",
    "    if results_p3:\n",
    "        df = pd.DataFrame(results_p3)\n",
    "        df = df[(df['Shot_Number'] <= 12) & (df['Shot_Number'] >= 2)]\n",
    "        \n",
    "        # --- PLOTTING ---\n",
    "        palette_single = {'Direction': '#1f77b4', 'Depth': '#d62728', 'Type': '#2ca02c'}\n",
    "        palette_pair   = {'Dir + Depth': '#9467bd', 'Dir + Type': '#17becf', 'Type + Depth': '#ff7f0e'}\n",
    "        palette_whole  = {'Whole Shot': '#000000'}\n",
    "\n",
    "        def setup_plot(title, baseline, base_label):\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.xlabel('Shot Number', fontsize=12)\n",
    "            plt.xticks(np.arange(2, 13, 1))\n",
    "            \n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "            ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.02))\n",
    "            plt.grid(True, which='major', axis='y', linestyle='-', linewidth=0.75, color='grey', alpha=0.6)\n",
    "            plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.5, color='grey', alpha=0.3)\n",
    "            plt.ylim(0.0, 1.0)\n",
    "            \n",
    "            if baseline:\n",
    "                plt.axhline(baseline, color='#FF1493', linestyle=':', alpha=0.8, linewidth=2, label=base_label)\n",
    "\n",
    "        # GRAPH 1: SINGLE\n",
    "        setup_plot('Single Task Accuracy vs. Rally Length', base_tp, f'Random Type ({base_tp:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Single'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_single, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='lower right'); plt.show()\n",
    "        \n",
    "        # GRAPH 2: PAIRWISE\n",
    "        setup_plot('Pairwise Accuracy vs. Rally Length', base_pair_avg, f'Random Pair (~{base_pair_avg:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Pair'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_pair, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "        # GRAPH 3: WHOLE SHOT\n",
    "        setup_plot('Whole Shot Accuracy vs. Rally Length', base_whole, f'Random Whole ({base_whole:.4f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Whole'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes={'Whole Shot':(2,2)}, palette=palette_whole, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 4: PLAYER FREQUENCY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 4: PLAYER FREQUENCY ({freq_matches} Matches) \\n\" + \"=\"*40)\n",
    "    chosen_matches = random.sample(unique_matches, min(freq_matches, len(unique_matches)))\n",
    "    pf_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    \n",
    "    p_counts = Counter()\n",
    "    p_stats = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in pf_indices:\n",
    "            sample = dataset[idx]\n",
    "            y = sample['y_target']\n",
    "            # --- ADAPTATION: We still access IDs for Stats, even if model ignores them ---\n",
    "            s_id, r_id = sample['x_s_id'].item(), sample['x_r_id'].item()\n",
    "            \n",
    "            # --- ADAPTATION: Model Prediction ---\n",
    "            preds = model(sample['x_seq'].unsqueeze(0).to(DEVICE), \n",
    "                          sample['context'].unsqueeze(0).to(DEVICE)).argmax(-1).squeeze(0)\n",
    "            \n",
    "            for t in range(len(y)):\n",
    "                if y[t] == 0: continue\n",
    "                hist_len = (sample['x_seq'][:t+1] != 0).sum().item()\n",
    "                actor = s_id if (hist_len + 1) % 2 != 0 else r_id\n",
    "                if actor <= 1: continue\n",
    "                \n",
    "                p_counts[actor] += 1\n",
    "                if actor not in p_stats: p_stats[actor] = {'tot': 0, 'corr': 0}\n",
    "                p_stats[actor]['tot'] += 1\n",
    "                if preds[t].item() == y[t].item(): p_stats[actor]['corr'] += 1\n",
    "\n",
    "    pf_data = [{'Freq': p_counts[a], 'Err': (1 - v['corr']/v['tot'])*100} for a, v in p_stats.items() if p_counts[a] > 10]\n",
    "    if pf_data:\n",
    "        df_pf = pd.DataFrame(pf_data)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.regplot(data=df_pf, x='Freq', y='Err', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        plt.xscale('log'); plt.title(f\"Error vs Frequency (Corr: {df_pf['Freq'].corr(df_pf['Err']):.2f})\"); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 5: ERA STABILITY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 5: ERA STABILITY ({era_matches} Matches/Era) \\n\" + \"=\"*40)\n",
    "    eras = {'Pre-2010': [], '2010-2019': [], '2020+': []}\n",
    "    for m_id in unique_matches:\n",
    "        try: y_year = int(str(m_id)[:4])\n",
    "        except: continue\n",
    "        if y_year < 2010: eras['Pre-2010'].append(m_id)\n",
    "        elif y_year < 2020: eras['2010-2019'].append(m_id)\n",
    "        else: eras['2020+'].append(m_id)\n",
    "\n",
    "    era_indices = []\n",
    "    era_labels_list = []\n",
    "    for era_name, m_list in eras.items():\n",
    "        if not m_list: continue\n",
    "        chosen = random.sample(m_list, min(era_matches, len(m_list)))\n",
    "        for m in chosen:\n",
    "            era_indices.extend(match_map[m])\n",
    "            era_labels_list.extend([era_name]*len(match_map[m]))\n",
    "            \n",
    "    era_res = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(era_indices):\n",
    "            sample = dataset[idx]\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "            # --- ADAPTATION ---\n",
    "            preds = model(sample['x_seq'].unsqueeze(0).to(DEVICE), \n",
    "                          sample['context'].unsqueeze(0).to(DEVICE)).argmax(-1).squeeze(0)\n",
    "            mask = y != 0\n",
    "            if mask.sum() > 0:\n",
    "                acc = (preds[mask] == y[mask]).float().mean().item()\n",
    "                era_res.append({'Era': era_labels_list[i], 'Whole Shot Acc': acc})\n",
    "    \n",
    "    if era_res:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(data=pd.DataFrame(era_res), x='Era', y='Whole Shot Acc', palette='viridis', order=['Pre-2010', '2010-2019', '2020+'])\n",
    "        plt.title('Accuracy by Era'); plt.ylim(0, 1); plt.show()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PART 6: RAW ERROR ANALYSIS BY SURFACE (Unified Model + Masked Depth)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" RAW ERROR ANALYSIS BY SURFACE (Unified Model) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Group Test Indices by Surface\n",
    "    surface_map = {'Clay': [], 'Hard': [], 'Grass': []}\n",
    "    for idx in test_indices:\n",
    "        # Robustly fetch surface (default to Hard if missing)\n",
    "        surf = dataset.match_meta.get(dataset.sample_match_ids[idx], {}).get('surface', 'Hard')\n",
    "        found = False\n",
    "        for k in surface_map: \n",
    "            if k in surf: \n",
    "                surface_map[k].append(idx)\n",
    "                found = True\n",
    "                break\n",
    "        if not found: surface_map['Hard'].append(idx)\n",
    "            \n",
    "    # 2. Select Samples Balanced by Surface\n",
    "    selected_indices, surface_labels = [], []\n",
    "    per_surf = speed_samples // 3 \n",
    "    \n",
    "    for s, inds in surface_map.items():\n",
    "        if not inds: continue\n",
    "        chosen = random.sample(inds, min(len(inds), per_surf))\n",
    "        selected_indices.extend(chosen)\n",
    "        surface_labels.extend([s]*len(chosen))\n",
    "        \n",
    "    # 3. Evaluation Loop\n",
    "    results = []\n",
    "    \n",
    "    # Ensure map is ready\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            sample = dataset[idx]\n",
    "            surf = surface_labels[i]\n",
    "            \n",
    "            # Prepare Inputs (Unified Model)\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c   = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            y_tgt = sample['y_target'].to(DEVICE)\n",
    "            \n",
    "            # Predict (One unified logit tensor)\n",
    "            logits = model(x_seq, x_c) \n",
    "            preds = logits.argmax(dim=-1).squeeze(0) # [SeqLen]\n",
    "            \n",
    "            seq_len = x_seq.shape[1]\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                # Get Unified IDs\n",
    "                t_uid = y_tgt[t].item()\n",
    "                p_uid = preds[t].item()\n",
    "\n",
    "                # Skip Padding or Special/Serve tokens if you want pure rally analysis\n",
    "                # (Assuming <pad>=0, <unk>=1)\n",
    "                if t_uid <= 1: continue \n",
    "                \n",
    "                # --- DECODE UNIFIED TOKENS ---\n",
    "                # Returns (Type, Dir, Depth)\n",
    "                p_t, p_d, p_dp = uni_map.get(p_uid, (0,0,0))\n",
    "                t_t, t_d, t_dp = uni_map.get(t_uid, (0,0,0))\n",
    "\n",
    "                # Skip if target decodes to type 0 (e.g. valid token but mapped to 0 like 'Let')\n",
    "                if t_t == 0: continue\n",
    "\n",
    "                # --- CALCULATE COMPONENT ERRORS ---\n",
    "                \n",
    "                # Whole Shot Error: Did we predict the EXACT same unified token?\n",
    "                # (Alternative: match on all 3 components)\n",
    "                whole_shot_miss = (p_uid != t_uid)\n",
    "\n",
    "                # Component Errors\n",
    "                type_err = 1.0 if p_t != t_t else 0.0\n",
    "                dir_err  = 1.0 if p_d != t_d else 0.0\n",
    "                \n",
    "                # --- DEPTH MASKING FIX ---\n",
    "                # Only penalize depth error if the TARGET actually has a depth (!= 0)\n",
    "                if t_dp != 0:\n",
    "                    depth_err = 1.0 if p_dp != t_dp else 0.0\n",
    "                else:\n",
    "                    depth_err = None \n",
    "\n",
    "                results.append({\n",
    "                    'Surface': surf,\n",
    "                    'Type Error': type_err,\n",
    "                    'Direction Error': dir_err,\n",
    "                    'Depth Error': depth_err,  # <--- Masked\n",
    "                    'Whole Shot Error': 1.0 if whole_shot_miss else 0.0\n",
    "                })\n",
    "                \n",
    "    # 4. Statistics & Plotting\n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print Table (Mean Error %) - Pandas ignores None in mean() automatically\n",
    "    stats = df.groupby('Surface')[['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error']].mean() * 100\n",
    "    print(\"\\n--- Mean Error Rates (%) [Depth calc only on non-zero targets] ---\")\n",
    "    print(stats.round(2))\n",
    "    \n",
    "    # Plotting\n",
    "    # Melt for seaborn (will drop Nones automatically or handle them)\n",
    "    df_melt = df.melt(id_vars=['Surface'], \n",
    "                      value_vars=['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error'], \n",
    "                      value_name='Error Rate')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Surface', y='Error Rate', hue='variable', \n",
    "                order=['Clay', 'Hard', 'Grass'], palette='viridis')\n",
    "    plt.title('Component Error Rates (Unified Model) by Surface')\n",
    "    plt.ylabel('Error Rate (0.0 - 1.0)')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. RUNNER SNIPPET (Add this to your notebook cell) ---\n",
    "if 'datasetSingle' in globals() and 'baselineSingleHead' in globals():\n",
    "    print(f\"Evaluating on the held-out 5% test set ({len(test_ds)} samples)...\")\n",
    "    \n",
    "    # Create loader\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Run!\n",
    "    run_full_evaluation(\n",
    "        model=baselineSingleHead, \n",
    "        dataset=datasetSingle, \n",
    "        loader=test_loader, \n",
    "        test_indices=test_indices,\n",
    "        live_samples=5000, \n",
    "        length_matches=2000,\n",
    "        freq_matches=2000\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: 'datasetSingle' or 'baselineSingleHead' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (RICH) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:46:03.486788Z",
     "iopub.status.busy": "2025-12-18T11:46:03.486218Z",
     "iopub.status.idle": "2025-12-18T11:46:03.504092Z",
     "shell.execute_reply": "2025-12-18T11:46:03.503434Z",
     "shell.execute_reply.started": "2025-12-18T11:46:03.486763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def train_rich_model(dataset, epochs=10, batch_size=64, lr=1e-3, device='cuda'):\n",
    "    print(\"--- STARTING RICH INPUT LSTM TRAINING ---\")\n",
    "\n",
    "    serve_token_ids = set()\n",
    "    for key, idx in dataset.unified_vocab.items():\n",
    "        if key.lower().startswith('serve') or key.startswith('S_'):\n",
    "            serve_token_ids.add(idx)\n",
    "    print(f\"Training will ignore {len(serve_token_ids)} serve tokens in targets.\")\n",
    "    \n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.80 * total_len)\n",
    "    val_len   = int(0.10 * total_len)\n",
    "    test_len  = total_len - train_len - val_len\n",
    "\n",
    "    print(f\"Total Samples: {total_len}\")\n",
    "    print(f\"Splits -> Train: {train_len}, Val: {val_len}, Test: {test_len}\")\n",
    "\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        dataset, \n",
    "        [train_len, val_len, test_len],\n",
    "        generator=torch.Generator().manual_seed(42) # Seed for reproducibility\n",
    "    )\n",
    "\n",
    "    #  Create DataLoaders\n",
    "    # num_workers=0 is safer for Windows/Debugging. Increase for speed on Linux.\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 2. Initialize Model\n",
    "    model = RichInputLSTM(\n",
    "        unified_vocab_size=len(dataset.unified_vocab),\n",
    "        num_players=len(dataset.player_vocab),\n",
    "        type_vocab_size=len(dataset.type_vocab),\n",
    "        dir_vocab_size=len(dataset.dir_vocab),\n",
    "        depth_vocab_size=len(dataset.depth_vocab),\n",
    "        context_dim=10\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_t  = batch['x_type'].to(device)\n",
    "            x_d  = batch['x_dir'].to(device)\n",
    "            x_dp = batch['x_depth'].to(device)\n",
    "            s_id = batch['x_s_id'].to(device)\n",
    "            r_id = batch['x_r_id'].to(device)\n",
    "            ctx  = batch['context'].to(device)\n",
    "            y    = batch['y_target'].to(device)\n",
    "            \n",
    "            # --- FIX: MASK TARGETS ---\n",
    "            y_masked = y.clone()\n",
    "            for s_token in serve_token_ids:\n",
    "                y_masked[y == s_token] = 0\n",
    "            # -------------------------\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(x_t, x_d, x_dp, s_id, r_id, ctx)\n",
    "            \n",
    "            # Calculate loss against masked targets\n",
    "            loss = criterion(logits.view(-1, len(dataset.unified_vocab)), y_masked.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_t  = batch['x_type'].to(device)\n",
    "                x_d  = batch['x_dir'].to(device)\n",
    "                x_dp = batch['x_depth'].to(device)\n",
    "                s_id = batch['x_s_id'].to(device)\n",
    "                r_id = batch['x_r_id'].to(device)\n",
    "                ctx  = batch['context'].to(device)\n",
    "                y    = batch['y_target'].to(device)\n",
    "                \n",
    "                # Mask validation targets too for fair accuracy\n",
    "                y_val_masked = y.clone()\n",
    "                for s_token in serve_token_ids:\n",
    "                    y_val_masked[y == s_token] = 0\n",
    "                \n",
    "                logits = model(x_t, x_d, x_dp, s_id, r_id, ctx)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                \n",
    "                mask = (y_val_masked != 0)\n",
    "                if mask.sum() > 0:\n",
    "                    correct += (preds[mask] == y_val_masked[mask]).sum().item()\n",
    "                    total += mask.sum().item()\n",
    "        \n",
    "        acc = (correct / total * 100) if total > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Val Acc (Non-Serve): {acc:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-17T21:10:41.700107Z",
     "iopub.status.idle": "2025-12-17T21:10:41.700307Z",
     "shell.execute_reply": "2025-12-17T21:10:41.700217Z",
     "shell.execute_reply.started": "2025-12-17T21:10:41.700209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/atp-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-m-points-2020s.csv',\n",
    "    base_path + 'charting-m-points-2010s.csv',\n",
    "    base_path + 'charting-m-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "\n",
    "atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "wta_path = '/kaggle/input/wta-players/wta_players.csv'\n",
    "\n",
    "datasetEnhanced = EnhancedTennisDataset(\n",
    "        points_paths_list=point_files,\n",
    "        matches_path=matches_path,\n",
    "        atp_path=atp_path,\n",
    "        wta_path=wta_path,\n",
    "        max_seq_len=SEQ_LEN  # Length of rally history to look at\n",
    "    )\n",
    "\n",
    "if 'datasetEnhanced' not in globals():\n",
    "    print(\"Please ensure 'dataset' is loaded.\")\n",
    "else:\n",
    "    richLSTM = train_rich_model(datasetEnhanced, epochs=15, batch_size=512, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:53:55.129359Z",
     "iopub.status.busy": "2025-12-18T11:53:55.128855Z",
     "iopub.status.idle": "2025-12-18T11:56:36.843189Z",
     "shell.execute_reply": "2025-12-18T11:56:36.842639Z",
     "shell.execute_reply.started": "2025-12-18T11:53:55.129329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/kaggle/input/richlstm/transformers/default/1/rich_best.pt\"\n",
    "\n",
    "base_path = '/kaggle/input/atp-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-m-points-2020s.csv',\n",
    "    base_path + 'charting-m-points-2010s.csv',\n",
    "    base_path + 'charting-m-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "\n",
    "atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "wta_path = '/kaggle/input/wta-players/wta_players.csv'\n",
    "\n",
    "def align_and_load_rich_lstm(checkpoint_path, dataset, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    1. Loads checkpoint.\n",
    "    2. Maps the dataset's existing tensors (targets) to match the checkpoint's vocabulary.\n",
    "    3. Loads the model weights.\n",
    "    \"\"\"\n",
    "    print(f\"📂 Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # --- STEP 1: VERIFY VOCABS EXIST ---\n",
    "    if \"unified_vocab\" not in checkpoint:\n",
    "        raise ValueError(\"Checkpoint must contain 'unified_vocab' to perform alignment!\")\n",
    "\n",
    "    ckpt_vocab = checkpoint[\"unified_vocab\"]\n",
    "    ds_vocab = dataset.unified_vocab # The vocab currently in the dataset\n",
    "    \n",
    "    print(f\"📊 Dataset Vocab Size: {len(ds_vocab)} | Checkpoint Vocab Size: {len(ckpt_vocab)}\")\n",
    "    \n",
    "    # --- STEP 2: BUILD TRANSLATION MAP (Old ID -> New ID) ---\n",
    "    print(\"🔄 Building translation map (Dataset IDs -> Checkpoint IDs)...\")\n",
    "    \n",
    "    # Create inverse map for checkpoint: Word -> New_ID\n",
    "    # We default to <unk> (usually 1) if a word is missing in checkpoint\n",
    "    unk_id = ckpt_vocab.get('<unk>', 1)\n",
    "    \n",
    "    # Map: Dataset_ID -> Word -> Checkpoint_ID\n",
    "    translation_map = {}\n",
    "    \n",
    "    # Iterate over the DATASET's current vocab\n",
    "    for word, ds_id in ds_vocab.items():\n",
    "        if word in ckpt_vocab:\n",
    "            translation_map[ds_id] = ckpt_vocab[word]\n",
    "        else:\n",
    "            translation_map[ds_id] = unk_id\n",
    "            \n",
    "    # Convert map to a tensor for fast lookup [Size: Max_Dataset_ID + 1]\n",
    "    max_ds_id = max(ds_vocab.values())\n",
    "    map_tensor = torch.zeros(max_ds_id + 1, dtype=torch.long)\n",
    "    for old_id, new_id in translation_map.items():\n",
    "        map_tensor[old_id] = new_id\n",
    "        \n",
    "    # --- STEP 3: TRANSLATE TENSORS ---\n",
    "    print(\"⚡ Translating dataset tensors...\")\n",
    "    \n",
    "    # Translate Targets (Crucial for Error Calculation)\n",
    "    # .apply_() is slow, using tensor indexing is fast: new_tensor = map_tensor[old_tensor]\n",
    "    dataset.y_target_tensor = map_tensor[dataset.y_target_tensor]\n",
    "    \n",
    "    # Translate Inputs (Unified Sequence) - Just to be safe, though RichLSTM uses decomposed\n",
    "    dataset.x_seq_tensor = map_tensor[dataset.x_seq_tensor]\n",
    "    \n",
    "    print(\"✅ Tensors aligned.\")\n",
    "\n",
    "    # --- STEP 4: OVERWRITE DATASET VOCABULARIES ---\n",
    "    # Now that tensors are translated, we must update the dataset's vocab definitions\n",
    "    # so the evaluation decoder uses the correct strings.\n",
    "    dataset.unified_vocab = checkpoint[\"unified_vocab\"]\n",
    "    dataset.inv_unified_vocab = {v: k for k, v in dataset.unified_vocab.items()}\n",
    "    \n",
    "    # Also sync player/other vocabs if present\n",
    "    if \"player_vocab\" in checkpoint: dataset.player_vocab = checkpoint[\"player_vocab\"]\n",
    "    if \"type_vocab\" in checkpoint: dataset.type_vocab = checkpoint[\"type_vocab\"]\n",
    "    if \"dir_vocab\" in checkpoint: dataset.dir_vocab = checkpoint[\"dir_vocab\"]\n",
    "    if \"depth_vocab\" in checkpoint: dataset.depth_vocab = checkpoint[\"depth_vocab\"]\n",
    "    \n",
    "    print(\"📦 Dataset vocabularies updated to match checkpoint.\")\n",
    "\n",
    "    # --- STEP 5: LOAD MODEL ---\n",
    "    model = RichInputLSTM(\n",
    "        unified_vocab_size=len(dataset.unified_vocab),\n",
    "        num_players=len(dataset.player_vocab),\n",
    "        type_vocab_size=len(dataset.type_vocab),\n",
    "        dir_vocab_size=len(dataset.dir_vocab),\n",
    "        depth_vocab_size=len(dataset.depth_vocab),\n",
    "        context_dim=10,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2, \n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "    model.eval()\n",
    "    print(\"🚀 Model weights loaded successfully.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:48:27.486179Z",
     "iopub.status.busy": "2025-12-18T11:48:27.485606Z",
     "iopub.status.idle": "2025-12-18T11:48:52.437634Z",
     "shell.execute_reply": "2025-12-18T11:48:52.437065Z",
     "shell.execute_reply.started": "2025-12-18T11:48:27.486154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# --- 1. SHARED CONFIG & HELPERS ---\n",
    "EVAL_SHOT_VOCAB = {'<pad>': 0, 'f': 1, 'b': 2, 'r': 3, 'v': 4, 'o': 5, 's': 6, 'u': 7, 'l': 8, 'm': 9, 'z': 10}\n",
    "EVAL_DIR_VOCAB  = {'<pad>': 0, '0': 0, '1': 1, '2': 2, '3': 3}\n",
    "EVAL_DEPTH_VOCAB = {'<pad>': 0, '0': 0, '7': 1, '8': 2, '9': 3}\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_fast_decoder_map(dataset):\n",
    "    \"\"\" Safe decoder that handles variable key lengths to prevent IndexErrors. \"\"\"\n",
    "    uni_map = {}\n",
    "    serve_id = EVAL_SHOT_VOCAB.get('s', 0)\n",
    "    \n",
    "    for uid, key in dataset.inv_unified_vocab.items():\n",
    "        if uid <= 1: \n",
    "            uni_map[uid] = (0,0,0)\n",
    "            continue\n",
    "            \n",
    "        parts = key.split('_')\n",
    "        \n",
    "        # 1. Handle Serves (Force Direction/Depth to 0)\n",
    "        if parts[0] == 'Serve':\n",
    "            uni_map[uid] = (serve_id, 0, 0)\n",
    "        \n",
    "        # 2. Handle Specials/Lets\n",
    "        elif parts[0] in ['LET', 'SPECIAL']:\n",
    "            uni_map[uid] = (0, 0, 0)\n",
    "            \n",
    "        # 3. Handle Standard Shots (Safe Access)\n",
    "        else:\n",
    "            t = EVAL_SHOT_VOCAB.get(parts[0], 0)\n",
    "            d = EVAL_DIR_VOCAB.get(parts[1], 0) if len(parts) > 1 else 0\n",
    "            dep = EVAL_DEPTH_VOCAB.get(parts[2], 0) if len(parts) > 2 else 0\n",
    "            uni_map[uid] = (t, d, dep)\n",
    "            \n",
    "    return uni_map\n",
    "\n",
    "def decode_unified_predictions(preds, dataset):\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    types, dirs, depths = [], [], []\n",
    "    for p in preds:\n",
    "        t, d, dep = uni_map.get(p, (0,0,0))\n",
    "        types.append(t); dirs.append(d); depths.append(dep)\n",
    "    return types, dirs, depths\n",
    "\n",
    "# --- 2. THE RICH-INPUT EVALUATION FUNCTION ---\n",
    "def run_full_evaluation(model, dataset, loader, test_indices, \n",
    "                        live_samples=5000, \n",
    "                        length_matches=2000, \n",
    "                        freq_matches=2000, \n",
    "                        era_matches=50, \n",
    "                        speed_samples=10000):\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Starting Full Evaluation on {len(test_indices)} test samples...\")\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    # Pre-calculate Match-to-Index Map\n",
    "    match_map = {}\n",
    "    for idx in test_indices:\n",
    "        mid = dataset.sample_match_ids[idx]\n",
    "        match_map.setdefault(mid, []).append(idx)\n",
    "    unique_matches = list(match_map.keys())\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PART 1: OVERALL TACTICAL METRICS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 1: OVERALL TACTICAL METRICS \\n\" + \"=\"*40)\n",
    "    all_preds_unified, all_targets_unified = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Unpack rich inputs\n",
    "            x_t  = batch['x_type'].to(DEVICE)\n",
    "            x_d  = batch['x_dir'].to(DEVICE)\n",
    "            x_dp = batch['x_depth'].to(DEVICE)\n",
    "            s_id = batch['x_s_id'].to(DEVICE)\n",
    "            r_id = batch['x_r_id'].to(DEVICE)\n",
    "            ctx  = batch['context'].to(DEVICE)\n",
    "            y    = batch['y_target'].to(DEVICE)\n",
    "            \n",
    "            logits = model(x_t, x_d, x_dp, s_id, r_id, ctx)\n",
    "            \n",
    "            mask = y.view(-1) != 0\n",
    "            all_preds_unified.extend(logits.argmax(-1).view(-1)[mask].cpu().numpy())\n",
    "            all_targets_unified.extend(y.view(-1)[mask].cpu().numpy())\n",
    "\n",
    "    pred_t, pred_d, pred_dp = decode_unified_predictions(all_preds_unified, dataset)\n",
    "    targ_t, targ_d, targ_dp = decode_unified_predictions(all_targets_unified, dataset)\n",
    "\n",
    "    # 1. Direction Report\n",
    "    print(\"\\n=== DIRECTION REPORT (1=Cross, 2=Center, 3=Line) ===\")\n",
    "    d_labels = [k for k,v in EVAL_DIR_VOCAB.items() if v in np.unique(targ_d) and k not in ['<pad>', '0']]\n",
    "    d_indices = [EVAL_DIR_VOCAB[k] for k in d_labels]\n",
    "    if d_indices:\n",
    "        print(classification_report(targ_d, pred_d, labels=d_indices, target_names=d_labels, zero_division=0))\n",
    "    else:\n",
    "        print(\"No valid directions found in targets.\")\n",
    "\n",
    "    # 2. Depth Report (RESTORED)\n",
    "    print(\"\\n=== DEPTH REPORT (7=Shallow, 8=Mid, 9=Deep) ===\")\n",
    "    dp_labels = [k for k,v in EVAL_DEPTH_VOCAB.items() if v in np.unique(targ_dp) and k not in ['<pad>', '0']]\n",
    "    dp_indices = [EVAL_DEPTH_VOCAB[k] for k in dp_labels]\n",
    "    print(classification_report(targ_dp, pred_dp, labels=dp_indices, target_names=dp_labels, zero_division=0))\n",
    "\n",
    "    # 3. Shot Type Report\n",
    "    print(\"\\n=== SHOT TYPE REPORT ===\")\n",
    "    tp_labels = [k for k,v in EVAL_SHOT_VOCAB.items() if v in np.unique(targ_t) and k not in ['<pad>']]\n",
    "    tp_indices = [EVAL_SHOT_VOCAB[k] for k in tp_labels]\n",
    "    print(classification_report(targ_t, pred_t, labels=tp_indices, target_names=tp_labels, zero_division=0))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 2: LIVE TEST CASES (Detailed History & Confidence)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 2: LIVE SAMPLES (Subset of {live_samples}) \\n\" + \"=\"*40)\n",
    "    \n",
    "    inv_dir = {v:k for k,v in EVAL_DIR_VOCAB.items()}\n",
    "    inv_typ = {v:k for k,v in EVAL_SHOT_VOCAB.items()}\n",
    "    \n",
    "    selected_indices = random.sample(test_indices, min(live_samples * 2, len(test_indices)))\n",
    "    results_buffer = {3: [], 2:[], 1:[], 0:[]}\n",
    "    printed_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices:\n",
    "            if printed_count >= live_samples: break\n",
    "            \n",
    "            sample = dataset[idx]\n",
    "            non_zeros = (sample['x_seq'] != 0).nonzero(as_tuple=True)[0]\n",
    "            if len(non_zeros) < 2: continue\n",
    "            \n",
    "            # Predict random point\n",
    "            valid_indices = non_zeros.tolist()\n",
    "            t = random.choice(valid_indices)\n",
    "            \n",
    "            # Unpack & Unsqueeze for single sample\n",
    "            x_t  = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_d  = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE)\n",
    "            s_id = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            r_id = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            ctx  = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            logits = model(x_t, x_d, x_dp, s_id, r_id, ctx)\n",
    "            \n",
    "            # --- Build History String ---\n",
    "            start_idx = valid_indices[0]\n",
    "            history_str = \"\"\n",
    "            for i in range(start_idx, t + 1):\n",
    "                uid = sample['x_seq'][i].item() # Use x_seq for history logic\n",
    "                typ, d, dep = uni_map.get(uid, (0,0,0))\n",
    "                z_in = inv_dir.get(d, '?')\n",
    "                t_in = inv_typ.get(typ, '?')\n",
    "                if i == start_idx:\n",
    "                    history_str += f\"[Serve {z_in}] \" if t_in == 's' else f\"[{t_in}{z_in}] \"\n",
    "                else:\n",
    "                    history_str += f\"-> {t_in}{z_in} \"\n",
    "\n",
    "            probs = torch.softmax(logits[0, t], dim=0)\n",
    "            pred_uid = probs.argmax().item()\n",
    "            conf = probs.max().item() * 100\n",
    "            \n",
    "            pred_t, pred_d, pred_dp = uni_map.get(pred_uid, (0,0,0))\n",
    "            true_uid = sample['y_target'][t].item()\n",
    "            true_t, true_d, true_dp = uni_map.get(true_uid, (0,0,0))\n",
    "            \n",
    "            if true_t == 0: continue\n",
    "\n",
    "            s_pred_d = inv_dir.get(pred_d, '?'); s_pred_t = inv_typ.get(pred_t, '?')\n",
    "            s_true_d = inv_dir.get(true_d, '?'); s_true_t = inv_typ.get(true_t, '?')\n",
    "            \n",
    "            check_d = \"✅\" if pred_d == true_d else \"❌\"\n",
    "            check_t = \"✅\" if pred_t == true_t else \"❌\"\n",
    "            check_dp = \"✅\" if pred_dp == true_dp else \"❌\"\n",
    "            \n",
    "            def d_lbl(x):\n",
    "                if x == 1: return \"Short\"\n",
    "                if x == 2: return \"Deep\"\n",
    "                if x == 3: return \"V.Deep\"\n",
    "                return \"N/A\"\n",
    "            \n",
    "            score = (1 if pred_d == true_d else 0) + (1 if pred_t == true_t else 0) + (1 if pred_dp == true_dp else 0)\n",
    "            m_id = dataset.sample_match_ids[idx]\n",
    "\n",
    "            out = []\n",
    "            out.append(f\"\\nMatch {m_id}:\")\n",
    "            out.append(f\"  History:    {history_str}\")\n",
    "            out.append(f\"  Prediction: {s_pred_t} to {s_pred_d} ({d_lbl(pred_dp)}) | Conf: {conf:.0f}%\")\n",
    "            out.append(f\"  ACTUAL:     {s_true_t} to {s_true_d} ({d_lbl(true_dp)}) | {check_t} Type {check_d} Dir {check_dp} Dep\")\n",
    "            \n",
    "            results_buffer[score].append(\"\\n\".join(out))\n",
    "            printed_count += 1\n",
    "            \n",
    "    print_flag = False\n",
    "    for s in [3, 2, 1, 0]:\n",
    "        items = results_buffer[s]\n",
    "        if items:\n",
    "            print(f\"\\n{'='*20} {s}/3 CORRECT ({len(items)} cases) {'='*20}\")\n",
    "            if print_flag:    \n",
    "                for item in items:\n",
    "                    print(item)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 3: GRANULAR ACCURACY VS RALLY LENGTH (3 Graphs)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 3: RALLY LENGTH vs ACCURACY ({length_matches} Matches) \\n\" + \"=\"*40)\n",
    "    \n",
    "    # 3.1 Calculate Baselines\n",
    "    print(\"Calculating dataset baselines...\")\n",
    "    all_d, all_dp, all_tp = [], [], []\n",
    "    for i in test_indices:\n",
    "        y_seq = dataset[i]['y_target']\n",
    "        for uid in y_seq:\n",
    "            if uid.item() <= 1: continue\n",
    "            t, d, dep = uni_map[uid.item()]\n",
    "            all_tp.append(t); all_d.append(d)\n",
    "            if dep != 0: all_dp.append(dep)\n",
    "\n",
    "    def calc_baseline(data_list):\n",
    "        if not data_list: return 0.33\n",
    "        counts = Counter(data_list)\n",
    "        total = sum(counts.values())\n",
    "        return sum([(c/total)**2 for c in counts.values()])\n",
    "\n",
    "    base_d = calc_baseline(all_d)\n",
    "    base_dp = calc_baseline(all_dp)\n",
    "    base_tp = calc_baseline(all_tp)\n",
    "    base_pair_avg = (base_d*base_dp + base_d*base_tp + base_tp*base_dp) / 3\n",
    "    base_whole = base_d * base_dp * base_tp\n",
    "    print(f\"Baselines -> Dir: {base_d:.2f}, Depth: {base_dp:.2f}, Type: {base_tp:.2f}, Whole: {base_whole:.4f}\")\n",
    "\n",
    "    # 3.2 Analysis Loop\n",
    "    chosen_matches = random.sample(unique_matches, min(length_matches, len(unique_matches)))\n",
    "    rl_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    \n",
    "    rl_results = []\n",
    "    with torch.no_grad():\n",
    "        for idx in rl_indices:\n",
    "            sample = dataset[idx]\n",
    "            x_t  = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_d  = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE)\n",
    "            s_id = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            r_id = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            ctx  = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "            \n",
    "            preds = model(x_t, x_d, x_dp, s_id, r_id, ctx).argmax(-1).squeeze(0)\n",
    "            \n",
    "            x_seq_cpu = sample['x_seq']\n",
    "            \n",
    "            for t in range(x_seq_cpu.shape[0]):\n",
    "                if x_seq_cpu[t] == 0 or y[t] <= 1: continue\n",
    "                \n",
    "                # Shot count logic\n",
    "                shot_num = (x_seq_cpu[:t+1] != 0).sum().item() + 1\n",
    "                \n",
    "                p_uid = preds[t].item(); t_uid = y[t].item()\n",
    "                pt, p_d, pdp = uni_map.get(p_uid, (0,0,0))\n",
    "                tt, td, tdp = uni_map.get(t_uid, (0,0,0))\n",
    "                \n",
    "                # Logic Split\n",
    "                rl_results.append({'Shot_Number': shot_num, 'Task': 'Direction', 'Type': 'Single', 'Acc': 1 if p_d == td else 0})\n",
    "                rl_results.append({'Shot_Number': shot_num, 'Task': 'Type', 'Type': 'Single', 'Acc': 1 if pt == tt else 0})\n",
    "                rl_results.append({'Shot_Number': shot_num, 'Task': 'Dir + Type', 'Type': 'Pair', 'Acc': 1 if (pd==td and pt==tt) else 0})\n",
    "                \n",
    "                if tdp != 0:\n",
    "                     rl_results.append({'Shot_Number': shot_num, 'Task': 'Depth', 'Type': 'Single', 'Acc': 1 if pdp == tdp else 0})\n",
    "                     rl_results.append({'Shot_Number': shot_num, 'Task': 'Dir + Depth', 'Type': 'Pair', 'Acc': 1 if (p_d==td and pdp==tdp) else 0})\n",
    "                     rl_results.append({'Shot_Number': shot_num, 'Task': 'Type + Depth', 'Type': 'Pair', 'Acc': 1 if (pt==tt and pdp==tdp) else 0})\n",
    "\n",
    "                rl_results.append({'Shot_Number': shot_num, 'Task': 'Whole Shot', 'Type': 'Whole', 'Acc': 1 if p_uid == t_uid else 0})\n",
    "\n",
    "    if rl_results:\n",
    "        df = pd.DataFrame(rl_results)\n",
    "        df = df[(df['Shot_Number'] >= 2) & (df['Shot_Number'] <= 12)]\n",
    "        \n",
    "        palette_single = {'Direction': '#1f77b4', 'Depth': '#d62728', 'Type': '#2ca02c'}\n",
    "        palette_pair   = {'Dir + Depth': '#9467bd', 'Dir + Type': '#17becf', 'Type + Depth': '#ff7f0e'}\n",
    "        palette_whole  = {'Whole Shot': '#000000'}\n",
    "\n",
    "        def setup_plot(title, baseline, base_label):\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.xlabel('Shot Number', fontsize=12)\n",
    "            plt.xticks(np.arange(2, 13, 1))\n",
    "            \n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "            ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.02))\n",
    "            plt.grid(True, which='major', axis='y', linestyle='-', linewidth=0.75, color='grey', alpha=0.6)\n",
    "            plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.5, color='grey', alpha=0.3)\n",
    "            plt.ylim(0.0, 1.0)\n",
    "            \n",
    "            if baseline:\n",
    "                plt.axhline(baseline, color='#FF1493', linestyle=':', alpha=0.8, linewidth=2, label=base_label)\n",
    "\n",
    "        # Graph 1: Single\n",
    "        setup_plot('Single Task Accuracy vs. Rally Length', base_tp, f'Random Type ({base_tp:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Single'], x='Shot_Number', y='Acc', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_single, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='lower right'); plt.show()\n",
    "        \n",
    "        # Graph 2: Pairwise\n",
    "        setup_plot('Pairwise Accuracy vs. Rally Length', base_pair_avg, f'Random Pair (~{base_pair_avg:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Pair'], x='Shot_Number', y='Acc', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_pair, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "        # Graph 3: Whole Shot\n",
    "        setup_plot('Whole Shot Accuracy vs. Rally Length', base_whole, f'Random Whole ({base_whole:.4f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Whole'], x='Shot_Number', y='Acc', hue='Task', style='Task', \n",
    "                     markers=True, dashes={'Whole Shot':(2,2)}, palette=palette_whole, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 4: PLAYER FREQUENCY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 4: PLAYER FREQUENCY ({freq_matches} Matches) \\n\" + \"=\"*40)\n",
    "    chosen_matches = random.sample(unique_matches, min(freq_matches, len(unique_matches)))\n",
    "    pf_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    \n",
    "    p_counts = Counter()\n",
    "    p_stats = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in pf_indices:\n",
    "            sample = dataset[idx]\n",
    "            x_t  = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_d  = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE)\n",
    "            s_id = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            r_id = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            ctx  = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target']\n",
    "            \n",
    "            preds = model(x_t, x_d, x_dp, s_id, r_id, ctx).argmax(-1).squeeze(0)\n",
    "            \n",
    "            s_val, r_val = sample['x_s_id'].item(), sample['x_r_id'].item()\n",
    "            x_seq_cpu = sample['x_seq']\n",
    "            \n",
    "            for t in range(len(y)):\n",
    "                if y[t] == 0: continue\n",
    "                hist_len = (x_seq_cpu[:t+1] != 0).sum().item()\n",
    "                actor = s_val if (hist_len + 1) % 2 != 0 else r_val\n",
    "                if actor <= 1: continue\n",
    "                \n",
    "                p_counts[actor] += 1\n",
    "                if actor not in p_stats: p_stats[actor] = {'tot': 0, 'corr': 0}\n",
    "                p_stats[actor]['tot'] += 1\n",
    "                if preds[t].item() == y[t].item(): p_stats[actor]['corr'] += 1\n",
    "\n",
    "    pf_data = [{'Freq': p_counts[a], 'Err': (1 - v['corr']/v['tot'])*100} for a, v in p_stats.items() if p_counts[a] > 10]\n",
    "    if pf_data:\n",
    "        df_pf = pd.DataFrame(pf_data)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.regplot(data=df_pf, x='Freq', y='Err', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        plt.xscale('log'); plt.title(f\"Error vs Frequency (Corr: {df_pf['Freq'].corr(df_pf['Err']):.2f})\"); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 5: ERA STABILITY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 5: ERA STABILITY ({era_matches} Matches/Era) \\n\" + \"=\"*40)\n",
    "    eras = {'Pre-2010': [], '2010-2019': [], '2020+': []}\n",
    "    for m_id in unique_matches:\n",
    "        try: y_year = int(str(m_id)[:4])\n",
    "        except: continue\n",
    "        if y_year < 2010: eras['Pre-2010'].append(m_id)\n",
    "        elif y_year < 2020: eras['2010-2019'].append(m_id)\n",
    "        else: eras['2020+'].append(m_id)\n",
    "\n",
    "    era_indices = []\n",
    "    era_labels_list = []\n",
    "    for era_name, m_list in eras.items():\n",
    "        if not m_list: continue\n",
    "        chosen = random.sample(m_list, min(era_matches, len(m_list)))\n",
    "        for m in chosen:\n",
    "            era_indices.extend(match_map[m])\n",
    "            era_labels_list.extend([era_name]*len(match_map[m]))\n",
    "            \n",
    "    era_res = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(era_indices):\n",
    "            sample = dataset[idx]\n",
    "            x_t  = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_d  = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE)\n",
    "            s_id = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            r_id = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            ctx  = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "\n",
    "            preds = model(x_t, x_d, x_dp, s_id, r_id, ctx).argmax(-1).squeeze(0)\n",
    "            mask = y != 0\n",
    "            if mask.sum() > 0:\n",
    "                acc = (preds[mask] == y[mask]).float().mean().item()\n",
    "                era_res.append({'Era': era_labels_list[i], 'Whole Shot Acc': acc})\n",
    "    \n",
    "    if era_res:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(data=pd.DataFrame(era_res), x='Era', y='Whole Shot Acc', palette='viridis', order=['Pre-2010', '2010-2019', '2020+'])\n",
    "        plt.title('Accuracy by Era'); plt.ylim(0, 1); plt.show()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PART 6: RAW ERROR ANALYSIS BY SURFACE (RichInputLSTM - Unified Output)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" RAW ERROR ANALYSIS BY SURFACE (RichInputLSTM) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Group Test Indices by Surface\n",
    "    surface_map = {'Clay': [], 'Hard': [], 'Grass': []}\n",
    "    for idx in test_indices:\n",
    "        surf = dataset.match_meta.get(dataset.sample_match_ids[idx], {}).get('surface', 'Hard')\n",
    "        found = False\n",
    "        for k in surface_map: \n",
    "            if k in surf: \n",
    "                surface_map[k].append(idx)\n",
    "                found = True\n",
    "                break\n",
    "        if not found: surface_map['Hard'].append(idx)\n",
    "            \n",
    "    # 2. Select Samples Balanced by Surface\n",
    "    selected_indices, surface_labels = [], []\n",
    "    per_surf = speed_samples // 3 \n",
    "    \n",
    "    for s, inds in surface_map.items():\n",
    "        if not inds: continue\n",
    "        chosen = random.sample(inds, min(len(inds), per_surf))\n",
    "        selected_indices.extend(chosen)\n",
    "        surface_labels.extend([s]*len(chosen))\n",
    "        \n",
    "    # 3. Evaluation Loop\n",
    "    # We need the decoder map to break Unified IDs back into components\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            sample = dataset[idx]\n",
    "            surf = surface_labels[i]\n",
    "            \n",
    "            # --- Inputs (Decomposed) ---\n",
    "            x_t  = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_d  = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE)\n",
    "            x_s  = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r  = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_c  = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # --- Target (Unified ID) ---\n",
    "            y_uid_gt = sample['y_target'].to(DEVICE)\n",
    "            \n",
    "            # --- Forward Pass (Unified Output) ---\n",
    "            logits = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "            pred_uid = logits.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            seq_len = x_t.shape[1]\n",
    "            for t in range(seq_len):\n",
    "                if y_uid_gt[t] == 0: continue \n",
    "                \n",
    "                # Get IDs\n",
    "                p_id = pred_uid[t].item()\n",
    "                t_id = y_uid_gt[t].item()\n",
    "                \n",
    "                # Decode components (Prediction vs Truth)\n",
    "                p_t, p_d, p_dp = uni_map.get(p_id, (0,0,0))\n",
    "                t_t, t_d, t_dp = uni_map.get(t_id, (0,0,0))\n",
    "                \n",
    "                # Whole Shot Error (Unified Mismatch)\n",
    "                whole_shot_miss = (p_id != t_id)\n",
    "\n",
    "                # --- DEPTH MASKING FIX ---\n",
    "                # Only calculate Depth Error if the target actually HAS depth\n",
    "                if t_dp != 0:\n",
    "                    depth_err = 1.0 if p_dp != t_dp else 0.0\n",
    "                else:\n",
    "                    depth_err = None \n",
    "\n",
    "                results.append({\n",
    "                    'Surface': surf,\n",
    "                    'Type Error': 1.0 if p_t != t_t else 0.0,\n",
    "                    'Direction Error': 1.0 if p_d != t_d else 0.0,\n",
    "                    'Depth Error': depth_err,  # <--- Masked\n",
    "                    'Whole Shot Error': 1.0 if whole_shot_miss else 0.0\n",
    "                })\n",
    "                \n",
    "    # 4. Statistics & Plotting\n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print Table (Mean Error %) - Pandas ignores None/NaN automatically\n",
    "    stats = df.groupby('Surface')[['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error']].mean() * 100\n",
    "    print(\"\\n--- Mean Error Rates (%) [Depth calculated only on non-zero targets] ---\")\n",
    "    print(stats.round(2))\n",
    "    \n",
    "    # Plotting\n",
    "    df_melt = df.melt(id_vars=['Surface'], \n",
    "                      value_vars=['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error'], \n",
    "                      value_name='Error Rate')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Surface', y='Error Rate', hue='variable', \n",
    "                order=['Clay', 'Hard', 'Grass'], palette='viridis')\n",
    "    plt.title('Error Rates by Component (Masked Depth) vs. Whole Shot')\n",
    "    plt.ylabel('Error Rate (0.0 - 1.0)')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# RUN THIS BLOCK BEFORE EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. RUNNER SNIPPET ---\n",
    "if 'datasetEnhanced' in globals() and 'richLSTM' in globals():\n",
    "    # 1. Init Dataset (This creates the \"Old\" IDs)\n",
    "    print(\"Initializing Dataset...\")\n",
    "    datasetEnhanced = EnhancedTennisDataset(\n",
    "        point_files, \n",
    "        matches_path, \n",
    "        atp_path, \n",
    "        wta_path, \n",
    "        max_seq_len=SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    # 2. Align Tensors & Load Model\n",
    "    richLSTM = align_and_load_rich_lstm(\n",
    "        checkpoint_path=\"/kaggle/input/richlstm/transformers/default/1/rich_best.pt\",\n",
    "        dataset=datasetEnhanced, \n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(\"Recreating validation/test split for evaluation...\")\n",
    "    seed_everything(42) \n",
    "    \n",
    "    total_len = len(datasetEnhanced)\n",
    "    train_len = int(0.80 * total_len)\n",
    "    val_len   = int(0.10 * total_len)\n",
    "    test_len  = total_len - train_len - val_len\n",
    "    \n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    _, _, test_ds = random_split(datasetEnhanced, [train_len, val_len, test_len], generator=gen)\n",
    "    \n",
    "    test_indices = test_ds.indices\n",
    "    test_loader_eval = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "    run_full_evaluation(\n",
    "        model=richLSTM, \n",
    "        dataset=datasetEnhanced, \n",
    "        loader=test_loader_eval, \n",
    "        test_indices=test_indices,\n",
    "        live_samples=5000, \n",
    "        length_matches=2000,\n",
    "        freq_matches=2000\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: 'datasetEnhanced' or 'richLSTM' not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIFIED TRANSFORMER (cristiangpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T12:33:58.297187Z",
     "iopub.status.busy": "2025-12-18T12:33:58.296913Z",
     "iopub.status.idle": "2025-12-18T12:33:58.329717Z",
     "shell.execute_reply": "2025-12-18T12:33:58.329183Z",
     "shell.execute_reply.started": "2025-12-18T12:33:58.297167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- CONFIGURAZIONE GLOBALE ---\n",
    "SEQ_LEN = 30       # Fixed sequence length\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the path\n",
    "save_path = 'tennis_shot_forecasting.pth'\n",
    "\n",
    "# Seed everything to avoid randomness\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Call it immediately\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNSAMPLED DATASET PER VEDERE COMPARISON FRA UOMINI E DONNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T20:52:36.042514Z",
     "iopub.status.busy": "2025-12-18T20:52:36.041686Z",
     "iopub.status.idle": "2025-12-18T20:52:40.844158Z",
     "shell.execute_reply": "2025-12-18T20:52:40.843321Z",
     "shell.execute_reply.started": "2025-12-18T20:52:36.042484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- CONFIGURAZIONE GLOBALE ---\n",
    "SEQ_LEN = 30        # Fixed sequence length\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the path\n",
    "save_path = 'tennis_shot_forecasting.pth'\n",
    "\n",
    "# Seed everything to avoid randomness\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Call it immediately\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T20:52:47.735581Z",
     "iopub.status.busy": "2025-12-18T20:52:47.735278Z",
     "iopub.status.idle": "2025-12-18T20:52:47.746107Z",
     "shell.execute_reply": "2025-12-18T20:52:47.745503Z",
     "shell.execute_reply.started": "2025-12-18T20:52:47.735559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T20:52:52.840091Z",
     "iopub.status.busy": "2025-12-18T20:52:52.839787Z",
     "iopub.status.idle": "2025-12-18T21:07:10.422292Z",
     "shell.execute_reply": "2025-12-18T21:07:10.421438Z",
     "shell.execute_reply.started": "2025-12-18T20:52:52.840070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "wta_path = '/kaggle/input/wta-players/wta_players.csv'\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# List all point files to merge\n",
    "base_path = '/kaggle/input/atp-points/'\n",
    "point_files = [\n",
    "    base_path + 'charting-m-points-2020s.csv',\n",
    "    base_path + 'charting-m-points-2010s.csv',\n",
    "    base_path + 'charting-m-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "\n",
    "# Standard Focal Loss (Reused for all heads)\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', ignore_index=0)\n",
    "        pt = torch.exp(-ce_loss) \n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            at = self.alpha.gather(0, targets.view(-1))\n",
    "            focal_loss = focal_loss * at\n",
    "            \n",
    "        if self.reduction == 'mean':\n",
    "            mask = targets != 0\n",
    "            if mask.sum() > 0:\n",
    "                return focal_loss[mask].mean()\n",
    "            else:\n",
    "                return torch.tensor(0.0, device=inputs.device, requires_grad=True)\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Helper function to calculate weights for a specific target list\n",
    "def get_balanced_weights(target_tensor, num_classes, power=0.3):\n",
    "    # 1. Flatten the big tensor directly (Very fast)\n",
    "    flat = target_tensor.view(-1).numpy()\n",
    "    \n",
    "    # 2. Filter padding (0)\n",
    "    valid = flat[flat != 0]\n",
    "    \n",
    "    # 3. Compute Weights\n",
    "    unique = np.unique(valid)\n",
    "    raw_weights = compute_class_weight(class_weight='balanced', classes=unique, y=valid)\n",
    "    smoothed_weights = raw_weights ** power\n",
    "    \n",
    "    weights_list = [0.0] # Index 0 is padding\n",
    "    for i in range(1, num_classes + 1):\n",
    "        if i in unique:\n",
    "            idx = np.where(unique == i)[0][0]\n",
    "            weights_list.append(float(smoothed_weights[idx]))\n",
    "        else:\n",
    "            weights_list.append(1.0)\n",
    "            \n",
    "    # Cap weights\n",
    "    MAX_WEIGHT = 15.0\n",
    "    weights_list = [min(w, MAX_WEIGHT) for w in weights_list]\n",
    "    return torch.tensor(weights_list, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Check if files exist\n",
    "if os.path.exists(matches_path):\n",
    "    # 1. Initialize Unified Dataset\n",
    "    # Make sure you have updated the MCPTennisDataset class definition \n",
    "    # to the \"Corrected\" version I gave you previously!\n",
    "    dataset = DownsampledDataset(\n",
    "        points_paths_list=point_files, \n",
    "        matches_path=matches_path, \n",
    "        atp_path=atp_path, \n",
    "        wta_path=wta_path, \n",
    "        max_seq_len=SEQ_LEN\n",
    "    )\n",
    "    \n",
    "    # --- 1. CALCULATE WEIGHTS FOR THE UNIFIED TOKENS ---\n",
    "    print(\"\\nCalculating Balanced Weights for Unified Vocab...\")\n",
    "    \n",
    "    # We now have just ONE target tensor: y_target_tensor\n",
    "    # It contains IDs like 42 (which represents \"Forehand_CrossCourt_Deep\")\n",
    "    w_unified = get_balanced_weights(dataset.y_target_tensor, len(dataset.unified_vocab))\n",
    "    print(f\"Unified Weights Shape: {w_unified.shape}\")\n",
    "        \n",
    "    # --- SPLIT BY MATCH ---\n",
    "    print(\"\\nSplitting Data (80/15/5)...\")\n",
    "    all_matches = sorted(list(set(dataset.sample_match_ids)))\n",
    "    \n",
    "    train_matches, temp_matches = train_test_split(all_matches, test_size=0.20, random_state=42)\n",
    "    val_matches, test_matches = train_test_split(temp_matches, test_size=0.25, random_state=42)\n",
    "    \n",
    "    train_set = set(train_matches)\n",
    "    val_set = set(val_matches)\n",
    "    test_set = set(test_matches)\n",
    "    \n",
    "    train_indices = [i for i, m in enumerate(dataset.sample_match_ids) if m in train_set]\n",
    "    val_indices = [i for i, m in enumerate(dataset.sample_match_ids) if m in val_set]\n",
    "    test_indices = [i for i, m in enumerate(dataset.sample_match_ids) if m in test_set]\n",
    "    \n",
    "    print(f\"Train: {len(train_indices)} | Val: {len(val_indices)} | Test: {len(test_indices)}\")\n",
    "\n",
    "    # Workers\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "    \n",
    "    train_loader = DataLoader(Subset(dataset, train_indices), batch_size=BATCH_SIZE, shuffle=True, num_workers=2, generator=g)\n",
    "    val_loader = DataLoader(Subset(dataset, val_indices), batch_size=BATCH_SIZE, shuffle=False, num_workers=2, generator=g)\n",
    "    test_loader = DataLoader(Subset(dataset, test_indices), batch_size=BATCH_SIZE, shuffle=False, num_workers=2, generator=g)\n",
    "\n",
    "    seed_everything(42)\n",
    "    \n",
    "    # 5. Initialize UNIFIED Model\n",
    "    model = UnifiedCristianGPT(\n",
    "        unified_vocab_size=len(dataset.unified_vocab),\n",
    "        num_players=len(dataset.player_vocab),\n",
    "        context_dim=10, # Remember we added Height/Pressure features\n",
    "        seq_len=SEQ_LEN,\n",
    "        embed_dim=128\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    \n",
    "    # Use aggressive learning rate as discussed\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=4e-4, steps_per_epoch=len(train_loader), \n",
    "                           epochs=EPOCHS, pct_start=0.3, div_factor=25, final_div_factor=1000)\n",
    "\n",
    "    # 6. Initialize SINGLE Loss Function\n",
    "    # We only predict one thing now: The Unified Token ID\n",
    "    criterion = FocalLoss(alpha=w_unified, gamma=2.0)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "        \n",
    "    # 7. Training Loop (Unified)\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Inputs (Updated for Unified Model signature)\n",
    "            x_seq = batch['x_seq'].to(DEVICE) # The sequence of unified tokens\n",
    "            x_c = batch['context'].to(DEVICE)\n",
    "            x_s = batch['x_s_id'].to(DEVICE)\n",
    "            x_r = batch['x_r_id'].to(DEVICE)\n",
    "            \n",
    "            # Single Target\n",
    "            y = batch['y_target'].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            \n",
    "            # Calculate Loss (Flatten batch and sequence)\n",
    "            # View(-1, vocab_size) vs View(-1)\n",
    "            loss = criterion(logits.view(-1, len(dataset.unified_vocab)), y.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_seq = batch['x_seq'].to(DEVICE)\n",
    "                x_c = batch['context'].to(DEVICE)\n",
    "                x_s = batch['x_s_id'].to(DEVICE)\n",
    "                x_r = batch['x_r_id'].to(DEVICE)\n",
    "                y = batch['y_target'].to(DEVICE)\n",
    "                \n",
    "                logits = model(x_seq, x_c, x_s, x_r)\n",
    "                \n",
    "                # Val Loss\n",
    "                l = criterion(logits.view(-1, len(dataset.unified_vocab)), y.view(-1))\n",
    "                val_loss += l.item()\n",
    "                \n",
    "                # Accuracy (Unified)\n",
    "                # We check if the predicted ID exactly matches the target ID\n",
    "                mask = y != 0\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds[mask] == y[mask]).sum().item()\n",
    "                total += mask.sum().item()\n",
    "\n",
    "        avg_train = train_loss / len(train_loader)\n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        \n",
    "        acc = (correct / total * 100) if total > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f} | Unified Acc: {acc:.2f}%\")\n",
    "        \n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            trigger_times = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"   --> New Best Model Saved! (Loss: {best_val_loss:.4f})\")\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print(\"   --> Early Stopping.\")\n",
    "                break\n",
    "else:\n",
    "    print(\"Dataset paths not found.\")\n",
    "\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model weights saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:10:13.006163Z",
     "iopub.status.busy": "2025-12-18T21:10:13.004958Z",
     "iopub.status.idle": "2025-12-18T21:10:41.236989Z",
     "shell.execute_reply": "2025-12-18T21:10:41.236248Z",
     "shell.execute_reply.started": "2025-12-18T21:10:13.006126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- 1. DEFINE STANDARD VOCABS (Quick Fix for Decoding) ---\n",
    "EVAL_SHOT_VOCAB = {'<pad>': 0, 'f': 1, 'b': 2, 'r': 3, 'v': 4, 'o': 5, 's': 6, 'u': 7, 'l': 8, 'm': 9, 'z': 10}\n",
    "EVAL_DIR_VOCAB  = {'<pad>': 0, '0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6}\n",
    "EVAL_DEPTH_VOCAB = {'<pad>': 0, '0': 0, '7': 1, '8': 2, '9': 3}\n",
    "\n",
    "# --- 2. DECODER HELPER ---\n",
    "def decode_unified_predictions(preds, dataset):\n",
    "    \"\"\" Converts list of Unified IDs -> (Type, Dir, Depth) \"\"\"\n",
    "    types, dirs, depths = [], [], []\n",
    "    id_map = {}\n",
    "    serve_type_id = EVAL_SHOT_VOCAB.get('s', 0)\n",
    "    \n",
    "    for uid, key in dataset.inv_unified_vocab.items():\n",
    "        if uid <= 1: \n",
    "            id_map[uid] = (0, 0, 0)\n",
    "            continue\n",
    "        parts = key.split('_')\n",
    "        if parts[0] == 'Serve':\n",
    "            t = serve_type_id\n",
    "            d = EVAL_DIR_VOCAB.get(parts[1], 0)\n",
    "            dep = 0\n",
    "        else:\n",
    "            t = EVAL_SHOT_VOCAB.get(parts[0], 0)\n",
    "            d = EVAL_DIR_VOCAB.get(parts[1], 0)\n",
    "            dep = EVAL_DEPTH_VOCAB.get(parts[2], 0)\n",
    "        id_map[uid] = (t, d, dep)\n",
    "        \n",
    "    for p in preds:\n",
    "        t, d, dep = id_map.get(p, (0,0,0))\n",
    "        types.append(t)\n",
    "        dirs.append(d)\n",
    "        depths.append(dep)\n",
    "    return types, dirs, depths\n",
    "\n",
    "def get_fast_decoder_map(dataset):\n",
    "    \"\"\" Returns a dict mapping UnifiedID -> (Type, Dir, Depth) for fast loops \"\"\"\n",
    "    uni_map = {}\n",
    "    serve_id = EVAL_SHOT_VOCAB.get('s', 0)\n",
    "    for uid, key in dataset.inv_unified_vocab.items():\n",
    "        if uid <= 1: \n",
    "            uni_map[uid] = (0,0,0)\n",
    "            continue\n",
    "        parts = key.split('_')\n",
    "        if parts[0] == 'Serve':\n",
    "            uni_map[uid] = (serve_id, EVAL_DIR_VOCAB.get(parts[1], 0), 0)\n",
    "        else:\n",
    "            uni_map[uid] = (EVAL_SHOT_VOCAB.get(parts[0], 0), \n",
    "                            EVAL_DIR_VOCAB.get(parts[1], 0), \n",
    "                            EVAL_DEPTH_VOCAB.get(parts[2], 0))\n",
    "    return uni_map\n",
    "\n",
    "def load_weights_into_model(model, path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠️  Weights file '{path}' not found. Using random init.\")\n",
    "        return model\n",
    "    print(f\"🔍 Loading weights from '{path}'...\")\n",
    "    try:\n",
    "        DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        checkpoint = torch.load(path, map_location=DEVICE)\n",
    "        state_dict = checkpoint['model_state_dict'] if (isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint) else checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"✅ Weights loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading weights: {e}\")\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. MAIN TACTICAL ANALYSIS\n",
    "# ==============================================================================\n",
    "def evaluation_analyze_tactics_multitask(model, loader, dataset):\n",
    "    model.eval()\n",
    "    all_preds_unified, all_targets_unified = [], []\n",
    "    \n",
    "    print(\"\\nRunning Evaluation on TEST SET (Unified Model)...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x_seq = batch['x_seq'].to(DEVICE)\n",
    "            x_c = batch['context'].to(DEVICE)\n",
    "            x_s = batch['x_s_id'].to(DEVICE)\n",
    "            x_r = batch['x_r_id'].to(DEVICE)\n",
    "            y = batch['y_target'].to(DEVICE)\n",
    "            \n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            \n",
    "            mask = y.view(-1) != 0\n",
    "            all_preds_unified.extend(logits.argmax(-1).view(-1)[mask].cpu().numpy())\n",
    "            all_targets_unified.extend(y.view(-1)[mask].cpu().numpy())\n",
    "\n",
    "    print(\"Decoding predictions...\")\n",
    "    pred_t, pred_d, pred_dp = decode_unified_predictions(all_preds_unified, dataset)\n",
    "    targ_t, targ_d, targ_dp = decode_unified_predictions(all_targets_unified, dataset)\n",
    "\n",
    "    # 1. Direction Report\n",
    "    print(\"\\n=== DIRECTION REPORT (1=Right, 2=Center, 3=Left) ===\")\n",
    "    d_labels = [k for k,v in EVAL_DIR_VOCAB.items() if v in np.unique(targ_d) and k not in ['<pad>', '0']]\n",
    "    d_indices = [EVAL_DIR_VOCAB[k] for k in d_labels]\n",
    "    print(classification_report(targ_d, pred_d, labels=d_indices, target_names=d_labels, zero_division=0))\n",
    "    \n",
    "    # 2. Depth Report\n",
    "    print(\"\\n=== DEPTH REPORT (7=Shallow, 8=Mid, 9=Deep) ===\")\n",
    "    dp_labels = [k for k,v in EVAL_DEPTH_VOCAB.items() if v in np.unique(targ_dp) and k not in ['<pad>', '0']]\n",
    "    dp_indices = [EVAL_DEPTH_VOCAB[k] for k in dp_labels]\n",
    "    print(classification_report(targ_dp, pred_dp, labels=dp_indices, target_names=dp_labels, zero_division=0))\n",
    "    \n",
    "    # 3. Shot Type Report\n",
    "    print(\"\\n=== SHOT TYPE REPORT ===\")\n",
    "    tp_labels = [k for k,v in EVAL_SHOT_VOCAB.items() if v in np.unique(targ_t) and k not in ['<pad>']]\n",
    "    tp_indices = [EVAL_SHOT_VOCAB[k] for k in tp_labels]\n",
    "    print(classification_report(targ_t, pred_t, labels=tp_indices, target_names=tp_labels, zero_division=0))\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. LIVE TEST CASES\n",
    "# ==============================================================================\n",
    "def evaluation_live_test_cases(model, dataset, test_indices, num_samples=10, print_flag=True):     \n",
    "    model.eval()\n",
    "    inv_dir = {v:k for k,v in EVAL_DIR_VOCAB.items()}\n",
    "    # Note: We rely on the indices (1,2,3), not the keys ('7','8','9')\n",
    "    inv_typ = {v:k for k,v in EVAL_SHOT_VOCAB.items()}\n",
    "    \n",
    "    print(f\"\\n--- LIVE TACTICAL EVALUATION ({num_samples} Random Test Cases) ---\")\n",
    "    # FIX 1: Shuffle ALL indices to ensure we have a large enough pool\n",
    "    candidates = list(test_indices)\n",
    "    random.shuffle(candidates)\n",
    "    \n",
    "    results_buffer = {3: [], 2:[], 1:[], 0:[]}\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    count_processed = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in candidates:\n",
    "            # FIX 2: Stop once we have processed the desired number of VALID samples\n",
    "            if count_processed >= num_samples:\n",
    "                break\n",
    "                \n",
    "            sample = dataset[idx]\n",
    "            non_zeros = (sample['x_seq'] != 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            #Skip invalid samples\n",
    "            if len(non_zeros) < 2: continue\n",
    "            \n",
    "            valid_indices = non_zeros.tolist()\n",
    "            t = random.choice(valid_indices)\n",
    "            \n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            \n",
    "            start_idx = valid_indices[0]\n",
    "            history_str = \"\"\n",
    "            for i in range(start_idx, t + 1):\n",
    "                uid = sample['x_seq'][i].item()\n",
    "                typ, d, dep = uni_map.get(uid, (0,0,0))\n",
    "                z_in = inv_dir.get(d, '?')\n",
    "                t_in = inv_typ.get(typ, '?')\n",
    "                if i == start_idx:\n",
    "                    history_str += f\"[Serve {z_in}] \" if t_in == 's' else f\"[{t_in}{z_in}] \"\n",
    "                else:\n",
    "                    history_str += f\"-> {t_in}{z_in} \"\n",
    "\n",
    "            probs = torch.softmax(logits[0, t], dim=0)\n",
    "            pred_uid = probs.argmax().item()\n",
    "            conf = probs.max().item() * 100\n",
    "            pred_t, pred_d, pred_dp = uni_map.get(pred_uid, (0,0,0))\n",
    "            \n",
    "            true_uid = sample['y_target'][t].item()\n",
    "            true_t, true_d, true_dp = uni_map.get(true_uid, (0,0,0))\n",
    "            \n",
    "            s_pred_d = inv_dir.get(pred_d, '?'); s_pred_t = inv_typ.get(pred_t, '?')\n",
    "            s_true_d = inv_dir.get(true_d, '?'); s_true_t = inv_typ.get(true_t, '?')\n",
    "            \n",
    "            check_d = \"✅\" if pred_d == true_d else \"❌\"\n",
    "            check_t = \"✅\" if pred_t == true_t else \"❌\"\n",
    "            check_dp = \"✅\" if pred_dp == true_dp else \"❌\"\n",
    "            \n",
    "            # --- FIX: Correctly decode the depth INDEX (not value) ---\n",
    "            # 0=N/A, 1='7'(Shallow), 2='8'(Deep), 3='9'(Very Deep)\n",
    "            def d_lbl(x):\n",
    "                if x == 0: return \"N/A\"\n",
    "                if x == 1: return \"Short\"\n",
    "                if x == 2: return \"Deep\"\n",
    "                if x == 3: return \"V.Deep\"\n",
    "                return \"?\"\n",
    "            \n",
    "            score = (1 if pred_d == true_d else 0) + (1 if pred_t == true_t else 0) + (1 if pred_dp == true_dp else 0)\n",
    "            \n",
    "            out = []\n",
    "            out.append(f\"\\nMatch {dataset.sample_match_ids[idx]}:\")\n",
    "            out.append(f\"  History:    {history_str}\")\n",
    "            out.append(f\"  Prediction: {s_pred_t} to {s_pred_d} ({d_lbl(pred_dp)}) | Conf: {conf:.0f}%\")\n",
    "            out.append(f\"  ACTUAL:     {s_true_t} to {s_true_d} ({d_lbl(true_dp)}) | {check_t} Type {check_d} Dir {check_dp} Dep\")\n",
    "            results_buffer[score].append(\"\\n\".join(out))\n",
    "            \n",
    "            count_processed += 1\n",
    "            \n",
    "    for s in [3,2,1,0]:\n",
    "        items = results_buffer[s]\n",
    "        if not items: continue\n",
    "        print(f\"\\n{'='*20} {s}/3 CORRECT ({len(items)} cases) {'='*20}\")\n",
    "        if print_flag:\n",
    "            for item in items: print(item)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. LENGTH VS ERROR\n",
    "# ==============================================================================\n",
    "def evaluation_length_vs_errrors(model, dataset, test_indices, num_matches=10):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Calculate Baselines (Weighted Random Probability)\n",
    "    print(\"Calculating dataset baselines...\")\n",
    "    from collections import Counter\n",
    "    all_d, all_dp, all_tp = [], [], []\n",
    "    \n",
    "    check_indices = test_indices[:5000] if len(test_indices) > 5000 else test_indices\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    for i in check_indices:\n",
    "        y_seq = dataset[i]['y_target']\n",
    "        for uid in y_seq:\n",
    "            if uid.item() <= 1: continue\n",
    "            t, d, dep = uni_map[uid.item()]\n",
    "            all_tp.append(t)\n",
    "            all_d.append(d)\n",
    "            if dep != 0: all_dp.append(dep)\n",
    "\n",
    "    def calc_baseline(data_list):\n",
    "        if not data_list: return 0.33\n",
    "        counts = Counter(data_list)\n",
    "        total = sum(counts.values())\n",
    "        return sum([(c/total)**2 for c in counts.values()])\n",
    "\n",
    "    base_d = calc_baseline(all_d)\n",
    "    base_dp = calc_baseline(all_dp)\n",
    "    base_tp = calc_baseline(all_tp)\n",
    "    \n",
    "    base_pair_avg = (base_d*base_dp + base_d*base_tp + base_tp*base_dp) / 3\n",
    "    base_whole = base_d * base_dp * base_tp\n",
    "    \n",
    "    print(f\"Baselines -> Dir: {base_d:.2f}, Depth: {base_dp:.2f}, Type: {base_tp:.2f}, Whole: {base_whole:.4f}\")\n",
    "    \n",
    "    # 2. Analysis Loop\n",
    "    test_match_ids = [dataset.sample_match_ids[i] for i in test_indices]\n",
    "    unique_matches = sorted(list(set(test_match_ids)))\n",
    "    selected_matches = random.sample(unique_matches, min(num_matches, len(unique_matches)))\n",
    "    selected_indices = [i for i in test_indices if dataset.sample_match_ids[i] in selected_matches]\n",
    "    print(f\"Analyzing {len(selected_indices)} points from {len(selected_matches)} matches...\")\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices:\n",
    "            sample = dataset[idx]\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "\n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            preds = logits.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            seq_len = x_seq.shape[1]\n",
    "            for t in range(seq_len):\n",
    "                if x_seq[0, t] == 0: continue\n",
    "                # Calculate absolute shot number in rally\n",
    "                history_so_far = x_seq[0, :t+1]\n",
    "                true_shot_count = (history_so_far != 0).sum().item()\n",
    "                shot_num = true_shot_count + 1\n",
    "\n",
    "                p_uid = preds[t].item(); t_uid = y[t].item()\n",
    "                if t_uid <= 1: continue \n",
    "                \n",
    "                p_t, p_d, p_dp = uni_map.get(p_uid, (0,0,0))\n",
    "                t_t, t_d, t_dp = uni_map.get(t_uid, (0,0,0))\n",
    "\n",
    "                # --- FIX: LOGIC SPLIT ---\n",
    "                \n",
    "                # 1. ALWAYS valid tasks\n",
    "                results.append({'Shot_Number': shot_num, 'Task': 'Direction', 'Type': 'Single', 'Accuracy': 1 if p_d == t_d else 0})\n",
    "                results.append({'Shot_Number': shot_num, 'Task': 'Type', 'Type': 'Single', 'Accuracy': 1 if p_t == t_t else 0})\n",
    "                results.append({'Shot_Number': shot_num, 'Task': 'Dir + Type', 'Type': 'Pair', 'Accuracy': 1 if (p_d==t_d and p_t==t_t) else 0})\n",
    "                \n",
    "                # 2. DEPTH-DEPENDENT tasks (Only count if target depth is NOT 0)\n",
    "                if t_dp != 0:\n",
    "                    results.append({'Shot_Number': shot_num, 'Task': 'Depth', 'Type': 'Single', 'Accuracy': 1 if p_dp == t_dp else 0})\n",
    "                    results.append({'Shot_Number': shot_num, 'Task': 'Dir + Depth', 'Type': 'Pair', 'Accuracy': 1 if (p_d==t_d and p_dp==t_dp) else 0})\n",
    "                    results.append({'Shot_Number': shot_num, 'Task': 'Type + Depth', 'Type': 'Pair', 'Accuracy': 1 if (p_t==t_t and p_dp==t_dp) else 0})\n",
    "                \n",
    "                # 3. Whole Shot (We count this even if depth is 0, because predicting \"No Depth\" correctly is part of the token)\n",
    "                results.append({'Shot_Number': shot_num, 'Task': 'Whole Shot', 'Type': 'Whole', 'Accuracy': 1 if p_uid == t_uid else 0})\n",
    "\n",
    "    if not results: return\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df[(df['Shot_Number'] <= 12) & (df['Shot_Number'] >= 2)]\n",
    "    \n",
    "    # --- PLOTTING ---\n",
    "    palette_single = {'Direction': '#1f77b4', 'Depth': '#d62728', 'Type': '#2ca02c'}\n",
    "    palette_pair   = {'Dir + Depth': '#9467bd', 'Dir + Type': '#17becf', 'Type + Depth': '#ff7f0e'}\n",
    "    palette_whole  = {'Whole Shot': '#000000'}\n",
    "\n",
    "    def setup_plot(title, baseline, base_label):\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.title(title, fontsize=14)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.xlabel('Shot Number', fontsize=12)\n",
    "        plt.xticks(np.arange(2, 13, 1))\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "        ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.02))\n",
    "        plt.grid(True, which='major', axis='y', linestyle='-', linewidth=0.75, color='grey', alpha=0.6)\n",
    "        plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.5, color='grey', alpha=0.3)\n",
    "        plt.ylim(0.0, 1.0)\n",
    "        \n",
    "        if baseline:\n",
    "            plt.axhline(baseline, color='#FF1493', linestyle=':', alpha=0.6, label=base_label)\n",
    "\n",
    "    # GRAPH 1: SINGLE\n",
    "    setup_plot('Single Task Accuracy vs. Rally Length', base_tp, f'Random Type ({base_tp:.2f})')\n",
    "    sns.lineplot(data=df[df['Type']=='Single'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                 markers=True, dashes=False, palette=palette_single, linewidth=2.5, errorbar=('ci', 68))\n",
    "    plt.legend(loc='lower right'); plt.show()\n",
    "    \n",
    "    # GRAPH 2: PAIRWISE\n",
    "    setup_plot('Pairwise Accuracy vs. Rally Length', base_pair_avg, f'Random Pair (~{base_pair_avg:.2f})')\n",
    "    sns.lineplot(data=df[df['Type']=='Pair'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                 markers=True, dashes=False, palette=palette_pair, linewidth=2.5, errorbar=('ci', 68))\n",
    "    plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "    # GRAPH 3: WHOLE SHOT\n",
    "    setup_plot('Whole Shot Accuracy vs. Rally Length', base_whole, f'Random Whole ({base_whole:.4f})')\n",
    "    sns.lineplot(data=df[df['Type']=='Whole'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                 markers=True, dashes={'Whole Shot':(2,2)}, palette=palette_whole, linewidth=2.5, errorbar=('ci', 68))\n",
    "    plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PLAYER FREQUENCY ANALYSIS\n",
    "# ==============================================================================\n",
    "def evaluation_player_frequency_vs_error(model, dataset, test_indices, num_matches=None):\n",
    "    \"\"\"\n",
    "    Analyzes if the model performs better on players it sees more often.\n",
    "    Plots Error Rate vs. Player Appearance Frequency.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if num_matches:\n",
    "        match_map = {}\n",
    "        for idx in test_indices:\n",
    "            mid = dataset.sample_match_ids[idx]\n",
    "            match_map.setdefault(mid, []).append(idx)\n",
    "        selected_mids = random.sample(list(match_map.keys()), min(num_matches, len(match_map)))\n",
    "        selected_indices = [idx for mid in selected_mids for idx in match_map[mid]]\n",
    "    else:\n",
    "        selected_indices = test_indices\n",
    "\n",
    "    print(f\"\\n--- PLAYER FREQUENCY ANALYSIS ({len(selected_indices)} samples) ---\")\n",
    "    \n",
    "    player_shot_counts = Counter()\n",
    "    player_stats = {} \n",
    "    inv_player_vocab = {v: k for k, v in dataset.player_vocab.items()}\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "\n",
    "    print(\"Gathering player stats and predictions (this might take a moment)...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c, x_s, x_r = sample['context'].unsqueeze(0).to(DEVICE), sample['x_s_id'].unsqueeze(0).to(DEVICE), sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "            \n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            preds = logits.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            seq_len = x_seq.shape[1]\n",
    "            s_id = sample['x_s_id'].item(); r_id = sample['x_r_id'].item()\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                if y[t] == 0: continue\n",
    "                \n",
    "                # Identify hitter\n",
    "                history_len = (x_seq[0, :t+1] != 0).sum().item()\n",
    "                actor_id = s_id if (history_len + 1) % 2 != 0 else r_id\n",
    "                if actor_id <= 1: continue\n",
    "\n",
    "                player_shot_counts[actor_id] += 1\n",
    "                if actor_id not in player_stats: \n",
    "                    player_stats[actor_id] = {'total': 0, 'correct_whole': 0, 'correct_type': 0}\n",
    "                player_stats[actor_id]['total'] += 1\n",
    "                \n",
    "                # Check\n",
    "                p_uid = preds[t].item(); t_uid = y[t].item()\n",
    "                p_t = uni_map.get(p_uid, (0,0,0))[0]\n",
    "                t_t = uni_map.get(t_uid, (0,0,0))[0]\n",
    "                \n",
    "                if p_uid == t_uid: player_stats[actor_id]['correct_whole'] += 1\n",
    "                if p_t == t_t: player_stats[actor_id]['correct_type'] += 1\n",
    "\n",
    "    # Prepare Data\n",
    "    plot_data = []\n",
    "    for pid, counts in player_stats.items():\n",
    "        if player_shot_counts[pid] < 15: continue\n",
    "        plot_data.append({\n",
    "            'Player': inv_player_vocab.get(pid, f\"ID_{pid}\").split(' ')[-1],\n",
    "            'Frequency': player_shot_counts[pid],\n",
    "            'Error_Rate_Whole': (1 - counts['correct_whole']/counts['total']) * 100,\n",
    "            'Error_Rate_Type': (1 - counts['correct_type']/counts['total']) * 100\n",
    "        })\n",
    "        \n",
    "    if not plot_data: \n",
    "        print(\"Not enough data points per player to plot.\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # Plot A: Whole Shot\n",
    "    sns.scatterplot(data=df, x='Frequency', y='Error_Rate_Whole', ax=axes[0], color='#d62728', alpha=0.6)\n",
    "    sns.regplot(data=df, x='Frequency', y='Error_Rate_Whole', ax=axes[0], scatter=False, color='black', line_kws={'linestyle':'--'})\n",
    "    axes[0].set_xscale('log')\n",
    "    axes[0].set_title('Does Fame Help? (Whole Shot Error vs. Frequency)', fontsize=14)\n",
    "    axes[0].grid(True, which=\"both\", alpha=0.2)\n",
    "    \n",
    "    # Plot B: Type\n",
    "    sns.scatterplot(data=df, x='Frequency', y='Error_Rate_Type', ax=axes[1], color='#1f77b4', alpha=0.6)\n",
    "    sns.regplot(data=df, x='Frequency', y='Error_Rate_Type', ax=axes[1], scatter=False, color='black', line_kws={'linestyle':'--'})\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].set_title('Shot Type Prediction Error vs. Frequency', fontsize=14)\n",
    "    axes[1].grid(True, which=\"both\", alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- ADDED: Correlation Calculation ---\n",
    "    corr = df['Frequency'].corr(df['Error_Rate_Whole'])\n",
    "    print(f\"Analyzed {len(df)} unique players.\")\n",
    "    print(f\"Correlation between Frequency and Error Rate: {corr:.4f}\")\n",
    "    \n",
    "    if corr < -0.3:\n",
    "        print(\">> Observation: Strong negative correlation. The model is significantly better at predicting famous players.\")\n",
    "    elif corr > 0:\n",
    "        print(\">> Observation: No advantage for famous players. The model generalizes well!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ERA COMPARISON\n",
    "# ==============================================================================\n",
    "def evaluation_compare_eras(model, dataset, test_indices, matches_per_era=50):\n",
    "    model.eval()\n",
    "    print(\"\\n--- ERA STABILITY ANALYSIS ---\")\n",
    "    match_to_indices = {}\n",
    "    for idx in test_indices:\n",
    "        match_to_indices.setdefault(dataset.sample_match_ids[idx], []).append(idx)\n",
    "        \n",
    "    eras = {'Pre-2010': [], '2010-2019': [], '2020-Present': []}\n",
    "    for m_id in match_to_indices:\n",
    "        try: y = int(str(m_id)[:4])\n",
    "        except: continue\n",
    "        if y < 2010: eras['Pre-2010'].append(m_id)\n",
    "        elif y < 2020: eras['2010-2019'].append(m_id)\n",
    "        else: eras['2020-Present'].append(m_id)\n",
    "        \n",
    "    selected_indices = []\n",
    "    era_labels = []\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    for era, m_list in eras.items():\n",
    "        chosen = random.sample(m_list, min(matches_per_era, len(m_list)))\n",
    "        for m in chosen:\n",
    "            selected_indices.extend(match_to_indices[m])\n",
    "            era_labels.extend([era]*len(match_to_indices[m]))\n",
    "            \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            sample = dataset[idx]; era = era_labels[i]\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "            \n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            preds = logits.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            for t in range(x_seq.shape[1]):\n",
    "                if y[t] == 0: continue\n",
    "                p_uid = preds[t].item(); t_uid = y[t].item()\n",
    "                p_t, p_d, _ = uni_map.get(p_uid, (0,0,0))\n",
    "                t_t, t_d, _ = uni_map.get(t_uid, (0,0,0))\n",
    "                \n",
    "                results.append({\n",
    "                    'Era': era,\n",
    "                    'Type Error': 1.0 if p_t != t_t else 0.0,\n",
    "                    'Direction Error': 1.0 if p_d != t_d else 0.0,\n",
    "                    'Whole Shot Error': 1.0 if p_uid != t_uid else 0.0\n",
    "                })\n",
    "                \n",
    "    if not results: return\n",
    "    df = pd.DataFrame(results)\n",
    "    stats = df.groupby('Era').mean() * 100\n",
    "    print(stats)\n",
    "    \n",
    "    df_melt = df.melt(id_vars=['Era'], value_vars=['Whole Shot Error', 'Type Error', 'Direction Error'], value_name='Error Rate')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Era', y='Error Rate', hue='variable', order=['Pre-2010','2010-2019','2020-Present'], errorbar=('ci',95))\n",
    "    plt.title('Error Rates Across Eras'); plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. COURT SPEED (Raw)\n",
    "# ==============================================================================\n",
    "def evaluation_court_speed_vs_error(model, dataset, test_indices, sample_size=5000):\n",
    "    model.eval()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" RAW ERROR ANALYSIS BY SURFACE (Fixed Depth Logic) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Group Test Indices by Surface\n",
    "    surface_map = {'Clay': [], 'Hard': [], 'Grass': []}\n",
    "    for idx in test_indices:\n",
    "        surf = dataset.match_meta.get(dataset.sample_match_ids[idx], {}).get('surface', 'Hard')\n",
    "        found = False\n",
    "        for k in surface_map: \n",
    "            if k in surf: \n",
    "                surface_map[k].append(idx)\n",
    "                found = True\n",
    "                break\n",
    "        if not found: surface_map['Hard'].append(idx)\n",
    "            \n",
    "    # 2. Select Samples Balanced by Surface\n",
    "    selected_indices, surface_labels = [], []\n",
    "    per_surf = sample_size // 3\n",
    "    \n",
    "    # Ensure we have the decoder map\n",
    "    uni_map = get_fast_decoder_map(dataset)\n",
    "    \n",
    "    for s, inds in surface_map.items():\n",
    "        if not inds: continue\n",
    "        chosen = random.sample(inds, min(len(inds), per_surf))\n",
    "        selected_indices.extend(chosen)\n",
    "        surface_labels.extend([s]*len(chosen))\n",
    "        \n",
    "    # 3. Evaluation Loop\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            sample = dataset[idx]\n",
    "            surf = surface_labels[i]\n",
    "            \n",
    "            # Add Batch Dimension\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y = sample['y_target'].to(DEVICE)\n",
    "            \n",
    "            # Forward Pass\n",
    "            logits = model(x_seq, x_c, x_s, x_r)\n",
    "            preds = logits.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            # Compare Tokens\n",
    "            for t in range(x_seq.shape[1]):\n",
    "                if y[t] == 0: continue # Skip padding\n",
    "                \n",
    "                p_uid = preds[t].item()\n",
    "                t_uid = y[t].item()\n",
    "                \n",
    "                # Decode (Type, Dir, Depth)\n",
    "                # 0=N/A, 1='7'(Shallow), 2='8'(Deep), 3='9'(Very Deep)\n",
    "                p_t, p_d, p_dp = uni_map.get(p_uid, (0,0,0))\n",
    "                t_t, t_d, t_dp = uni_map.get(t_uid, (0,0,0))\n",
    "                \n",
    "                # --- FIX: MASK DEPTH ERROR ---\n",
    "                # We only care about Depth Error if the TARGET actually has a depth annotation.\n",
    "                # If Target Depth is 0 (N/A), we set error to None so Pandas ignores it in mean().\n",
    "                if t_dp != 0:\n",
    "                    depth_err = 1.0 if p_dp != t_dp else 0.0\n",
    "                else:\n",
    "                    depth_err = None\n",
    "\n",
    "                results.append({\n",
    "                    'Surface': surf,\n",
    "                    'Type Error': 1.0 if p_t != t_t else 0.0,\n",
    "                    'Direction Error': 1.0 if p_d != t_d else 0.0,\n",
    "                    'Depth Error': depth_err, # Will be float or None\n",
    "                    'Whole Shot Error': 1.0 if p_uid != t_uid else 0.0\n",
    "                })\n",
    "                \n",
    "    # 4. Statistics & Plotting\n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print Table (Mean Error %)\n",
    "    # Pandas .mean() automatically ignores None/NaN values, giving us the correct accuracy\n",
    "    # for only the shots where depth was applicable.\n",
    "    stats = df.groupby('Surface')[['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error']].mean() * 100\n",
    "    print(\"\\n--- Mean Error Rates (%) [Depth calculated only on valid targets] ---\")\n",
    "    print(stats.round(2))\n",
    "    \n",
    "    # Plotting\n",
    "    # Melt handles None values (they become NaNs), seaborn barplot ignores them in calculation\n",
    "    df_melt = df.melt(id_vars=['Surface'], \n",
    "                      value_vars=['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error'], \n",
    "                      value_name='Error Rate')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Surface', y='Error Rate', hue='variable', \n",
    "                order=['Clay', 'Hard', 'Grass'], palette='viridis')\n",
    "    plt.title('Error Rates by Component vs. Whole Shot (Depth Masked)')\n",
    "    plt.ylabel('Error Rate (0.0 - 1.0)')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- RUNNER ---\n",
    "#evaluation_analyze_tactics_multitask(model, test_loader, dataset)\n",
    "if 'test_indices' in locals():\n",
    "    #evaluation_live_test_cases(model, dataset, test_indices, num_samples=5000,print_flag=False)\n",
    "    #evaluation_length_vs_errrors(model, dataset, test_indices, num_matches=2000)\n",
    "    #evaluation_player_frequency_vs_error(model, dataset, test_indices, num_matches=2000)\n",
    "    #evaluation_compare_eras(model, dataset, test_indices)\n",
    "    evaluation_court_speed_vs_error(model, dataset, test_indices, sample_size=10000)\n",
    "else:\n",
    "    print(\"Please run split first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:30:58.846878Z",
     "iopub.status.busy": "2025-12-18T14:30:58.846224Z",
     "iopub.status.idle": "2025-12-18T14:30:58.876710Z",
     "shell.execute_reply": "2025-12-18T14:30:58.875836Z",
     "shell.execute_reply.started": "2025-12-18T14:30:58.846856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- CONFIGURAZIONE GLOBALE ---\n",
    "SEQ_LEN = 30       # Fixed sequence length\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the path\n",
    "save_path = 'tennis_shot_forecasting.pth'\n",
    "\n",
    "# Seed everything to avoid randomness\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "# Call it immediately\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:31:04.932817Z",
     "iopub.status.busy": "2025-12-18T14:31:04.932224Z",
     "iopub.status.idle": "2025-12-18T14:31:04.951666Z",
     "shell.execute_reply": "2025-12-18T14:31:04.950901Z",
     "shell.execute_reply.started": "2025-12-18T14:31:04.932793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def train_baseline_model(dataset, epochs=5, batch_size=64, lr=1e-3, device='cuda'):\n",
    "    print(\"--- STARTING BASELINE (FEED-FORWARD) TRAINING ---\")\n",
    "    \n",
    "    # 1. IDENTIFY SERVE TOKENS (To exclude from Type/Dir evaluation)\n",
    "    serve_type_ids = set()\n",
    "    if hasattr(dataset, 'type_vocab'):\n",
    "        for key, idx in dataset.type_vocab.items():\n",
    "            if key.lower().startswith('serve') or key.startswith('S_'):\n",
    "                serve_type_ids.add(idx)\n",
    "    \n",
    "    # Split\n",
    "    # Calculate lengths\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.80 * total_len)\n",
    "    val_len   = int(0.15 * total_len)\n",
    "    test_len  = total_len - train_len - val_len # Remaining to ensure sum is correct\n",
    "    \n",
    "    # Perform the Split\n",
    "    train_subset, val_subset, test_subset = random_split(\n",
    "        dataset, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Create Loaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_subset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader  = DataLoader(test_subset,  batch_size=1, shuffle=True) # Batch size 1 makes live sampling easier\n",
    "\n",
    "    model = SimpleMultiHeadBaseline(\n",
    "        unified_vocab_size=len(dataset.unified_vocab),\n",
    "        type_vocab_size=len(dataset.type_vocab),\n",
    "        dir_vocab_size=len(dataset.dir_vocab),\n",
    "        depth_vocab_size=len(dataset.depth_vocab),\n",
    "        context_dim=10\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    serve_tensor = torch.tensor(list(serve_type_ids), device=device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_seq = batch['x_seq'].to(device)\n",
    "            ctx   = batch['context'].to(device)\n",
    "            y_type = batch['y_type'].to(device)\n",
    "            y_dir  = batch['y_dir'].to(device)\n",
    "            y_depth= batch['y_depth'].to(device)\n",
    "            \n",
    "            # --- Mask Serves for Loss ---\n",
    "            is_serve = torch.isin(y_type, serve_tensor)\n",
    "            \n",
    "            y_type_masked = y_type.clone()\n",
    "            y_type_masked[is_serve] = 0\n",
    "            \n",
    "            y_dir_masked = y_dir.clone()\n",
    "            y_dir_masked[is_serve] = 0\n",
    "            \n",
    "            # For Depth LOSS, we likely want to ignore \"No Depth\" (1) as well as Pad (0)\n",
    "            # if we want the model to focus on learning actual depths.\n",
    "            # But usually we leave Loss as-is (just ignore 0) and fix the Metric.\n",
    "            # However, if 90% of data is class 1, the loss will be dominated by class 1.\n",
    "            # Optional: Uncomment below to ignore class 1 in training too\n",
    "            # y_depth_masked = y_depth.clone()\n",
    "            # y_depth_masked[is_serve] = 0\n",
    "            # y_depth_masked[y_depth == 1] = 0 \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l_type, l_dir, l_depth = model(x_seq, ctx)\n",
    "            \n",
    "            loss_t = criterion(l_type.view(-1, l_type.size(-1)), y_type_masked.view(-1))\n",
    "            loss_d = criterion(l_dir.view(-1, l_dir.size(-1)), y_dir_masked.view(-1))\n",
    "            loss_p = criterion(l_depth.view(-1, l_depth.size(-1)), y_depth.view(-1)) # Keep original (or masked)\n",
    "            \n",
    "            loss = loss_t + loss_d + loss_p\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # --- Evaluation ---\n",
    "        model.eval()\n",
    "        acc_type, acc_dir, acc_depth = 0, 0, 0\n",
    "        tokens_type, tokens_dir, tokens_depth = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_seq = batch['x_seq'].to(device)\n",
    "                ctx   = batch['context'].to(device)\n",
    "                y_type = batch['y_type'].to(device)\n",
    "                y_dir  = batch['y_dir'].to(device)\n",
    "                y_depth= batch['y_depth'].to(device)\n",
    "                \n",
    "                l_type, l_dir, l_depth = model(x_seq, ctx)\n",
    "                \n",
    "                # --- MASKS ---\n",
    "                is_serve = torch.isin(y_type, serve_tensor)\n",
    "                \n",
    "                # 1. Type/Dir Mask: Ignore Pad (0) AND Serves\n",
    "                mask_common = (y_type != 0) & (~is_serve)\n",
    "                \n",
    "                # 2. Depth Mask: Ignore Pad (0) AND Index 1 (Likely \"N/A\" or \"Unknown\")\n",
    "                #    We also ignore Serves just in case.\n",
    "                mask_depth = (y_depth > 1) & (~is_serve) \n",
    "\n",
    "                # Count valid tokens\n",
    "                tokens_type += mask_common.sum().item()\n",
    "                tokens_depth += mask_depth.sum().item()\n",
    "                \n",
    "                # Accumulate Correct Predictions\n",
    "                if mask_common.sum() > 0:\n",
    "                    acc_type += (l_type.argmax(-1)[mask_common] == y_type[mask_common]).sum().item()\n",
    "                    acc_dir  += (l_dir.argmax(-1)[mask_common]  == y_dir[mask_common]).sum().item()\n",
    "                \n",
    "                if mask_depth.sum() > 0:\n",
    "                    acc_depth += (l_depth.argmax(-1)[mask_depth] == y_depth[mask_depth]).sum().item()\n",
    "        \n",
    "        # Print\n",
    "        loss_avg = total_loss/len(train_loader)\n",
    "        type_pct = (acc_type / tokens_type * 100) if tokens_type > 0 else 0\n",
    "        dir_pct  = (acc_dir  / tokens_type * 100) if tokens_type > 0 else 0\n",
    "        depth_pct= (acc_depth/ tokens_depth * 100) if tokens_depth > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss_avg:.4f}\")\n",
    "        print(f\"   Type Acc:  {type_pct:.2f}% (N={tokens_type})\")\n",
    "        print(f\"   Dir Acc:   {dir_pct:.2f}% (N={tokens_type})\")\n",
    "        print(f\"   Depth Acc: {depth_pct:.2f}% (N={tokens_depth}) <--- Only counting depth > 1\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:31:08.952468Z",
     "iopub.status.busy": "2025-12-18T14:31:08.951904Z",
     "iopub.status.idle": "2025-12-18T14:44:16.200342Z",
     "shell.execute_reply": "2025-12-18T14:44:16.199703Z",
     "shell.execute_reply.started": "2025-12-18T14:31:08.952445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/atp-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-m-points-2020s.csv',\n",
    "    base_path + 'charting-m-points-2010s.csv',\n",
    "    base_path + 'charting-m-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "\n",
    "atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "wta_path = '/kaggle/input/atp-players/wta_players.csv'\n",
    "dataset = MCPMultiTaskDataset(point_files, matches_path, atp_path, wta_path, max_seq_len=SEQ_LEN) \n",
    "\n",
    "if 'dataset' not in globals():\n",
    "    print(\"Please ensure 'dataset' is loaded.\")\n",
    "else:\n",
    "    baseline = train_baseline_model(dataset, epochs=15, batch_size=512, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:56:59.457800Z",
     "iopub.status.busy": "2025-12-18T14:56:59.457199Z",
     "iopub.status.idle": "2025-12-18T14:57:52.468923Z",
     "shell.execute_reply": "2025-12-18T14:57:52.468343Z",
     "shell.execute_reply.started": "2025-12-18T14:56:59.457776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# --- 1. SHARED CONFIG & HELPERS ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_inverse_vocabs(dataset):\n",
    "    \"\"\"Creates reverse lookups for the multi-head vocabularies\"\"\"\n",
    "    inv_type  = {v: k for k, v in dataset.type_vocab.items()}\n",
    "    inv_dir   = {v: k for k, v in dataset.dir_vocab.items()}\n",
    "    inv_depth = {v: k for k, v in dataset.depth_vocab.items()}\n",
    "    return inv_type, inv_dir, inv_depth\n",
    "\n",
    "# --- 2. THE MULTI-TASK EVALUATION FUNCTION ---\n",
    "def run_full_evaluation(model, dataset, loader, test_indices, \n",
    "                        live_samples=5000, \n",
    "                        length_matches=2000, \n",
    "                        freq_matches=2000, \n",
    "                        era_matches=50, \n",
    "                        speed_samples=10000):\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Starting Multi-Task Evaluation on {len(test_indices)} test samples...\")\n",
    "    print(f\"Batch Size: {loader.batch_size} (Part 1 will run much faster now)\")\n",
    "\n",
    "    # --- Helpers ---\n",
    "    inv_type, inv_dir, inv_depth = get_inverse_vocabs(dataset)\n",
    "    \n",
    "    # Reverse Unified Vocab\n",
    "    inv_unified = {}\n",
    "    if hasattr(dataset, 'unified_vocab'):\n",
    "        inv_unified = {v: k for k, v in dataset.unified_vocab.items()}\n",
    "\n",
    "    serve_type_id = dataset.type_vocab.get('serve', dataset.type_vocab.get('s', -1))\n",
    "    unk_depth_id = dataset.depth_vocab.get('0', -1)\n",
    "    unk_dir_id = dataset.dir_vocab.get('0', -1)\n",
    "\n",
    "    # --- Match Map ---\n",
    "    if not hasattr(dataset, 'sample_match_ids'):\n",
    "        print(\"Error: Dataset missing 'sample_match_ids'.\")\n",
    "        return\n",
    "\n",
    "    match_map = {}\n",
    "    for idx in test_indices:\n",
    "        mid = dataset.sample_match_ids[idx]\n",
    "        match_map.setdefault(mid, []).append(idx)\n",
    "    unique_matches = list(match_map.keys())\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 1: OVERALL TACTICAL METRICS (Vectorized & Fast)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 1: OVERALL TACTICAL METRICS \\n\" + \"=\"*40)\n",
    "    \n",
    "    all_preds_type, all_targs_type = [], []\n",
    "    all_preds_dir,  all_targs_dir  = [], []\n",
    "    all_preds_depth, all_targs_depth = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i % 50 == 0: print(f\"Processing batch {i}...\", end='\\r')\n",
    "            \n",
    "            x_seq = batch['x_seq'].to(DEVICE)\n",
    "            x_c   = batch['context'].to(DEVICE)\n",
    "            \n",
    "            # Inference on full batch\n",
    "            l_type, l_dir, l_depth = model(x_seq, x_c)\n",
    "            \n",
    "            # --- Type ---\n",
    "            y_type = batch['y_type'].to(DEVICE).view(-1)\n",
    "            mask_t = (y_type != 0)\n",
    "            if serve_type_id != -1: mask_t &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_t.sum() > 0:\n",
    "                all_preds_type.extend(l_type.argmax(-1).view(-1)[mask_t].cpu().numpy())\n",
    "                all_targs_type.extend(y_type[mask_t].cpu().numpy())\n",
    "\n",
    "            # --- Direction ---\n",
    "            y_dir = batch['y_dir'].to(DEVICE).view(-1)\n",
    "            mask_d = (y_dir != 0) & (y_dir != unk_dir_id)\n",
    "            if serve_type_id != -1: mask_d &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_d.sum() > 0:\n",
    "                all_preds_dir.extend(l_dir.argmax(-1).view(-1)[mask_d].cpu().numpy())\n",
    "                all_targs_dir.extend(y_dir[mask_d].cpu().numpy())\n",
    "\n",
    "            # --- Depth ---\n",
    "            y_depth = batch['y_depth'].to(DEVICE).view(-1)\n",
    "            mask_dp = (y_depth != 0) & (y_depth != unk_depth_id)\n",
    "            if serve_type_id != -1: mask_dp &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_dp.sum() > 0:\n",
    "                all_preds_depth.extend(l_depth.argmax(-1).view(-1)[mask_dp].cpu().numpy())\n",
    "                all_targs_depth.extend(y_depth[mask_dp].cpu().numpy())\n",
    "\n",
    "    # Reports\n",
    "    print(\"\\n=== SHOT TYPE REPORT (Rally Only) ===\")\n",
    "    labels = [k for k in dataset.type_vocab if k not in ['<pad>', '<unk>', 'serve', 's']]\n",
    "    indices = [dataset.type_vocab[k] for k in labels]\n",
    "    present = [i for i in indices if i in np.unique(all_targs_type)]\n",
    "    present_lbls = [inv_type[i] for i in present]\n",
    "    if present:\n",
    "        print(classification_report(all_targs_type, all_preds_type, labels=present, target_names=present_lbls, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== DIRECTION REPORT ===\")\n",
    "    labels = [k for k in dataset.dir_vocab if k not in ['<pad>', '0']]\n",
    "    indices = [dataset.dir_vocab[k] for k in labels]\n",
    "    if indices:\n",
    "        print(classification_report(all_targs_dir, all_preds_dir, labels=indices, target_names=labels, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== DEPTH REPORT ===\")\n",
    "    labels = [k for k in dataset.depth_vocab if k not in ['<pad>', '0']]\n",
    "    indices = [dataset.depth_vocab[k] for k in labels]\n",
    "    if indices:\n",
    "        print(classification_report(all_targs_depth, all_preds_depth, labels=indices, target_names=labels, zero_division=0))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 2: LIVE SAMPLES (Updated for Large Batch Sizes)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 2: LIVE SAMPLES (Showing {live_samples} Cases) \\n\" + \"=\"*40)\n",
    "    \n",
    "    processed_count = 0\n",
    "    correct_3_of_3 = 0\n",
    "    results_buffer = {3: [], 2:[], 1:[], 0:[]} \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if processed_count >= live_samples:\n",
    "                break\n",
    "                \n",
    "            x_seq_batch = batch['x_seq'].to(DEVICE)     # [B, SeqLen]\n",
    "            ctx_batch   = batch['context'].to(DEVICE)   # [B, ContextDim]\n",
    "            y_t_batch   = batch['y_type'].to(DEVICE)\n",
    "            y_d_batch   = batch['y_dir'].to(DEVICE)\n",
    "            y_dp_batch  = batch['y_depth'].to(DEVICE)\n",
    "\n",
    "            # Inference on WHOLE batch\n",
    "            l_t_batch, l_d_batch, l_dp_batch = model(x_seq_batch, ctx_batch) \n",
    "\n",
    "            # Iterate through items in the batch\n",
    "            batch_size = x_seq_batch.size(0)\n",
    "            \n",
    "            for k in range(batch_size):\n",
    "                if processed_count >= live_samples: break\n",
    "                \n",
    "                # --- SINGLE ITEM LOGIC START ---\n",
    "                # Check valid indices for this specific rally 'k'\n",
    "                seq_len = x_seq_batch.size(1)\n",
    "                valid_indices = []\n",
    "                for t in range(seq_len):\n",
    "                    true_type = y_t_batch[k, t].item()\n",
    "                    if true_type > 1 and inv_type.get(true_type) not in ['serve', 's']:\n",
    "                        valid_indices.append(t)\n",
    "                \n",
    "                if not valid_indices: \n",
    "                    continue\n",
    "\n",
    "                t = random.choice(valid_indices)\n",
    "                \n",
    "                # RECONSTRUCT HISTORY (Using index k)\n",
    "                history_str = \"\"\n",
    "                raw_history = x_seq_batch[k, :t+1].cpu().numpy()\n",
    "                hist_tokens = []\n",
    "                for h_idx in raw_history:\n",
    "                    if h_idx == 0: continue \n",
    "                    token_str = inv_unified.get(h_idx, '?')\n",
    "                    parts = token_str.split('_')\n",
    "                    if parts[0].lower() in ['serve', 's']:\n",
    "                        hist_tokens.append(f\"[S{parts[1] if len(parts)>1 else ''}]\")\n",
    "                    else:\n",
    "                        shot = parts[0].upper()\n",
    "                        dire = parts[1] if len(parts)>1 else ''\n",
    "                        hist_tokens.append(f\"{shot}{dire}\")\n",
    "                \n",
    "                history_str = \" -> \".join(hist_tokens[-6:]) \n",
    "                \n",
    "                # PREDICTION (Using index k)\n",
    "                pred_t = l_t_batch[k, t].argmax().item()\n",
    "                pred_d = l_d_batch[k, t].argmax().item()\n",
    "                pred_dp = l_dp_batch[k, t].argmax().item()\n",
    "                conf_t = torch.softmax(l_t_batch[k, t], dim=0).max().item() * 100\n",
    "\n",
    "                true_t = y_t_batch[k, t].item()\n",
    "                true_d = y_d_batch[k, t].item()\n",
    "                true_dp = y_dp_batch[k, t].item()\n",
    "\n",
    "                s_pred_t = inv_type.get(pred_t, '?')\n",
    "                s_pred_d = inv_dir.get(pred_d, '?')\n",
    "                s_pred_dp = inv_depth.get(pred_dp, '?')\n",
    "                \n",
    "                s_true_t = inv_type.get(true_t, '?')\n",
    "                s_true_d = inv_dir.get(true_d, '?')\n",
    "                s_true_dp = inv_depth.get(true_dp, '?')\n",
    "\n",
    "                ok_t = \"✅\" if pred_t == true_t else \"❌\"\n",
    "                ok_d = \"✅\" if pred_d == true_d else \"❌\"\n",
    "                ok_dp = \"✅\" if pred_dp == true_dp else \"❌\"\n",
    "\n",
    "                score = (1 if pred_t==true_t else 0) + (1 if pred_d==true_d else 0) + (1 if pred_dp==true_dp else 0)\n",
    "                if score == 3: correct_3_of_3 += 1\n",
    "                \n",
    "                res_str = (\n",
    "                    f\"Sample #{processed_count+1}\\n\"\n",
    "                    f\"History:  ... {history_str}\\n\"\n",
    "                    f\"Model:    [{s_pred_t}] to Zone [{s_pred_d}] (Depth {s_pred_dp}) | Conf: {conf_t:.0f}%\\n\"\n",
    "                    f\"Actual:   [{s_true_t}] to Zone [{s_true_d}] (Depth {s_true_dp}) | {ok_t}{ok_d}{ok_dp}\\n\"\n",
    "                    f\"{'-'*40}\"\n",
    "                )\n",
    "                results_buffer[score].append(res_str)\n",
    "                processed_count += 1\n",
    "                # --- SINGLE ITEM LOGIC END ---\n",
    "\n",
    "            if processed_count % 500 == 0:\n",
    "                print(f\"Processed {processed_count}/{live_samples} samples...\", end='\\r')\n",
    "\n",
    "    # Print Best Matches\n",
    "    print_flag = False\n",
    "    print(\"\\n--- SAMPLE PREDICTIONS ---\")\n",
    "    for s in [3, 2, 1, 0]:\n",
    "        items = results_buffer[s]\n",
    "        if items:\n",
    "            print(f\"\\n{'='*20} {s}/3 CORRECT ({len(items)} cases) {'='*20}\")\n",
    "            if print_flag:    \n",
    "                for item in items:\n",
    "                    print(item)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 3: GRANULAR ACCURACY VS RALLY LENGTH\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 3: GRANULAR ACCURACY VS RALLY LENGTH \\n\" + \"=\"*40)\n",
    "    \n",
    "    # 3.1 Calculate Baselines\n",
    "    print(\"Calculating dataset baselines...\")\n",
    "    all_d, all_dp, all_tp = [], [], []\n",
    "    for i in test_indices:\n",
    "        sample = dataset[i]\n",
    "        yt = sample['y_type']\n",
    "        yd = sample['y_dir']\n",
    "        ydp = sample['y_depth']\n",
    "        \n",
    "        if torch.is_tensor(yt): yt = yt.cpu().numpy()\n",
    "        if torch.is_tensor(yd): yd = yd.cpu().numpy()\n",
    "        if torch.is_tensor(ydp): ydp = ydp.cpu().numpy()\n",
    "\n",
    "        for j in range(len(yt)):\n",
    "            if yt[j] == 0: continue\n",
    "            if yt[j] != serve_type_id:\n",
    "                all_tp.append(yt[j])\n",
    "                if yd[j] != unk_dir_id and yd[j] != 0: all_d.append(yd[j])\n",
    "                if ydp[j] != unk_depth_id and ydp[j] != 0: all_dp.append(ydp[j])\n",
    "\n",
    "    def calc_baseline(data_list):\n",
    "        if not data_list: return 0.33\n",
    "        counts = Counter(data_list)\n",
    "        total = sum(counts.values())\n",
    "        return sum([(c/total)**2 for c in counts.values()])\n",
    "\n",
    "    base_d = calc_baseline(all_d)\n",
    "    base_dp = calc_baseline(all_dp)\n",
    "    base_tp = calc_baseline(all_tp)\n",
    "    base_pair_avg = (base_d*base_dp + base_d*base_tp + base_tp*base_dp) / 3\n",
    "    base_whole = base_d * base_dp * base_tp\n",
    "    print(f\"Baselines -> Dir: {base_d:.2f}, Depth: {base_dp:.2f}, Type: {base_tp:.2f}, Whole: {base_whole:.4f}\")\n",
    "    \n",
    "    # 3.2 Analysis Loop\n",
    "    chosen_matches = random.sample(unique_matches, min(length_matches, len(unique_matches)))\n",
    "    rl_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    print(f\"Analyzing {len(rl_indices)} points from {len(chosen_matches)} matches...\")\n",
    "\n",
    "    results_p3 = []\n",
    "    with torch.no_grad():\n",
    "        for idx in rl_indices:\n",
    "            sample = dataset[idx]\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c   = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            l_type, l_dir, l_depth = model(x_seq, x_c)\n",
    "            preds_t = l_type.argmax(dim=-1).squeeze(0)\n",
    "            preds_d = l_dir.argmax(dim=-1).squeeze(0)\n",
    "            preds_dp = l_depth.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            y_t = sample['y_type'].to(DEVICE)\n",
    "            y_d = sample['y_dir'].to(DEVICE)\n",
    "            y_dp = sample['y_depth'].to(DEVICE)\n",
    "            \n",
    "            x_seq_cpu = sample['x_seq']\n",
    "            \n",
    "            limit = min(len(y_t), len(preds_t))\n",
    "\n",
    "            for t_step in range(limit):\n",
    "                if y_t[t_step] == 0: continue\n",
    "                if y_t[t_step] == serve_type_id: continue\n",
    "\n",
    "                shot_num = (x_seq_cpu[:t_step+1] != 0).sum().item() + 1\n",
    "                \n",
    "                pt, p_d, pdp = preds_t[t_step].item(), preds_d[t_step].item(), preds_dp[t_step].item()\n",
    "                tt, td, tdp = y_t[t_step].item(), y_d[t_step].item(), y_dp[t_step].item()\n",
    "\n",
    "                ok_t = (pt == tt)\n",
    "                has_dir = (td != 0 and td != unk_dir_id)\n",
    "                has_depth = (tdp != 0 and tdp != unk_depth_id)\n",
    "                ok_d = (p_d == td) if has_dir else False\n",
    "                ok_dp = (pdp == tdp) if has_depth else False\n",
    "\n",
    "                # Logic Split - Single\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Type', 'Type': 'Single', 'Accuracy': 1 if ok_t else 0})\n",
    "                if has_dir:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Direction', 'Type': 'Single', 'Accuracy': 1 if ok_d else 0})\n",
    "                if has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Depth', 'Type': 'Single', 'Accuracy': 1 if ok_dp else 0})\n",
    "                \n",
    "                # Logic Split - Pair\n",
    "                if has_dir:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Type', 'Type': 'Pair', 'Accuracy': 1 if (ok_d and ok_t) else 0})\n",
    "                if has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Type + Depth', 'Type': 'Pair', 'Accuracy': 1 if (ok_dp and ok_t) else 0})\n",
    "                if has_dir and has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Depth', 'Type': 'Pair', 'Accuracy': 1 if (ok_d and ok_dp) else 0})\n",
    "                \n",
    "                # Logic Split - Whole\n",
    "                if has_dir and has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Whole Shot', 'Type': 'Whole', 'Accuracy': 1 if (ok_d and ok_dp and ok_t) else 0})\n",
    "\n",
    "    if results_p3:\n",
    "        df = pd.DataFrame(results_p3)\n",
    "        df = df[(df['Shot_Number'] <= 12) & (df['Shot_Number'] >= 2)]\n",
    "        \n",
    "        palette_single = {'Direction': '#1f77b4', 'Depth': '#d62728', 'Type': '#2ca02c'}\n",
    "        palette_pair   = {'Dir + Depth': '#9467bd', 'Dir + Type': '#17becf', 'Type + Depth': '#ff7f0e'}\n",
    "        palette_whole  = {'Whole Shot': '#000000'}\n",
    "\n",
    "        def setup_plot(title, baseline, base_label):\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.xlabel('Shot Number', fontsize=12)\n",
    "            plt.xticks(np.arange(2, 13, 1))\n",
    "            \n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "            ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.02))\n",
    "            plt.grid(True, which='major', axis='y', linestyle='-', linewidth=0.75, color='grey', alpha=0.6)\n",
    "            plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.5, color='grey', alpha=0.3)\n",
    "            plt.ylim(0.0, 1.0)\n",
    "            \n",
    "            if baseline:\n",
    "                plt.axhline(baseline, color='#FF1493', linestyle=':', alpha=0.8, linewidth=2, label=base_label)\n",
    "\n",
    "        setup_plot('Single Task Accuracy vs. Rally Length', base_tp, f'Random Type ({base_tp:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Single'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_single, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='lower right'); plt.show()\n",
    "        \n",
    "        setup_plot('Pairwise Accuracy vs. Rally Length', base_pair_avg, f'Random Pair (~{base_pair_avg:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Pair'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_pair, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "        setup_plot('Whole Shot Accuracy vs. Rally Length', base_whole, f'Random Whole ({base_whole:.4f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Whole'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes={'Whole Shot':(2,2)}, palette=palette_whole, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "    # ==============================================================================\n",
    "    # PART 4: PLAYER FREQUENCY\n",
    "    # ==============================================================================\n",
    "    if 'x_s_id' in dataset[0]:\n",
    "        print(\"\\n\" + \"=\"*40 + f\"\\n PART 4: PLAYER FREQUENCY ({freq_matches} Matches) \\n\" + \"=\"*40)\n",
    "        chosen_matches = random.sample(unique_matches, min(freq_matches, len(unique_matches)))\n",
    "        pf_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "        \n",
    "        p_counts = Counter()\n",
    "        p_stats = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in pf_indices:\n",
    "                sample = dataset[idx]\n",
    "                x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "                x_c   = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "                y_type = sample['y_type'].to(DEVICE)\n",
    "                \n",
    "                l_t, _, _ = model(x_seq, x_c)\n",
    "                preds = l_t.argmax(-1).squeeze(0)\n",
    "                \n",
    "                s_val = sample['x_s_id'] if isinstance(sample['x_s_id'], int) else sample['x_s_id'].item()\n",
    "                r_val = sample['x_r_id'] if isinstance(sample['x_r_id'], int) else sample['x_r_id'].item()\n",
    "                \n",
    "                x_seq_cpu = sample['x_seq']\n",
    "                \n",
    "                for t in range(len(y_type)):\n",
    "                    if y_type[t] == 0: continue\n",
    "                    if y_type[t] == serve_type_id: continue\n",
    "                    if t >= len(preds): break\n",
    "\n",
    "                    hist_len = (x_seq_cpu[:t+1] != 0).sum().item()\n",
    "                    actor = s_val if (hist_len + 1) % 2 != 0 else r_val\n",
    "                    if actor <= 1: continue \n",
    "                    \n",
    "                    p_counts[actor] += 1\n",
    "                    if actor not in p_stats: p_stats[actor] = {'tot': 0, 'corr': 0}\n",
    "                    p_stats[actor]['tot'] += 1\n",
    "                    if preds[t].item() == y_type[t].item(): p_stats[actor]['corr'] += 1\n",
    "\n",
    "        pf_data = [{'Freq': p_counts[a], 'Err': (1 - v['corr']/v['tot'])*100} for a, v in p_stats.items() if p_counts[a] > 10]\n",
    "        if pf_data:\n",
    "            df_pf = pd.DataFrame(pf_data)\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.regplot(data=df_pf, x='Freq', y='Err', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "            plt.xscale('log'); plt.title(f\"Type Error vs Frequency\"); plt.show()\n",
    "    else:\n",
    "        print(\"\\n(Part 4 Skipped: Player IDs not in dataset)\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 5: ERA STABILITY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 5: ERA STABILITY ({era_matches} Matches/Era) \\n\" + \"=\"*40)\n",
    "    eras = {'Pre-2010': [], '2010-2019': [], '2020+': []}\n",
    "    for m_id in unique_matches:\n",
    "        try: y_year = int(str(m_id)[:4])\n",
    "        except: continue\n",
    "        if y_year < 2010: eras['Pre-2010'].append(m_id)\n",
    "        elif y_year < 2020: eras['2010-2019'].append(m_id)\n",
    "        else: eras['2020+'].append(m_id)\n",
    "\n",
    "    era_indices = []\n",
    "    era_labels_list = []\n",
    "    for era_name, m_list in eras.items():\n",
    "        if not m_list: continue\n",
    "        chosen = random.sample(m_list, min(era_matches, len(m_list)))\n",
    "        for m in chosen:\n",
    "            era_indices.extend(match_map[m])\n",
    "            era_labels_list.extend([era_name]*len(match_map[m]))\n",
    "            \n",
    "    era_res = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(era_indices):\n",
    "            sample = dataset[idx]\n",
    "            x_seq = sample['x_seq'].unsqueeze(0).to(DEVICE)\n",
    "            x_c   = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            y_type = sample['y_type'].to(DEVICE)\n",
    "            \n",
    "            l_t, _, _ = model(x_seq, x_c)\n",
    "            p_t = l_t.argmax(-1).squeeze(0)\n",
    "            \n",
    "            # Trim to match logic\n",
    "            limit = min(len(y_type), len(p_t))\n",
    "            p_t_trim = p_t[:limit]\n",
    "            y_type_trim = y_type[:limit]\n",
    "\n",
    "            mask = (y_type_trim > 1) & (y_type_trim != serve_type_id)\n",
    "            if mask.sum() > 0:\n",
    "                correct = (p_t_trim[mask] == y_type_trim[mask])\n",
    "                acc = correct.float().mean().item()\n",
    "                era_res.append({'Era': era_labels_list[i], 'Type Acc': acc})\n",
    "    \n",
    "    if era_res:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(data=pd.DataFrame(era_res), x='Era', y='Type Acc', palette='viridis', order=['Pre-2010', '2010-2019', '2020+'])\n",
    "        plt.title('Shot Type Accuracy by Era'); plt.ylim(0, 1); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 6: SURFACE DIFFICULTY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" PART 6: SURFACE ERROR ANALYSIS (Baseline Model) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- 1. Efficient Surface Grouping ---\n",
    "    print(f\"Grouping {len(test_indices)} test samples by surface...\")\n",
    "    surface_indices = {'Clay': [], 'Hard': [], 'Grass': []}\n",
    "    \n",
    "    # Pre-fetch match IDs via dataset metadata\n",
    "    for idx in test_indices:\n",
    "        m_id = dataset.sample_match_ids[idx]\n",
    "        surf = dataset.match_meta.get(m_id, {}).get('surface', 'Hard')\n",
    "        \n",
    "        # Normalize strings\n",
    "        if 'Clay' in surf:   target = 'Clay'\n",
    "        elif 'Grass' in surf: target = 'Grass'\n",
    "        else:                 target = 'Hard'\n",
    "        \n",
    "        surface_indices[target].append(idx)\n",
    "\n",
    "    # --- 2. Evaluation Loop ---\n",
    "    results = []\n",
    "    inv_depth = {v: k for k, v in dataset.depth_vocab.items()}\n",
    "    inv_dir   = {v: k for k, v in dataset.dir_vocab.items()}\n",
    "    \n",
    "    # Identify valid IDs (Strict Mode)\n",
    "    # Valid Depth: '7' (Short), '8' (Deep), '9' (Very Deep)\n",
    "    valid_depth_ids = [v for k, v in dataset.depth_vocab.items() if k in ['7', '8', '9']]\n",
    "    \n",
    "    # Valid Direction: '1', '2', '3' (Excluding '0' center/unknown)\n",
    "    valid_dir_ids = [v for k, v in dataset.dir_vocab.items() if k not in ['<pad>', '0', '<unk>']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for surf, inds in surface_indices.items():\n",
    "            if not inds: \n",
    "                continue\n",
    "            \n",
    "            print(f\"Scanning {len(inds)} samples for {surf}...\")\n",
    "            \n",
    "            # Create a DataLoader for this surface slice\n",
    "            surf_ds = Subset(dataset, inds)\n",
    "            loader = DataLoader(surf_ds, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            for batch in loader:\n",
    "                # Move Inputs (Baseline uses x_seq + context)\n",
    "                x_seq = batch['x_seq'].to(DEVICE)\n",
    "                ctx   = batch['context'].to(DEVICE)\n",
    "                \n",
    "                # Move Targets\n",
    "                y_t  = batch['y_type'].to(DEVICE)\n",
    "                y_d  = batch['y_dir'].to(DEVICE)\n",
    "                y_dp = batch['y_depth'].to(DEVICE)\n",
    "                \n",
    "                # Forward Pass (Baseline Signature)\n",
    "                l_t, l_d, l_dp = model(x_seq, ctx)\n",
    "                \n",
    "                # Get Predictions\n",
    "                p_t  = l_t.argmax(dim=-1)\n",
    "                p_d  = l_d.argmax(dim=-1)\n",
    "                p_dp = l_dp.argmax(dim=-1)\n",
    "                \n",
    "                # --- Vectorized Error Calculation ---\n",
    "                \n",
    "                # 1. Base Mask: Must be a real shot (not padding)\n",
    "                # We also usually exclude serves from analysis to focus on rally dynamics\n",
    "                # Check if 'serve' exists in vocab to handle it safely\n",
    "                serve_id = dataset.type_vocab.get('serve', -1)\n",
    "                \n",
    "                valid_mask = (y_t != 0) # Ignore Pad\n",
    "                if serve_id != -1:\n",
    "                    valid_mask &= (y_t != serve_id) # Ignore Serves\n",
    "                \n",
    "                if valid_mask.sum() == 0: continue\n",
    "\n",
    "                # Filter down to valid shots (Flattening for easy iteration)\n",
    "                curr_y_t  = y_t[valid_mask].cpu().numpy()\n",
    "                curr_p_t  = p_t[valid_mask].cpu().numpy()\n",
    "                \n",
    "                curr_y_d  = y_d[valid_mask].cpu().numpy()\n",
    "                curr_p_d  = p_d[valid_mask].cpu().numpy()\n",
    "                \n",
    "                curr_y_dp = y_dp[valid_mask].cpu().numpy()\n",
    "                curr_p_dp = p_dp[valid_mask].cpu().numpy()\n",
    "\n",
    "                for i in range(len(curr_y_t)):\n",
    "                    # A. Type Error\n",
    "                    t_err = 1.0 if curr_y_t[i] != curr_p_t[i] else 0.0\n",
    "                    \n",
    "                    # B. Direction Error (Strict Masking)\n",
    "                    # Only count if ground truth is a specific direction (1,2,3)\n",
    "                    if curr_y_d[i] in valid_dir_ids:\n",
    "                        d_err = 1.0 if curr_y_d[i] != curr_p_d[i] else 0.0\n",
    "                    else:\n",
    "                        d_err = None # Ignore 'Center' or 'Unknown' shots\n",
    "                        \n",
    "                    # C. Depth Error (VERY Strict Masking)\n",
    "                    # Only count if ground truth is strictly Deep/Short (7,8,9)\n",
    "                    if curr_y_dp[i] in valid_depth_ids:\n",
    "                        dp_err = 1.0 if curr_y_dp[i] != curr_p_dp[i] else 0.0\n",
    "                    else:\n",
    "                        dp_err = None # Ignore standard depth shots\n",
    "                        \n",
    "                    # D. Whole Shot Error\n",
    "                    # Defined as: Type must be right, AND (Dir right if valid), AND (Depth right if valid)\n",
    "                    miss = False\n",
    "                    if t_err == 1.0: miss = True\n",
    "                    if d_err is not None and d_err == 1.0: miss = True\n",
    "                    if dp_err is not None and dp_err == 1.0: miss = True\n",
    "                    \n",
    "                    ws_err = 1.0 if miss else 0.0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Surface': surf,\n",
    "                        'Type Error': t_err,\n",
    "                        'Direction Error': d_err,\n",
    "                        'Depth Error': dp_err,\n",
    "                        'Whole Shot Error': ws_err\n",
    "                    })\n",
    "\n",
    "    # --- 3. Output Generation ---\n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Diagnostics\n",
    "    depth_counts = df.groupby('Surface')['Depth Error'].count()\n",
    "    print(\"\\n[Diagnostics] Valid Depth Samples found per surface (Baseline):\")\n",
    "    print(depth_counts)\n",
    "    \n",
    "    # Stats Table\n",
    "    stats = df.groupby('Surface')[['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error']].mean() * 100\n",
    "    print(\"\\n--- Mean Error Rates (%) [Lower is Better] ---\")\n",
    "    print(stats.round(2))\n",
    "    \n",
    "    # Plot\n",
    "    df_melt = df.melt(id_vars=['Surface'], \n",
    "                      value_vars=['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error'], \n",
    "                      value_name='Error Rate')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Surface', y='Error Rate', hue='variable', \n",
    "                order=['Clay', 'Hard', 'Grass'], palette='viridis', errorbar=('ci', 95))\n",
    "    \n",
    "    plt.title(f'Baseline Model: Error Rates by Component (N={len(df)})')\n",
    "    plt.ylabel('Error Rate (0.0 - 1.0)')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- 3. RUNNER SNIPPET ---\n",
    "if 'dataset' in globals() and 'baseline' in globals():\n",
    "    print(\"Recreating 80/15/5 split for evaluation...\")\n",
    "    \n",
    "    seed_everything(42) \n",
    "    \n",
    "    # Calculate Exact Lengths\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.80 * total_len)\n",
    "    val_len   = int(0.15 * total_len)\n",
    "    test_len  = total_len - train_len - val_len\n",
    "    \n",
    "    # 3-Way Split\n",
    "    _, _, test_ds = random_split(\n",
    "        dataset, \n",
    "        [train_len, val_len, test_len], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Extract Indices and Loader\n",
    "    test_indices = test_ds.indices\n",
    "    \n",
    "    # IMPORTANT: Use a LARGE batch size (e.g., 512) for speed. \n",
    "    # The updated function now handles this correctly for all parts.\n",
    "    test_loader_eval = DataLoader(test_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "    print(f\"Test Set Ready: {len(test_ds)} samples.\")\n",
    "\n",
    "    # Run Full Evaluation\n",
    "    run_full_evaluation(\n",
    "        model=baseline, \n",
    "        dataset=dataset, \n",
    "        loader=test_loader_eval, \n",
    "        test_indices=test_indices,\n",
    "        live_samples=5000,     \n",
    "        length_matches=2000,   \n",
    "        freq_matches=2000,     \n",
    "        era_matches=50,       \n",
    "        speed_samples=10000  \n",
    "    )\n",
    "else:\n",
    "    print(\"Error: 'dataset' or 'baseline' not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAINED TRANSFORMER (hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T10:54:56.691186Z",
     "iopub.status.busy": "2025-12-18T10:54:56.690364Z",
     "iopub.status.idle": "2025-12-18T10:54:56.703159Z",
     "shell.execute_reply": "2025-12-18T10:54:56.702359Z",
     "shell.execute_reply.started": "2025-12-18T10:54:56.691149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_hierarchical_model(dataset, epochs=10, batch_size=64, lr=1e-3, device='cuda'):\n",
    "    print(f\"--- STARTING HIERARCHICAL TRANSFORMER TRAINING ---\")\n",
    "    \n",
    "    # 0. IDENTIFY SPECIAL TOKENS\n",
    "    serve_type_id = dataset.type_vocab.get('serve', dataset.type_vocab.get('s', -1))\n",
    "    \n",
    "    # Identify the ID for '0' (Unknown) in depth/dir vocabs\n",
    "    unk_depth_id = dataset.depth_vocab.get('0', -1)\n",
    "    unk_dir_id = dataset.dir_vocab.get('0', -1)\n",
    "    \n",
    "    print(f\"Masking Config:\")\n",
    "    print(f\" - Serve Type ID: {serve_type_id} (Will mask Dir/Depth for serves)\")\n",
    "    print(f\" - Unknown Depth ID: {unk_depth_id} (Will be ignored in loss)\")\n",
    "    print(f\" - Unknown Dir ID: {unk_dir_id} (Will be ignored in loss)\")\n",
    "\n",
    "    # 1. Split Dataset\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.8 * total_len)\n",
    "    val_len = int(0.1 * total_len)\n",
    "    test_len = total_len - train_len - val_len\n",
    "    \n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        dataset, [train_len, val_len, test_len], \n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    try:\n",
    "        real_context_dim = dataset.context_tensor.shape[1]\n",
    "    except AttributeError:\n",
    "        real_context_dim = dataset.context.shape[1]\n",
    "\n",
    "    # 2. Initialize Model\n",
    "    model = HierarchicalCristianGPT(\n",
    "        dir_vocab_size=len(dataset.dir_vocab),\n",
    "        depth_vocab_size=len(dataset.depth_vocab),\n",
    "        type_vocab_size=len(dataset.type_vocab), \n",
    "        num_players=len(dataset.player_vocab),\n",
    "        context_dim=real_context_dim,   \n",
    "        embed_dim=64, \n",
    "        n_head=4, \n",
    "        n_cycles=3\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_z = batch['x_dir'].to(device)\n",
    "            x_t = batch['x_type'].to(device)\n",
    "            x_c = batch['context'].to(device)\n",
    "            x_s = batch['x_s_id'].to(device)\n",
    "            x_r = batch['x_r_id'].to(device)\n",
    "            \n",
    "            y_d = batch['y_dir'].to(device)\n",
    "            y_dp = batch['y_depth'].to(device)\n",
    "            y_t = batch['y_type'].to(device)\n",
    "            \n",
    "            # --- CRITICAL FIX: MASKING TARGETS ---\n",
    "            # We must mask specific conditions to prevent learning useless \"Unknown\" labels\n",
    "            \n",
    "            # 1. Clone targets\n",
    "            y_d = y_d.clone()\n",
    "            y_dp = y_dp.clone()\n",
    "            y_t = y_t.clone()\n",
    "            \n",
    "            # 2. Mask \"Unknown\" Depths and Directions (Set to 0/Padding)\n",
    "            # This forces the model to only learn when valid Depth/Dir exists\n",
    "            if unk_depth_id != -1:\n",
    "                y_dp[y_dp == unk_depth_id] = 0\n",
    "            if unk_dir_id != -1:\n",
    "                y_d[y_d == unk_dir_id] = 0\n",
    "\n",
    "            # 3. Mask Dir/Depth completely if the shot is a Serve\n",
    "            if serve_type_id != -1:\n",
    "                is_serve = (y_t == serve_type_id)\n",
    "                y_d[is_serve] = 0\n",
    "                y_dp[is_serve] = 0\n",
    "                # Note: We do NOT mask y_t here, we want it to learn to predict \"Serve\" type\n",
    "            # ---------------------------------------\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred_d, pred_dp, pred_t = model(x_z, x_t, x_c, x_s, x_r)\n",
    "            \n",
    "            l_d = criterion(pred_d.view(-1, len(dataset.dir_vocab)), y_d.view(-1))\n",
    "            l_dp = criterion(pred_dp.view(-1, len(dataset.depth_vocab)), y_dp.view(-1))\n",
    "            l_t = criterion(pred_t.view(-1, len(dataset.type_vocab)), y_t.view(-1))\n",
    "            \n",
    "            loss = l_d + l_dp + l_t\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        val_metrics = evaluate_hierarchical(model, val_loader, dataset, device, serve_type_id)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_metrics['type'])\n",
    "        \n",
    "        print(f\"Ep {epoch+1}/{epochs} | Loss: {avg_train_loss:.4f} | LR: {current_lr:.1e} | \"\n",
    "              f\"Val Dir: {val_metrics['dir']:.1f}% | \"\n",
    "              f\"Val Depth: {val_metrics['depth']:.1f}% | \"\n",
    "              f\"Val Type: {val_metrics['type']:.1f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T10:55:07.226602Z",
     "iopub.status.busy": "2025-12-18T10:55:07.225824Z",
     "iopub.status.idle": "2025-12-18T10:55:07.238263Z",
     "shell.execute_reply": "2025-12-18T10:55:07.237497Z",
     "shell.execute_reply.started": "2025-12-18T10:55:07.226574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T10:55:09.923344Z",
     "iopub.status.busy": "2025-12-18T10:55:09.922564Z",
     "iopub.status.idle": "2025-12-18T10:56:29.878073Z",
     "shell.execute_reply": "2025-12-18T10:56:29.877101Z",
     "shell.execute_reply.started": "2025-12-18T10:55:09.923282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_path = '/kaggle/input/atp-points/'\n",
    "\n",
    "    # List all point files to merge\n",
    "    point_files = [\n",
    "        base_path + 'charting-m-points-2020s.csv',\n",
    "        base_path + 'charting-m-points-2010s.csv',\n",
    "        base_path + 'charting-m-points-to-2009.csv'\n",
    "    ]\n",
    "    \n",
    "    # New Matches File\n",
    "    matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "    \n",
    "    atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "    wta_path = '/kaggle/input/atp-players/wta_players.csv'\n",
    "    \n",
    "    \n",
    "    dataset = HierarchicalTennisDataset(point_files, matches_path, atp_path, wta_path, max_seq_len=SEQ_LEN) \n",
    "    hierarchical_model = train_hierarchical_model(dataset, epochs=10, batch_size=512, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T10:56:38.163983Z",
     "iopub.status.busy": "2025-12-18T10:56:38.163212Z",
     "iopub.status.idle": "2025-12-18T10:56:38.170748Z",
     "shell.execute_reply": "2025-12-18T10:56:38.170137Z",
     "shell.execute_reply.started": "2025-12-18T10:56:38.163958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_standalone_checkpoint(checkpoint_path, dataset, device='cuda'):\n",
    "    print(f\"📂 Loading checkpoint from: {checkpoint_path}\")\n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        device = 'cpu'\n",
    "        \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    cfg = checkpoint.get(\"config\")\n",
    "    vocabs = checkpoint.get(\"vocabs\")\n",
    "    \n",
    "    if cfg is None or vocabs is None:\n",
    "        raise ValueError(\"Checkpoint is missing config/vocabs. Ensure it is the new Hierarchical version.\")\n",
    "\n",
    "    # 1. Re-Initialize Model with saved config\n",
    "    # This ensures the layer dimensions match exactly what was trained\n",
    "    model = HierarchicalCristianGPT(\n",
    "        dir_vocab_size=len(vocabs[\"dir_vocab\"]),\n",
    "        depth_vocab_size=len(vocabs[\"depth_vocab\"]),\n",
    "        type_vocab_size=len(vocabs[\"type_vocab\"]),\n",
    "        num_players=len(vocabs[\"player_vocab\"]),\n",
    "        context_dim=cfg[\"context_dim\"],\n",
    "        embed_dim=cfg[\"embed_dim\"],\n",
    "        n_head=cfg[\"n_head\"],\n",
    "        n_cycles=cfg[\"n_cycles\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # 2. Load Weights\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # 3. Sync Dataset Vocabs\n",
    "    print(\"🔄 Syncing dataset vocabularies with checkpoint...\")\n",
    "    dataset.type_vocab   = vocabs[\"type_vocab\"]\n",
    "    dataset.dir_vocab    = vocabs[\"dir_vocab\"]\n",
    "    dataset.depth_vocab  = vocabs[\"depth_vocab\"]\n",
    "    dataset.player_vocab = vocabs[\"player_vocab\"]\n",
    "    \n",
    "    # Update inverse lookups (needed for the decomposition step)\n",
    "    dataset.inv_type_vocab = {v: k for k, v in dataset.type_vocab.items()}\n",
    "    dataset.inv_dir_vocab  = {v: k for k, v in dataset.dir_vocab.items()}\n",
    "    dataset.inv_depth_vocab= {v: k for k, v in dataset.depth_vocab.items()}\n",
    "    \n",
    "    # 4. CRITICAL: Re-decompose data\n",
    "    # Since dataset was init'd with default vocabs, we must re-run this \n",
    "    # so the x_type/x_dir tensors map to the correct new IDs.\n",
    "    dataset._decompose_data()\n",
    "    \n",
    "    print(f\"✅ Model loaded and Dataset tensors refreshed. (Epoch {checkpoint.get('epoch', '?')})\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:32:39.911200Z",
     "iopub.status.busy": "2025-12-18T11:32:39.910674Z",
     "iopub.status.idle": "2025-12-18T11:36:20.703244Z",
     "shell.execute_reply": "2025-12-18T11:36:20.702654Z",
     "shell.execute_reply.started": "2025-12-18T11:32:39.911175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# --- 1. SHARED CONFIG & HELPERS ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_inverse_vocabs(dataset):\n",
    "    \"\"\"Creates reverse lookups for the hierarchical vocabs\"\"\"\n",
    "    inv_type  = {v: k for k, v in dataset.type_vocab.items()}\n",
    "    inv_dir   = {v: k for k, v in dataset.dir_vocab.items()}\n",
    "    inv_depth = {v: k for k, v in dataset.depth_vocab.items()}\n",
    "    return inv_type, inv_dir, inv_depth\n",
    "\n",
    "# --- 2. THE HIERARCHICAL EVALUATION FUNCTION ---\n",
    "def run_full_evaluation(model, dataset, loader, test_indices, \n",
    "                        live_samples=5000, \n",
    "                        length_matches=2000, \n",
    "                        freq_matches=2000, \n",
    "                        era_matches=50, \n",
    "                        speed_samples=10000):\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Starting Full Evaluation on {len(test_indices)} TEST SET samples...\")\n",
    "    \n",
    "    # Helpers & IDs\n",
    "    inv_type, inv_dir, inv_depth = get_inverse_vocabs(dataset)\n",
    "    serve_type_id = dataset.type_vocab.get('serve', dataset.type_vocab.get('s', -1))\n",
    "    unk_depth_id = dataset.depth_vocab.get('0', -1)\n",
    "    unk_dir_id = dataset.dir_vocab.get('0', -1)\n",
    "\n",
    "    # Pre-calculate Match-to-Index Map\n",
    "    match_map = {}\n",
    "    for idx in test_indices:\n",
    "        mid = dataset.sample_match_ids[idx]\n",
    "        match_map.setdefault(mid, []).append(idx)\n",
    "    unique_matches = list(match_map.keys())\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PART 1: OVERALL TACTICAL METRICS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 1: OVERALL TACTICAL METRICS \\n\" + \"=\"*40)\n",
    "    \n",
    "    all_p_type, all_t_type = [], []\n",
    "    all_p_dir,  all_t_dir  = [], []\n",
    "    all_p_depth, all_t_depth = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Inputs\n",
    "            x_z = batch['x_dir'].to(DEVICE)\n",
    "            x_t = batch['x_type'].to(DEVICE)\n",
    "            x_c = batch['context'].to(DEVICE)\n",
    "            x_s = batch['x_s_id'].to(DEVICE)\n",
    "            x_r = batch['x_r_id'].to(DEVICE)\n",
    "            \n",
    "            # Forward (3 Heads)\n",
    "            l_dir, l_depth, l_type = model(x_z, x_t, x_c, x_s, x_r)\n",
    "            \n",
    "            # Targets\n",
    "            y_type = batch['y_type'].to(DEVICE).view(-1)\n",
    "            y_dir  = batch['y_dir'].to(DEVICE).view(-1)\n",
    "            y_depth = batch['y_depth'].to(DEVICE).view(-1)\n",
    "\n",
    "            # --- 1. Type (Predicts everything except PAD and SERVES) ---\n",
    "            mask_t = (y_type != 0)\n",
    "            if serve_type_id != -1: mask_t &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_t.sum() > 0:\n",
    "                all_p_type.extend(l_type.argmax(-1).view(-1)[mask_t].cpu().numpy())\n",
    "                all_t_type.extend(y_type[mask_t].cpu().numpy())\n",
    "\n",
    "            # --- 2. Direction (Mask PAD, UNKNOWN, and SERVES) ---\n",
    "            mask_d = (y_dir != 0) \n",
    "            if unk_dir_id != -1: mask_d &= (y_dir != unk_dir_id)\n",
    "            if serve_type_id != -1: mask_d &= (y_type != serve_type_id)\n",
    "            \n",
    "            if mask_d.sum() > 0:\n",
    "                all_p_dir.extend(l_dir.argmax(-1).view(-1)[mask_d].cpu().numpy())\n",
    "                all_t_dir.extend(y_dir[mask_d].cpu().numpy())\n",
    "\n",
    "            # --- 3. Depth (Mask PAD, UNKNOWN, and SERVES) ---\n",
    "            mask_dp = (y_depth != 0)\n",
    "            if unk_depth_id != -1: mask_dp &= (y_depth != unk_depth_id)\n",
    "            if serve_type_id != -1: mask_dp &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_dp.sum() > 0:\n",
    "                all_p_depth.extend(l_depth.argmax(-1).view(-1)[mask_dp].cpu().numpy())\n",
    "                all_t_depth.extend(y_depth[mask_dp].cpu().numpy())\n",
    "\n",
    "    # Reports\n",
    "    print(\"\\n=== DIRECTION REPORT (Excluding Serves) ===\")\n",
    "    labels = [k for k in dataset.dir_vocab if k not in ['<pad>', '0']]\n",
    "    indices = [dataset.dir_vocab[k] for k in labels]\n",
    "    if indices:\n",
    "        print(classification_report(all_t_dir, all_p_dir, labels=indices, target_names=labels, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== DEPTH REPORT (Excluding Unknowns) ===\")\n",
    "    labels = [k for k in dataset.depth_vocab if k not in ['<pad>', '0']]\n",
    "    indices = [dataset.depth_vocab[k] for k in labels]\n",
    "    print(classification_report(all_t_depth, all_p_depth, labels=indices, target_names=labels, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== SHOT TYPE REPORT (Rally Only) ===\")\n",
    "    labels = [k for k in dataset.type_vocab if k not in ['<pad>', '<unk>', 'serve', 's']]\n",
    "    indices = [dataset.type_vocab[k] for k in labels]\n",
    "    # Filter to present\n",
    "    present = sorted(list(set(all_t_type)))\n",
    "    present_names = [inv_type[i] for i in present]\n",
    "    if present:\n",
    "        print(classification_report(all_t_type, all_p_type, labels=present, target_names=present_names, zero_division=0))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 2: LIVE SAMPLES (Showing {live_samples} Cases)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 2: LIVE SAMPLES (Showing {live_samples} Cases) \\n\" + \"=\"*40)\n",
    "    \n",
    "    selected_indices = random.sample(test_indices, min(live_samples * 2, len(test_indices)))\n",
    "    results_buffer = {3: [], 2:[], 1:[], 0:[]}\n",
    "    printed_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices:\n",
    "            if printed_count >= live_samples: break\n",
    "            \n",
    "            sample = dataset[idx]\n",
    "            non_zeros = (sample['x_type'] != 0).nonzero(as_tuple=True)[0]\n",
    "            if len(non_zeros) < 2: continue\n",
    "            \n",
    "            # Predict a random point\n",
    "            valid_indices = non_zeros.tolist()\n",
    "            t = random.choice(valid_indices)\n",
    "            \n",
    "            x_z = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            l_dir, l_depth, l_type = model(x_z, x_t, x_c, x_s, x_r)\n",
    "            \n",
    "            # --- Build History String ---\n",
    "            start_idx = valid_indices[0]\n",
    "            history_str = \"\"\n",
    "            for i in range(start_idx, t + 1):\n",
    "                typ_idx = sample['x_type'][i].item()\n",
    "                dir_idx = sample['x_dir'][i].item()\n",
    "                t_in = inv_type.get(typ_idx, '?')\n",
    "                z_in = inv_dir.get(dir_idx, '?')\n",
    "                \n",
    "                if i == start_idx:\n",
    "                    history_str += f\"[Serve {z_in}] \" if t_in in ['serve', 's'] else f\"[{t_in}{z_in}] \"\n",
    "                else:\n",
    "                    history_str += f\"-> {t_in}{z_in} \"\n",
    "            \n",
    "            # --- Get Prediction ---\n",
    "            probs = torch.softmax(l_type[0, t], dim=0)\n",
    "            conf = probs.max().item() * 100\n",
    "            \n",
    "            pred_t = l_type[0, t].argmax().item()\n",
    "            pred_d = l_dir[0, t].argmax().item()\n",
    "            pred_dp = l_depth[0, t].argmax().item()\n",
    "            \n",
    "            true_t = sample['y_type'][t].item()\n",
    "            true_d = sample['y_dir'][t].item()\n",
    "            true_dp = sample['y_depth'][t].item()\n",
    "            \n",
    "            if true_t == 0: continue\n",
    "\n",
    "            s_pred_d = inv_dir.get(pred_d, '?'); s_pred_t = inv_type.get(pred_t, '?')\n",
    "            s_true_d = inv_dir.get(true_d, '?'); s_true_t = inv_type.get(true_t, '?')\n",
    "            \n",
    "            check_d = \"✅\" if pred_d == true_d else \"❌\"\n",
    "            check_t = \"✅\" if pred_t == true_t else \"❌\"\n",
    "            check_dp = \"✅\" if pred_dp == true_dp else \"❌\"\n",
    "            \n",
    "            def d_lbl(x): return inv_depth.get(x, 'N/A')\n",
    "            \n",
    "            score = (1 if pred_d == true_d else 0) + (1 if pred_t == true_t else 0) + (1 if pred_dp == true_dp else 0)\n",
    "            m_id = dataset.sample_match_ids[idx]\n",
    "\n",
    "            out = []\n",
    "            out.append(f\"\\nMatch {m_id}:\")\n",
    "            out.append(f\"  History:    {history_str}\")\n",
    "            out.append(f\"  Prediction: {s_pred_t} to {s_pred_d} (Depth {d_lbl(pred_dp)}) | Conf: {conf:.0f}%\")\n",
    "            out.append(f\"  ACTUAL:     {s_true_t} to {s_true_d} (Depth {d_lbl(true_dp)}) | {check_t} Type {check_d} Dir {check_dp} Dep\")\n",
    "            \n",
    "            results_buffer[score].append(\"\\n\".join(out))\n",
    "            printed_count += 1\n",
    "\n",
    "    print_flag = False\n",
    "    \n",
    "    for s in [3,2,1,0]:\n",
    "        items = results_buffer[s]\n",
    "        if items:\n",
    "            print(f\"\\n{'='*20} {s}/3 CORRECT ({len(items)} cases) {'='*20}\")\n",
    "            if print_flag:\n",
    "                for item in items: \n",
    "                    print(item)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 3: GRANULAR ACCURACY VS RALLY LENGTH (Restored 3 Graphs)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 3: GRANULAR ACCURACY VS RALLY LENGTH \\n\" + \"=\"*40)\n",
    "    \n",
    "    # 3.1 Calculate Baselines\n",
    "    print(\"Calculating dataset baselines...\")\n",
    "    all_d, all_dp, all_tp = [], [], []\n",
    "    for i in test_indices:\n",
    "        yt = dataset[i]['y_type']\n",
    "        yd = dataset[i]['y_dir']\n",
    "        ydp = dataset[i]['y_depth']\n",
    "        for j in range(len(yt)):\n",
    "            if yt[j] == 0: continue\n",
    "            is_srv = (yt[j] == serve_type_id)\n",
    "            if not is_srv:\n",
    "                all_tp.append(yt[j].item())\n",
    "                if yd[j] != unk_dir_id and yd[j] != 0: all_d.append(yd[j].item())\n",
    "                if ydp[j] != unk_depth_id and ydp[j] != 0: all_dp.append(ydp[j].item())\n",
    "\n",
    "    def calc_baseline(data_list):\n",
    "        if not data_list: return 0.33\n",
    "        counts = Counter(data_list)\n",
    "        total = sum(counts.values())\n",
    "        return sum([(c/total)**2 for c in counts.values()])\n",
    "\n",
    "    base_d = calc_baseline(all_d)\n",
    "    base_dp = calc_baseline(all_dp)\n",
    "    base_tp = calc_baseline(all_tp)\n",
    "    base_pair_avg = (base_d*base_dp + base_d*base_tp + base_tp*base_dp) / 3\n",
    "    base_whole = base_d * base_dp * base_tp\n",
    "    print(f\"Baselines -> Dir: {base_d:.2f}, Depth: {base_dp:.2f}, Type: {base_tp:.2f}, Whole: {base_whole:.4f}\")\n",
    "    \n",
    "    # 3.2 Analysis Loop\n",
    "    chosen_matches = random.sample(unique_matches, min(length_matches, len(unique_matches)))\n",
    "    rl_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    print(f\"Analyzing {len(rl_indices)} points from {len(chosen_matches)} matches...\")\n",
    "\n",
    "    results_p3 = []\n",
    "    with torch.no_grad():\n",
    "        for idx in rl_indices:\n",
    "            sample = dataset[idx]\n",
    "            x_z = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y_t = sample['y_type']\n",
    "            y_d = sample['y_dir']\n",
    "            y_dp = sample['y_depth']\n",
    "\n",
    "            l_dir, l_depth, l_type = model(x_z, x_t, x_c, x_s, x_r)\n",
    "            preds_t = l_type.argmax(dim=-1).squeeze(0)\n",
    "            preds_d = l_dir.argmax(dim=-1).squeeze(0)\n",
    "            preds_dp = l_depth.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            x_seq_cpu = sample['x_type']\n",
    "            \n",
    "            for t_step in range(len(y_t)):\n",
    "                if y_t[t_step] == 0: continue\n",
    "                # Skip serves for rally analysis\n",
    "                if y_t[t_step] == serve_type_id: continue\n",
    "\n",
    "                shot_num = (x_seq_cpu[:t_step+1] != 0).sum().item() + 1\n",
    "                \n",
    "                pt, p_d, pdp = preds_t[t_step].item(), preds_d[t_step].item(), preds_dp[t_step].item()\n",
    "                tt, td, tdp = y_t[t_step].item(), y_d[t_step].item(), y_dp[t_step].item()\n",
    "\n",
    "                ok_t = (pt == tt)\n",
    "                has_dir = (td != 0 and td != unk_dir_id)\n",
    "                has_depth = (tdp != 0 and tdp != unk_depth_id)\n",
    "                ok_d = (p_d == td) if has_dir else False\n",
    "                ok_dp = (pdp == tdp) if has_depth else False\n",
    "\n",
    "                # Logic Split - Single\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Type', 'Type': 'Single', 'Accuracy': 1 if ok_t else 0})\n",
    "                if has_dir:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Direction', 'Type': 'Single', 'Accuracy': 1 if ok_d else 0})\n",
    "                if has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Depth', 'Type': 'Single', 'Accuracy': 1 if ok_dp else 0})\n",
    "                \n",
    "                # Logic Split - Pair\n",
    "                if has_dir:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Type', 'Type': 'Pair', 'Accuracy': 1 if (ok_d and ok_t) else 0})\n",
    "                if has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Type + Depth', 'Type': 'Pair', 'Accuracy': 1 if (ok_dp and ok_t) else 0})\n",
    "                if has_dir and has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Depth', 'Type': 'Pair', 'Accuracy': 1 if (ok_d and ok_dp) else 0})\n",
    "                \n",
    "                # Logic Split - Whole\n",
    "                if has_dir and has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Whole Shot', 'Type': 'Whole', 'Accuracy': 1 if (ok_d and ok_dp and ok_t) else 0})\n",
    "\n",
    "    if results_p3:\n",
    "        df = pd.DataFrame(results_p3)\n",
    "        df = df[(df['Shot_Number'] <= 12) & (df['Shot_Number'] >= 2)]\n",
    "        \n",
    "        palette_single = {'Direction': '#1f77b4', 'Depth': '#d62728', 'Type': '#2ca02c'}\n",
    "        palette_pair   = {'Dir + Depth': '#9467bd', 'Dir + Type': '#17becf', 'Type + Depth': '#ff7f0e'}\n",
    "        palette_whole  = {'Whole Shot': '#000000'}\n",
    "\n",
    "        def setup_plot(title, baseline, base_label):\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.xlabel('Shot Number', fontsize=12)\n",
    "            plt.xticks(np.arange(2, 13, 1))\n",
    "            \n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "            ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.02))\n",
    "            plt.grid(True, which='major', axis='y', linestyle='-', linewidth=0.75, color='grey', alpha=0.6)\n",
    "            plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.5, color='grey', alpha=0.3)\n",
    "            plt.ylim(0.0, 1.0)\n",
    "            \n",
    "            if baseline:\n",
    "                plt.axhline(baseline, color='#FF1493', linestyle=':', alpha=0.8, linewidth=2, label=base_label)\n",
    "\n",
    "        # Graph 1: Single\n",
    "        setup_plot('Single Task Accuracy vs. Rally Length', base_tp, f'Random Type ({base_tp:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Single'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_single, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='lower right'); plt.show()\n",
    "        \n",
    "        # Graph 2: Pairwise\n",
    "        setup_plot('Pairwise Accuracy vs. Rally Length', base_pair_avg, f'Random Pair (~{base_pair_avg:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Pair'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_pair, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "        # Graph 3: Whole Shot\n",
    "        setup_plot('Whole Shot Accuracy vs. Rally Length', base_whole, f'Random Whole ({base_whole:.4f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Whole'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes={'Whole Shot':(2,2)}, palette=palette_whole, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 4: PLAYER FREQUENCY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 4: PLAYER FREQUENCY ({freq_matches} Matches) \\n\" + \"=\"*40)\n",
    "    chosen_matches = random.sample(unique_matches, min(freq_matches, len(unique_matches)))\n",
    "    pf_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    \n",
    "    p_counts = Counter()\n",
    "    p_stats = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in pf_indices:\n",
    "            sample = dataset[idx]\n",
    "            x_z = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y_t = sample['y_type']\n",
    "            \n",
    "            _, _, l_t = model(x_z, x_t, x_c, x_s, x_r)\n",
    "            preds = l_t.argmax(-1).squeeze(0)\n",
    "            \n",
    "            s_val, r_val = sample['x_s_id'].item(), sample['x_r_id'].item()\n",
    "            x_seq_cpu = sample['x_type']\n",
    "            \n",
    "            for t in range(len(y_t)):\n",
    "                if y_t[t] == 0: continue\n",
    "                if y_t[t] == serve_type_id: continue # Skip serves\n",
    "\n",
    "                hist_len = (x_seq_cpu[:t+1] != 0).sum().item()\n",
    "                actor = s_val if (hist_len + 1) % 2 != 0 else r_val\n",
    "                if actor <= 1: continue\n",
    "                \n",
    "                p_counts[actor] += 1\n",
    "                if actor not in p_stats: p_stats[actor] = {'tot': 0, 'corr': 0}\n",
    "                p_stats[actor]['tot'] += 1\n",
    "                if preds[t].item() == y_t[t].item(): p_stats[actor]['corr'] += 1\n",
    "\n",
    "    pf_data = [{'Freq': p_counts[a], 'Err': (1 - v['corr']/v['tot'])*100} for a, v in p_stats.items() if p_counts[a] > 10]\n",
    "    if pf_data:\n",
    "        df_pf = pd.DataFrame(pf_data)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.regplot(data=df_pf, x='Freq', y='Err', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        plt.xscale('log'); plt.title(f\"Type Error vs Frequency (Corr: {df_pf['Freq'].corr(df_pf['Err']):.2f})\"); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 5: ERA STABILITY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 5: ERA STABILITY ({era_matches} Matches/Era) \\n\" + \"=\"*40)\n",
    "    eras = {'Pre-2010': [], '2010-2019': [], '2020+': []}\n",
    "    for m_id in unique_matches:\n",
    "        try: y_year = int(str(m_id)[:4])\n",
    "        except: continue\n",
    "        if y_year < 2010: eras['Pre-2010'].append(m_id)\n",
    "        elif y_year < 2020: eras['2010-2019'].append(m_id)\n",
    "        else: eras['2020+'].append(m_id)\n",
    "\n",
    "    era_indices = []\n",
    "    era_labels_list = []\n",
    "    for era_name, m_list in eras.items():\n",
    "        if not m_list: continue\n",
    "        chosen = random.sample(m_list, min(era_matches, len(m_list)))\n",
    "        for m in chosen:\n",
    "            era_indices.extend(match_map[m])\n",
    "            era_labels_list.extend([era_name]*len(match_map[m]))\n",
    "            \n",
    "    era_res = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(era_indices):\n",
    "            sample = dataset[idx]\n",
    "            x_z = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y_t = sample['y_type'].to(DEVICE)\n",
    "            y_d = sample['y_dir'].to(DEVICE)\n",
    "            y_dp = sample['y_depth'].to(DEVICE)\n",
    "            \n",
    "            l_d, l_dp, l_t = model(x_z, x_t, x_c, x_s, x_r)\n",
    "            p_d = l_d.argmax(-1).squeeze(0)\n",
    "            p_t = l_t.argmax(-1).squeeze(0)\n",
    "            p_dp = l_dp.argmax(-1).squeeze(0)\n",
    "            \n",
    "            mask = (y_t > 1) & (y_t != serve_type_id) & (y_d != unk_dir_id) & (y_dp != unk_depth_id)\n",
    "            if mask.sum() > 0:\n",
    "                correct = (p_t[mask] == y_t[mask]) & (p_d[mask] == y_d[mask]) & (p_dp[mask] == y_dp[mask])\n",
    "                acc = correct.float().mean().item()\n",
    "                era_res.append({'Era': era_labels_list[i], 'Whole Shot Acc': acc})\n",
    "    \n",
    "    if era_res:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(data=pd.DataFrame(era_res), x='Era', y='Whole Shot Acc', palette='viridis', order=['Pre-2010', '2010-2019', '2020+'])\n",
    "        plt.title('Whole Shot Accuracy by Era'); plt.ylim(0, 1); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 6: RAW ERROR ANALYSIS BY SURFACE (Masked Depth)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" RAW ERROR ANALYSIS BY SURFACE (HierarchicalCristianGPT) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # 1. Group Test Indices by Surface\n",
    "    surface_map = {'Clay': [], 'Hard': [], 'Grass': []}\n",
    "    for idx in test_indices:\n",
    "        surf = dataset.match_meta.get(dataset.sample_match_ids[idx], {}).get('surface', 'Hard')\n",
    "        found = False\n",
    "        for k in surface_map: \n",
    "            if k in surf: \n",
    "                surface_map[k].append(idx)\n",
    "                found = True\n",
    "                break\n",
    "        if not found: surface_map['Hard'].append(idx)\n",
    "            \n",
    "    # 2. Select Samples Balanced by Surface\n",
    "    selected_indices, surface_labels = [], []\n",
    "    per_surf = speed_samples // 3 \n",
    "    \n",
    "    for s, inds in surface_map.items():\n",
    "        if not inds: continue\n",
    "        chosen = random.sample(inds, min(len(inds), per_surf))\n",
    "        selected_indices.extend(chosen)\n",
    "        surface_labels.extend([s]*len(chosen))\n",
    "        \n",
    "    # 3. Evaluation Loop\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            sample = dataset[idx]\n",
    "            surf = surface_labels[i]\n",
    "            \n",
    "            x_dir  = sample['x_dir'].unsqueeze(0).to(DEVICE)\n",
    "            x_type = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_c    = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s    = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r    = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            y_t_gt = sample['y_type'].to(DEVICE)\n",
    "            y_d_gt = sample['y_dir'].to(DEVICE)\n",
    "            y_dp_gt = sample['y_depth'].to(DEVICE)\n",
    "            \n",
    "            logits_dir, logits_depth, logits_type_out = model(x_dir, x_type, x_c, x_s, x_r)\n",
    "            \n",
    "            pred_t = logits_type_out.argmax(dim=-1).squeeze(0)\n",
    "            pred_d = logits_dir.argmax(dim=-1).squeeze(0)\n",
    "            pred_dp = logits_depth.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            seq_len = x_dir.shape[1]\n",
    "            for t in range(seq_len):\n",
    "                if y_t_gt[t] == 0: continue \n",
    "                \n",
    "                p_t, p_d, p_dp = pred_t[t].item(), pred_d[t].item(), pred_dp[t].item()\n",
    "                t_t, t_d, t_dp = y_t_gt[t].item(), y_d_gt[t].item(), y_dp_gt[t].item()\n",
    "                \n",
    "                whole_shot_miss = (p_t != t_t) or (p_d != t_d) or (p_dp != t_dp)\n",
    "\n",
    "                # --- DEPTH MASKING FIX ---\n",
    "                # Only calculate Depth Error if the target actually HAS depth (is not 0)\n",
    "                if t_dp != 0:\n",
    "                    depth_err = 1.0 if p_dp != t_dp else 0.0\n",
    "                else:\n",
    "                    depth_err = None \n",
    "\n",
    "                results.append({\n",
    "                    'Surface': surf,\n",
    "                    'Type Error': 1.0 if p_t != t_t else 0.0,\n",
    "                    'Direction Error': 1.0 if p_d != t_d else 0.0,\n",
    "                    'Depth Error': depth_err,  # <--- Masked\n",
    "                    'Whole Shot Error': 1.0 if whole_shot_miss else 0.0\n",
    "                })\n",
    "                \n",
    "    # 4. Statistics & Plotting\n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print Table (Mean Error %) - Pandas ignores None in mean()\n",
    "    stats = df.groupby('Surface')[['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error']].mean() * 100\n",
    "    print(\"\\n--- Mean Error Rates (%) [Depth calc only on non-zero targets] ---\")\n",
    "    print(stats.round(2))\n",
    "    \n",
    "    # Plotting\n",
    "    df_melt = df.melt(id_vars=['Surface'], \n",
    "                      value_vars=['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error'], \n",
    "                      value_name='Error Rate')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Surface', y='Error Rate', hue='variable', \n",
    "                order=['Clay', 'Hard', 'Grass'], palette='viridis')\n",
    "    plt.title('Error Rates by Component (Masked Depth) vs. Whole Shot')\n",
    "    plt.ylabel('Error Rate (0.0 - 1.0)')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "# --- 3. RUNNER SNIPPET ---\n",
    "print(\"Recreating validation/test split for evaluation...\")\n",
    "checkpoint_path=\"/kaggle/input/hierarchical-best/other/default/1/hierarchical_best.pth\"\n",
    "\n",
    "dataset = HierarchicalTennisDataset(point_files, matches_path, atp_path, wta_path, max_seq_len=SEQ_LEN)\n",
    "\n",
    "hierarchical_model = load_standalone_checkpoint(checkpoint_path, dataset, device=DEVICE)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "total_len = len(dataset)\n",
    "train_len = int(0.8 * total_len)\n",
    "val_len = int(0.1 * total_len)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "_, _, test_ds = random_split(dataset, [train_len, val_len, test_len], generator=gen)\n",
    "\n",
    "test_indices = test_ds.indices\n",
    "test_loader_eval = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "run_full_evaluation(\n",
    "    model=hierarchical_model, \n",
    "    dataset=dataset, \n",
    "    loader=test_loader_eval, \n",
    "    test_indices=test_indices,\n",
    "    live_samples=5000, \n",
    "    length_matches=2000,\n",
    "    freq_matches=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYBRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-HEAD RICH LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNSAMPLED DATASET PER COMPARARE UOMINI E DONNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:19:30.396388Z",
     "iopub.status.busy": "2025-12-18T21:19:30.395534Z",
     "iopub.status.idle": "2025-12-18T21:19:30.418446Z",
     "shell.execute_reply": "2025-12-18T21:19:30.417575Z",
     "shell.execute_reply.started": "2025-12-18T21:19:30.396349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# --- 2. Helper to Calculate Accuracy ---\n",
    "def calculate_accuracy(l_type, l_dir, l_depth, y_type, y_dir, y_depth):\n",
    "    \"\"\"\n",
    "    Computes accuracy for all three heads combined (Average of the 3 accuracies).\n",
    "    Ignores padding (0).\n",
    "    \"\"\"\n",
    "    # Get Predictions\n",
    "    p_t = l_type.argmax(dim=-1)\n",
    "    p_d = l_dir.argmax(dim=-1)\n",
    "    p_dp = l_depth.argmax(dim=-1)\n",
    "\n",
    "    # Masks (Ignore 0/Padding)\n",
    "    mask_t = y_type != 0\n",
    "    mask_d = y_dir != 0\n",
    "    mask_dp = y_depth != 0\n",
    "\n",
    "    # Calculate Corrects\n",
    "    acc_t = (p_t[mask_t] == y_type[mask_t]).float().mean().item() if mask_t.sum() > 0 else 0\n",
    "    acc_d = (p_d[mask_d] == y_dir[mask_d]).float().mean().item() if mask_d.sum() > 0 else 0\n",
    "    acc_dp = (p_dp[mask_dp] == y_depth[mask_dp]).float().mean().item() if mask_dp.sum() > 0 else 0\n",
    "\n",
    "    return acc_t, acc_d, acc_dp\n",
    "\n",
    "def train_hybrid_model(dataset, epochs=10, batch_size=64, lr=1e-3, device='cuda'):\n",
    "    print(\"--- STARTING HYBRID (PARALLEL HEADS) TRAINING ---\")\n",
    "    \n",
    "    # Setup DataLoaders\n",
    "    # 80/15/5\n",
    "    train_len = int(0.8 * len(dataset))\n",
    "    val_len = int(0.15 * len(dataset))\n",
    "    test_len = len(dataset) - train_len - val_len\n",
    "    \n",
    "    train_ds, val_ds, _ = torch.utils.data.random_split(\n",
    "        dataset, [train_len, val_len, test_len],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = HybridRichLSTM(\n",
    "        num_players=len(dataset.player_vocab),\n",
    "        type_vocab_size=len(dataset.type_vocab),\n",
    "        dir_vocab_size=len(dataset.dir_vocab),\n",
    "        depth_vocab_size=len(dataset.depth_vocab),\n",
    "        context_dim=10\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Loss Function (ignores padding 0)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Early Stopping & Checkpointing Vars\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        t_acc_t, t_acc_d, t_acc_dp = 0, 0, 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move to device\n",
    "            x_t, x_d, x_dp = batch['x_type'].to(device), batch['x_dir'].to(device), batch['x_depth'].to(device)\n",
    "            s_id, r_id, ctx = batch['x_s_id'].to(device), batch['x_r_id'].to(device), batch['context'].to(device)\n",
    "            y_t, y_d, y_dp = batch['y_type'].to(device), batch['y_dir'].to(device), batch['y_depth'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l_t, l_d, l_dp = model(x_t, x_d, x_dp, s_id, r_id, ctx)\n",
    "            \n",
    "            # Loss\n",
    "            loss_t = criterion(l_t.view(-1, len(dataset.type_vocab)), y_t.view(-1))\n",
    "            loss_d = criterion(l_d.view(-1, len(dataset.dir_vocab)), y_d.view(-1))\n",
    "            loss_dp= criterion(l_dp.view(-1, len(dataset.depth_vocab)), y_dp.view(-1))\n",
    "            loss = loss_t + loss_d + loss_dp\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Train Acc (Batch)\n",
    "            at, ad, adp = calculate_accuracy(l_t, l_d, l_dp, y_t, y_d, y_dp)\n",
    "            t_acc_t += at; t_acc_d += ad; t_acc_dp += adp\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_t_acc = (t_acc_t + t_acc_d + t_acc_dp) / (3 * len(train_loader)) # Average of 3 tasks\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        v_acc_t, v_acc_d, v_acc_dp = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_t, x_d, x_dp = batch['x_type'].to(device), batch['x_dir'].to(device), batch['x_depth'].to(device)\n",
    "                s_id, r_id, ctx = batch['x_s_id'].to(device), batch['x_r_id'].to(device), batch['context'].to(device)\n",
    "                y_t, y_d, y_dp = batch['y_type'].to(device), batch['y_dir'].to(device), batch['y_depth'].to(device)\n",
    "                \n",
    "                l_t, l_d, l_dp = model(x_t, x_d, x_dp, s_id, r_id, ctx)\n",
    "                \n",
    "                loss_t = criterion(l_t.view(-1, len(dataset.type_vocab)), y_t.view(-1))\n",
    "                loss_d = criterion(l_d.view(-1, len(dataset.dir_vocab)), y_d.view(-1))\n",
    "                loss_dp= criterion(l_dp.view(-1, len(dataset.depth_vocab)), y_dp.view(-1))\n",
    "                val_loss += (loss_t + loss_d + loss_dp).item()\n",
    "\n",
    "                at, ad, adp = calculate_accuracy(l_t, l_d, l_dp, y_t, y_d, y_dp)\n",
    "                v_acc_t += at; v_acc_d += ad; v_acc_dp += adp\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_v_acc = (v_acc_t + v_acc_d + v_acc_dp) / (3 * len(val_loader))\n",
    "\n",
    "        # --- REPORTING ---\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} Acc: {avg_t_acc:.2%} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} Acc: {avg_v_acc:.2%}\")\n",
    "\n",
    "        # --- EARLY STOPPING & CHECKPOINT ---\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            # Save checkpoint (overwrite each time performance improves)\n",
    "            torch.save(best_model_wts, 'hybrid_rich_lstm.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 5:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "                \n",
    "    # Load best weights\n",
    "    print(\"Loading best model weights...\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:19:32.655345Z",
     "iopub.status.busy": "2025-12-18T21:19:32.654618Z",
     "iopub.status.idle": "2025-12-18T21:39:42.935370Z",
     "shell.execute_reply": "2025-12-18T21:39:42.934540Z",
     "shell.execute_reply.started": "2025-12-18T21:19:32.655322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "atp_path = '/kaggle/input/atp-players/atp_players.csv'\n",
    "wta_path = '/kaggle/input/wta-players/wta_players.csv'\n",
    "\n",
    "\n",
    "base_path = '/kaggle/input/atp-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-m-points-2020s.csv',\n",
    "    base_path + 'charting-m-points-2010s.csv',\n",
    "    base_path + 'charting-m-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/atp-matches-updated/charting-m-matches-updated.csv'\n",
    "\n",
    "'''\n",
    "base_path = '/kaggle/input/wta-points/'\n",
    "\n",
    "# List all point files to merge\n",
    "point_files = [\n",
    "    base_path + 'charting-w-points-2020s.csv',\n",
    "    base_path + 'charting-w-points-2010s.csv',\n",
    "    base_path + 'charting-w-points-to-2009.csv'\n",
    "]\n",
    "\n",
    "# New Matches File\n",
    "matches_path = '/kaggle/input/wta-matches/charting-w-matches.csv'\n",
    "'''\n",
    "\n",
    "dataset = DownsampledHierarchical(point_files, matches_path, atp_path, wta_path, max_seq_len=SEQ_LEN) \n",
    "\n",
    "rich_hybrid_LSTM = train_hybrid_model(dataset, epochs=15, batch_size=64, lr=1e-3, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:41:42.682384Z",
     "iopub.status.busy": "2025-12-18T21:41:42.681613Z",
     "iopub.status.idle": "2025-12-18T21:43:12.603713Z",
     "shell.execute_reply": "2025-12-18T21:43:12.603001Z",
     "shell.execute_reply.started": "2025-12-18T21:41:42.682360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "# --- 1. SHARED CONFIG & HELPERS ---\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_inverse_vocabs(dataset):\n",
    "    \"\"\"Creates reverse lookups for the hierarchical vocabs\"\"\"\n",
    "    inv_type  = {v: k for k, v in dataset.type_vocab.items()}\n",
    "    inv_dir   = {v: k for k, v in dataset.dir_vocab.items()}\n",
    "    inv_depth = {v: k for k, v in dataset.depth_vocab.items()}\n",
    "    return inv_type, inv_dir, inv_depth\n",
    "\n",
    "# Helper for seed\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --- 2. THE HIERARCHICAL EVALUATION FUNCTION (ADAPTED) ---\n",
    "def run_full_evaluation(model, dataset, loader, test_indices, \n",
    "                        live_samples=5000, \n",
    "                        length_matches=2000, \n",
    "                        freq_matches=2000, \n",
    "                        era_matches=50, \n",
    "                        speed_samples=10000):\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Starting Full Evaluation on {len(test_indices)} TEST SET samples...\")\n",
    "    \n",
    "    # Helpers & IDs\n",
    "    inv_type, inv_dir, inv_depth = get_inverse_vocabs(dataset)\n",
    "    serve_type_id = dataset.type_vocab.get('serve', dataset.type_vocab.get('s', -1))\n",
    "    unk_depth_id = dataset.depth_vocab.get('0', -1)\n",
    "    unk_dir_id = dataset.dir_vocab.get('0', -1)\n",
    "\n",
    "    # Pre-calculate Match-to-Index Map\n",
    "    match_map = {}\n",
    "    for idx in test_indices:\n",
    "        mid = dataset.sample_match_ids[idx]\n",
    "        match_map.setdefault(mid, []).append(idx)\n",
    "    unique_matches = list(match_map.keys())\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PART 1: OVERALL TACTICAL METRICS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 1: OVERALL TACTICAL METRICS \\n\" + \"=\"*40)\n",
    "    \n",
    "    all_p_type, all_t_type = [], []\n",
    "    all_p_dir,  all_t_dir  = [], []\n",
    "    all_p_depth, all_t_depth = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Inputs (ADAPTED to include depth and match new model input names/order)\n",
    "            x_d = batch['x_dir'].to(DEVICE) # Direction Input\n",
    "            x_t = batch['x_type'].to(DEVICE) # Type Input\n",
    "            x_dp = batch['x_depth'].to(DEVICE) # DEPTH Input (New)\n",
    "            x_c = batch['context'].to(DEVICE)\n",
    "            x_s = batch['x_s_id'].to(DEVICE)\n",
    "            x_r = batch['x_r_id'].to(DEVICE)\n",
    "            \n",
    "            # Forward (3 Heads) - Model returns (Type, Dir, Depth)\n",
    "            l_type_pred, l_dir_pred, l_depth_pred = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "            \n",
    "            # Re-assign to match original variable names in evaluation logic\n",
    "            l_type = l_type_pred\n",
    "            l_dir = l_dir_pred\n",
    "            l_depth = l_depth_pred\n",
    "            \n",
    "            # Targets\n",
    "            y_type = batch['y_type'].to(DEVICE).view(-1)\n",
    "            y_dir  = batch['y_dir'].to(DEVICE).view(-1)\n",
    "            y_depth = batch['y_depth'].to(DEVICE).view(-1)\n",
    "\n",
    "            # --- 1. Type (Predicts everything except PAD and SERVES) ---\n",
    "            mask_t = (y_type != 0)\n",
    "            if serve_type_id != -1: mask_t &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_t.sum() > 0:\n",
    "                all_p_type.extend(l_type.argmax(-1).view(-1)[mask_t].cpu().numpy())\n",
    "                all_t_type.extend(y_type[mask_t].cpu().numpy())\n",
    "\n",
    "            # --- 2. Direction (Mask PAD, UNKNOWN, and SERVES) ---\n",
    "            mask_d = (y_dir != 0) \n",
    "            if unk_dir_id != -1: mask_d &= (y_dir != unk_dir_id)\n",
    "            if serve_type_id != -1: mask_d &= (y_type != serve_type_id)\n",
    "            \n",
    "            if mask_d.sum() > 0:\n",
    "                all_p_dir.extend(l_dir.argmax(-1).view(-1)[mask_d].cpu().numpy())\n",
    "                all_t_dir.extend(y_dir[mask_d].cpu().numpy())\n",
    "\n",
    "            # --- 3. Depth (Mask PAD, UNKNOWN, and SERVES) ---\n",
    "            mask_dp = (y_depth != 0)\n",
    "            if unk_depth_id != -1: mask_dp &= (y_depth != unk_depth_id)\n",
    "            if serve_type_id != -1: mask_dp &= (y_type != serve_type_id)\n",
    "\n",
    "            if mask_dp.sum() > 0:\n",
    "                all_p_depth.extend(l_depth.argmax(-1).view(-1)[mask_dp].cpu().numpy())\n",
    "                all_t_depth.extend(y_depth[mask_dp].cpu().numpy())\n",
    "\n",
    "    # Reports\n",
    "    print(\"\\n=== DIRECTION REPORT (Excluding Serves) ===\")\n",
    "    labels = [k for k in dataset.dir_vocab if k not in ['<pad>', '0']]\n",
    "    indices = [dataset.dir_vocab[k] for k in labels]\n",
    "    if indices:\n",
    "        print(classification_report(all_t_dir, all_p_dir, labels=indices, target_names=labels, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== DEPTH REPORT (Excluding Unknowns) ===\")\n",
    "    labels = [k for k in dataset.depth_vocab if k not in ['<pad>', '0']]\n",
    "    indices = [dataset.depth_vocab[k] for k in labels]\n",
    "    print(classification_report(all_t_depth, all_p_depth, labels=indices, target_names=labels, zero_division=0))\n",
    "\n",
    "    print(\"\\n=== SHOT TYPE REPORT (Rally Only) ===\")\n",
    "    labels = [k for k in dataset.type_vocab if k not in ['<pad>', '<unk>', 'serve', 's']]\n",
    "    indices = [dataset.type_vocab[k] for k in labels]\n",
    "    # Filter to present\n",
    "    present = sorted(list(set(all_t_type)))\n",
    "    present_names = [inv_type[i] for i in present]\n",
    "    if present:\n",
    "        print(classification_report(all_t_type, all_p_type, labels=present, target_names=present_names, zero_division=0))\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 2: LIVE SAMPLES (Showing {live_samples} Cases)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 2: LIVE SAMPLES (Showing {live_samples} Cases) \\n\" + \"=\"*40)\n",
    "    \n",
    "    selected_indices = random.sample(test_indices, min(live_samples * 2, len(test_indices)))\n",
    "    results_buffer = {3: [], 2:[], 1:[], 0:[]}\n",
    "    printed_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in selected_indices:\n",
    "            if printed_count >= live_samples: break\n",
    "            \n",
    "            sample = dataset[idx]\n",
    "            non_zeros = (sample['x_type'] != 0).nonzero(as_tuple=True)[0]\n",
    "            if len(non_zeros) < 2: continue\n",
    "            \n",
    "            # Predict a random point\n",
    "            valid_indices = non_zeros.tolist()\n",
    "            t = random.choice(valid_indices)\n",
    "            \n",
    "            # Inputs (ADAPTED)\n",
    "            x_d = sample['x_dir'].unsqueeze(0).to(DEVICE) # Direction Input (was x_z)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE) # DEPTH Input (New)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # Forward (3 Heads) - Model returns (Type, Dir, Depth)\n",
    "            l_type_pred, l_dir_pred, l_depth_pred = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "            \n",
    "            # Re-assign to match original variable names\n",
    "            l_type = l_type_pred\n",
    "            l_dir = l_dir_pred\n",
    "            l_depth = l_depth_pred\n",
    "            \n",
    "            # ... [Rest of Part 2 logic remains the same]\n",
    "            # --- Build History String ---\n",
    "            start_idx = valid_indices[0]\n",
    "            history_str = \"\"\n",
    "            for i in range(start_idx, t + 1):\n",
    "                typ_idx = sample['x_type'][i].item()\n",
    "                dir_idx = sample['x_dir'][i].item()\n",
    "                t_in = inv_type.get(typ_idx, '?')\n",
    "                z_in = inv_dir.get(dir_idx, '?')\n",
    "                \n",
    "                if i == start_idx:\n",
    "                    history_str += f\"[Serve {z_in}] \" if t_in in ['serve', 's'] else f\"[{t_in}{z_in}] \"\n",
    "                else:\n",
    "                    history_str += f\"-> {t_in}{z_in} \"\n",
    "            \n",
    "            # --- Get Prediction ---\n",
    "            probs = torch.softmax(l_type[0, t], dim=0)\n",
    "            conf = probs.max().item() * 100\n",
    "            \n",
    "            pred_t = l_type[0, t].argmax().item()\n",
    "            pred_d = l_dir[0, t].argmax().item()\n",
    "            pred_dp = l_depth[0, t].argmax().item()\n",
    "            \n",
    "            true_t = sample['y_type'][t].item()\n",
    "            true_d = sample['y_dir'][t].item()\n",
    "            true_dp = sample['y_depth'][t].item()\n",
    "            \n",
    "            if true_t == 0: continue\n",
    "\n",
    "            s_pred_d = inv_dir.get(pred_d, '?'); s_pred_t = inv_type.get(pred_t, '?')\n",
    "            s_true_d = inv_dir.get(true_d, '?'); s_true_t = inv_type.get(true_t, '?')\n",
    "            \n",
    "            check_d = \"✅\" if pred_d == true_d else \"❌\"\n",
    "            check_t = \"✅\" if pred_t == true_t else \"❌\"\n",
    "            check_dp = \"✅\" if pred_dp == true_dp else \"❌\"\n",
    "            \n",
    "            def d_lbl(x): return inv_depth.get(x, 'N/A')\n",
    "            \n",
    "            score = (1 if pred_d == true_d else 0) + (1 if pred_t == true_t else 0) + (1 if pred_dp == true_dp else 0)\n",
    "            m_id = dataset.sample_match_ids[idx]\n",
    "\n",
    "            out = []\n",
    "            out.append(f\"\\nMatch {m_id}:\")\n",
    "            out.append(f\"  History:    {history_str}\")\n",
    "            out.append(f\"  Prediction: {s_pred_t} to {s_pred_d} (Depth {d_lbl(pred_dp)}) | Conf: {conf:.0f}%\")\n",
    "            out.append(f\"  ACTUAL:     {s_true_t} to {s_true_d} (Depth {d_lbl(true_dp)}) | {check_t} Type {check_d} Dir {check_dp} Dep\")\n",
    "            \n",
    "            results_buffer[score].append(\"\\n\".join(out))\n",
    "            printed_count += 1\n",
    "\n",
    "    print_flag = False\n",
    "    \n",
    "    for s in [3,2,1,0]:\n",
    "        items = results_buffer[s]\n",
    "        if items:\n",
    "            print(f\"\\n{'='*20} {s}/3 CORRECT ({len(items)} cases) {'='*20}\")\n",
    "            if print_flag:\n",
    "                for item in items: \n",
    "                    print(item)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 3: GRANULAR ACCURACY VS RALLY LENGTH (Restored 3 Graphs)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n PART 3: GRANULAR ACCURACY VS RALLY LENGTH \\n\" + \"=\"*40)\n",
    "    \n",
    "    # 3.1 Calculate Baselines\n",
    "    print(\"Calculating dataset baselines...\")\n",
    "    all_d, all_dp, all_tp = [], [], []\n",
    "    for i in test_indices:\n",
    "        yt = dataset[i]['y_type']\n",
    "        yd = dataset[i]['y_dir']\n",
    "        ydp = dataset[i]['y_depth']\n",
    "        for j in range(len(yt)):\n",
    "            if yt[j] == 0: continue\n",
    "            is_srv = (yt[j] == serve_type_id)\n",
    "            if not is_srv:\n",
    "                all_tp.append(yt[j].item())\n",
    "                if yd[j] != unk_dir_id and yd[j] != 0: all_d.append(yd[j].item())\n",
    "                if ydp[j] != unk_depth_id and ydp[j] != 0: all_dp.append(ydp[j].item())\n",
    "\n",
    "    def calc_baseline(data_list):\n",
    "        if not data_list: return 0.33\n",
    "        counts = Counter(data_list)\n",
    "        total = sum(counts.values())\n",
    "        return sum([(c/total)**2 for c in counts.values()])\n",
    "\n",
    "    base_d = calc_baseline(all_d)\n",
    "    base_dp = calc_baseline(all_dp)\n",
    "    base_tp = calc_baseline(all_tp)\n",
    "    base_pair_avg = (base_d*base_dp + base_d*base_tp + base_tp*base_dp) / 3\n",
    "    base_whole = base_d * base_dp * base_tp\n",
    "    print(f\"Baselines -> Dir: {base_d:.2f}, Depth: {base_dp:.2f}, Type: {base_tp:.2f}, Whole: {base_whole:.4f}\")\n",
    "    \n",
    "    # 3.2 Analysis Loop\n",
    "    chosen_matches = random.sample(unique_matches, min(length_matches, len(unique_matches)))\n",
    "    rl_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    print(f\"Analyzing {len(rl_indices)} points from {len(chosen_matches)} matches...\")\n",
    "\n",
    "    results_p3 = []\n",
    "    with torch.no_grad():\n",
    "        for idx in rl_indices:\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            # Inputs (ADAPTED)\n",
    "            x_d = sample['x_dir'].unsqueeze(0).to(DEVICE) # Direction Input (was x_z)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE) # DEPTH Input (New)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            y_t = sample['y_type']\n",
    "            y_d = sample['y_dir']\n",
    "            y_dp = sample['y_depth']\n",
    "\n",
    "            # Forward (3 Heads) - Model returns (Type, Dir, Depth)\n",
    "            l_type_pred, l_dir_pred, l_depth_pred = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "            \n",
    "            # Prediction assignment (ADAPTED)\n",
    "            preds_t = l_type_pred.argmax(dim=-1).squeeze(0)\n",
    "            preds_d = l_dir_pred.argmax(dim=-1).squeeze(0)\n",
    "            preds_dp = l_depth_pred.argmax(dim=-1).squeeze(0)\n",
    "            \n",
    "            x_seq_cpu = sample['x_type']\n",
    "            \n",
    "            for t_step in range(len(y_t)):\n",
    "                if y_t[t_step] == 0: continue\n",
    "                # Skip serves for rally analysis\n",
    "                if y_t[t_step] == serve_type_id: continue\n",
    "\n",
    "                shot_num = (x_seq_cpu[:t_step+1] != 0).sum().item() + 1\n",
    "                \n",
    "                pt, p_d, pdp = preds_t[t_step].item(), preds_d[t_step].item(), preds_dp[t_step].item()\n",
    "                tt, td, tdp = y_t[t_step].item(), y_d[t_step].item(), y_dp[t_step].item()\n",
    "\n",
    "                ok_t = (pt == tt)\n",
    "                has_dir = (td != 0 and td != unk_dir_id)\n",
    "                has_depth = (tdp != 0 and tdp != unk_depth_id)\n",
    "                ok_d = (p_d == td) if has_dir else False\n",
    "                ok_dp = (pdp == tdp) if has_depth else False\n",
    "\n",
    "                # Logic Split - Single\n",
    "                results_p3.append({'Shot_Number': shot_num, 'Task': 'Type', 'Type': 'Single', 'Accuracy': 1 if ok_t else 0})\n",
    "                if has_dir:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Direction', 'Type': 'Single', 'Accuracy': 1 if ok_d else 0})\n",
    "                if has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Depth', 'Type': 'Single', 'Accuracy': 1 if ok_dp else 0})\n",
    "                \n",
    "                # Logic Split - Pair\n",
    "                if has_dir:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Type', 'Type': 'Pair', 'Accuracy': 1 if (ok_d and ok_t) else 0})\n",
    "                if has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Type + Depth', 'Type': 'Pair', 'Accuracy': 1 if (ok_dp and ok_t) else 0})\n",
    "                if has_dir and has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Dir + Depth', 'Type': 'Pair', 'Accuracy': 1 if (ok_d and ok_dp) else 0})\n",
    "                \n",
    "                # Logic Split - Whole\n",
    "                if has_dir and has_depth:\n",
    "                    results_p3.append({'Shot_Number': shot_num, 'Task': 'Whole Shot', 'Type': 'Whole', 'Accuracy': 1 if (ok_d and ok_dp and ok_t) else 0})\n",
    "\n",
    "    if results_p3:\n",
    "        df = pd.DataFrame(results_p3)\n",
    "        df = df[(df['Shot_Number'] <= 12) & (df['Shot_Number'] >= 2)]\n",
    "        \n",
    "        palette_single = {'Direction': '#1f77b4', 'Depth': '#d62728', 'Type': '#2ca02c'}\n",
    "        palette_pair   = {'Dir + Depth': '#9467bd', 'Dir + Type': '#17becf', 'Type + Depth': '#ff7f0e'}\n",
    "        palette_whole  = {'Whole Shot': '#000000'}\n",
    "\n",
    "        def setup_plot(title, baseline, base_label):\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.ylabel('Accuracy', fontsize=12)\n",
    "            plt.xlabel('Shot Number', fontsize=12)\n",
    "            plt.xticks(np.arange(2, 13, 1))\n",
    "            \n",
    "            ax = plt.gca()\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "            ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.02))\n",
    "            plt.grid(True, which='major', axis='y', linestyle='-', linewidth=0.75, color='grey', alpha=0.6)\n",
    "            plt.grid(True, which='minor', axis='y', linestyle='--', linewidth=0.5, color='grey', alpha=0.3)\n",
    "            plt.ylim(0.0, 1.0)\n",
    "            \n",
    "            if baseline:\n",
    "                plt.axhline(baseline, color='#FF1493', linestyle=':', alpha=0.8, linewidth=2, label=base_label)\n",
    "\n",
    "        # Graph 1: Single\n",
    "        setup_plot('Single Task Accuracy vs. Rally Length', base_tp, f'Random Type ({base_tp:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Single'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_single, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='lower right'); plt.show()\n",
    "        \n",
    "        # Graph 2: Pairwise\n",
    "        setup_plot('Pairwise Accuracy vs. Rally Length', base_pair_avg, f'Random Pair (~{base_pair_avg:.2f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Pair'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes=False, palette=palette_pair, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "        # Graph 3: Whole Shot\n",
    "        setup_plot('Whole Shot Accuracy vs. Rally Length', base_whole, f'Random Whole ({base_whole:.4f})')\n",
    "        sns.lineplot(data=df[df['Type']=='Whole'], x='Shot_Number', y='Accuracy', hue='Task', style='Task', \n",
    "                     markers=True, dashes={'Whole Shot':(2,2)}, palette=palette_whole, linewidth=2.5, errorbar=('ci', 68))\n",
    "        plt.legend(loc='upper right'); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 4: PLAYER FREQUENCY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 4: PLAYER FREQUENCY ({freq_matches} Matches) \\n\" + \"=\"*40)\n",
    "    chosen_matches = random.sample(unique_matches, min(freq_matches, len(unique_matches)))\n",
    "    pf_indices = [idx for mid in chosen_matches for idx in match_map[mid]]\n",
    "    \n",
    "    p_counts = Counter()\n",
    "    p_stats = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in pf_indices:\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            # Inputs (ADAPTED)\n",
    "            x_d = sample['x_dir'].unsqueeze(0).to(DEVICE) # Direction Input (was x_z)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE) # DEPTH Input (New)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            y_t = sample['y_type']\n",
    "            \n",
    "            # Forward (3 Heads) - Model returns (Type, Dir, Depth)\n",
    "            l_type_pred, _, _ = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "            \n",
    "            # Re-assign to match original variable names\n",
    "            l_t = l_type_pred\n",
    "            \n",
    "            preds = l_t.argmax(-1).squeeze(0)\n",
    "            \n",
    "            s_val, r_val = sample['x_s_id'].item(), sample['x_r_id'].item()\n",
    "            x_seq_cpu = sample['x_type']\n",
    "            \n",
    "            for t in range(len(y_t)):\n",
    "                if y_t[t] == 0: continue\n",
    "                if y_t[t] == serve_type_id: continue # Skip serves\n",
    "\n",
    "                hist_len = (x_seq_cpu[:t+1] != 0).sum().item()\n",
    "                actor = s_val if (hist_len + 1) % 2 != 0 else r_val\n",
    "                if actor <= 1: continue\n",
    "                \n",
    "                p_counts[actor] += 1\n",
    "                if actor not in p_stats: p_stats[actor] = {'tot': 0, 'corr': 0}\n",
    "                p_stats[actor]['tot'] += 1\n",
    "                if preds[t].item() == y_t[t].item(): p_stats[actor]['corr'] += 1\n",
    "\n",
    "    pf_data = [{'Freq': p_counts[a], 'Err': (1 - v['corr']/v['tot'])*100} for a, v in p_stats.items() if p_counts[a] > 10]\n",
    "    if pf_data:\n",
    "        df_pf = pd.DataFrame(pf_data)\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.regplot(data=df_pf, x='Freq', y='Err', scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        plt.xscale('log'); plt.title(f\"Type Error vs Frequency (Corr: {df_pf['Freq'].corr(df_pf['Err']):.2f})\"); plt.show()\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PART 5: ERA STABILITY\n",
    "    # ==============================================================================\n",
    "    print(\"\\n\" + \"=\"*40 + f\"\\n PART 5: ERA STABILITY ({era_matches} Matches/Era) \\n\" + \"=\"*40)\n",
    "    eras = {'Pre-2010': [], '2010-2019': [], '2020+': []}\n",
    "    for m_id in unique_matches:\n",
    "        try: y_year = int(str(m_id)[:4])\n",
    "        except: continue\n",
    "        if y_year < 2010: eras['Pre-2010'].append(m_id)\n",
    "        elif y_year < 2020: eras['2010-2019'].append(m_id)\n",
    "        else: eras['2020+'].append(m_id)\n",
    "\n",
    "    era_indices = []\n",
    "    era_labels_list = []\n",
    "    for era_name, m_list in eras.items():\n",
    "        if not m_list: continue\n",
    "        chosen = random.sample(m_list, min(era_matches, len(m_list)))\n",
    "        for m in chosen:\n",
    "            era_indices.extend(match_map[m])\n",
    "            era_labels_list.extend([era_name]*len(match_map[m]))\n",
    "            \n",
    "    era_res = []\n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(era_indices):\n",
    "            sample = dataset[idx]\n",
    "            \n",
    "            # Inputs (ADAPTED)\n",
    "            x_d = sample['x_dir'].unsqueeze(0).to(DEVICE) # Direction Input (was x_z)\n",
    "            x_t = sample['x_type'].unsqueeze(0).to(DEVICE)\n",
    "            x_dp = sample['x_depth'].unsqueeze(0).to(DEVICE) # DEPTH Input (New)\n",
    "            x_c = sample['context'].unsqueeze(0).to(DEVICE)\n",
    "            x_s = sample['x_s_id'].unsqueeze(0).to(DEVICE)\n",
    "            x_r = sample['x_r_id'].unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            y_t = sample['y_type'].to(DEVICE)\n",
    "            y_d = sample['y_dir'].to(DEVICE)\n",
    "            y_dp = sample['y_depth'].to(DEVICE)\n",
    "            \n",
    "            # Forward (3 Heads) - Model returns (Type, Dir, Depth)\n",
    "            l_t, l_d, l_dp = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "            \n",
    "            p_d = l_d.argmax(-1).squeeze(0)\n",
    "            p_t = l_t.argmax(-1).squeeze(0)\n",
    "            p_dp = l_dp.argmax(-1).squeeze(0)\n",
    "            \n",
    "            mask = (y_t > 1) & (y_t != serve_type_id) & (y_d != unk_dir_id) & (y_dp != unk_depth_id)\n",
    "            if mask.sum() > 0:\n",
    "                correct = (p_t[mask] == y_t[mask]) & (p_d[mask] == y_d[mask]) & (p_dp[mask] == y_dp[mask])\n",
    "                acc = correct.float().mean().item()\n",
    "                era_res.append({'Era': era_labels_list[i], 'Whole Shot Acc': acc})\n",
    "    \n",
    "    if era_res:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.barplot(data=pd.DataFrame(era_res), x='Era', y='Whole Shot Acc', palette='viridis', order=['Pre-2010', '2010-2019', '2020+'])\n",
    "        plt.title('Whole Shot Accuracy by Era'); plt.ylim(0, 1); plt.show()\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PART 6: SURFACES (Depth Masked - Full Scan)\n",
    "    # ==============================================================================\n",
    "    batch_size=64\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" PART 6: SURFACE ERROR ANALYSIS (Full Test Set Scan) \")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # --- 1. Efficient Surface Grouping ---\n",
    "    print(f\"Grouping {len(test_indices)} test samples by surface...\")\n",
    "    surface_indices = {'Clay': [], 'Hard': [], 'Grass': []}\n",
    "    \n",
    "    for idx in test_indices:\n",
    "        m_id = dataset.sample_match_ids[idx]\n",
    "        surf = dataset.match_meta.get(m_id, {}).get('surface', 'Hard')\n",
    "        \n",
    "        if 'Clay' in surf:   target = 'Clay'\n",
    "        elif 'Grass' in surf: target = 'Grass'\n",
    "        else:                 target = 'Hard'\n",
    "        \n",
    "        surface_indices[target].append(idx)\n",
    "\n",
    "    # --- 2. Evaluation Loop ---\n",
    "    results = []\n",
    "    inv_depth = {v: k for k, v in dataset.depth_vocab.items()}\n",
    "    inv_dir = {v: k for k, v in dataset.dir_vocab.items()}\n",
    "    \n",
    "    # Identify IDs to strictly target or ignore (7, 8, 9)\n",
    "    valid_depth_ids = [v for k, v in dataset.depth_vocab.items() if k in ['7', '8', '9']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for surf, inds in surface_indices.items():\n",
    "            if not inds: \n",
    "                continue\n",
    "            \n",
    "            print(f\"Scanning {len(inds)} samples for {surf}...\")\n",
    "            \n",
    "            # Create a DataLoader for this surface slice\n",
    "            surf_ds = Subset(dataset, inds)\n",
    "            loader = DataLoader(surf_ds, batch_size=batch_size, shuffle=False) # batch_size is now defined\n",
    "            \n",
    "            for batch in loader:\n",
    "                # Move Inputs\n",
    "                x_t = batch['x_type'].to(DEVICE)\n",
    "                x_d = batch['x_dir'].to(DEVICE)\n",
    "                x_dp = batch['x_depth'].to(DEVICE)\n",
    "                x_s = batch['x_s_id'].to(DEVICE)\n",
    "                x_r = batch['x_r_id'].to(DEVICE)\n",
    "                x_c = batch['context'].to(DEVICE)\n",
    "                \n",
    "                # Move Targets\n",
    "                y_t = batch['y_type'].to(DEVICE)\n",
    "                y_d = batch['y_dir'].to(DEVICE)\n",
    "                y_dp = batch['y_depth'].to(DEVICE)\n",
    "                \n",
    "                l_t, l_d, l_dp = model(x_t, x_d, x_dp, x_s, x_r, x_c)\n",
    "                \n",
    "                p_t = l_t.argmax(dim=-1)\n",
    "                p_d = l_d.argmax(dim=-1)\n",
    "                p_dp = l_dp.argmax(dim=-1)\n",
    "                \n",
    "                # --- Vectorized Error Calculation ---\n",
    "                valid_mask = (y_t != 0)\n",
    "                if valid_mask.sum() == 0: continue\n",
    "\n",
    "                curr_y_t = y_t[valid_mask].cpu().numpy()\n",
    "                curr_p_t = p_t[valid_mask].cpu().numpy()\n",
    "                \n",
    "                curr_y_d = y_d[valid_mask].cpu().numpy()\n",
    "                curr_p_d = p_d[valid_mask].cpu().numpy()\n",
    "                \n",
    "                curr_y_dp = y_dp[valid_mask].cpu().numpy()\n",
    "                curr_p_dp = p_dp[valid_mask].cpu().numpy()\n",
    "\n",
    "                for i in range(len(curr_y_t)):\n",
    "                    # A. Type Error\n",
    "                    t_err = 1.0 if curr_y_t[i] != curr_p_t[i] else 0.0\n",
    "                    \n",
    "                    # B. Direction Error (Strict Masking)\n",
    "                    d_target_str = inv_dir.get(curr_y_d[i], '0')\n",
    "                    if curr_y_d[i] != 0 and d_target_str not in ['<pad>', '<unk>', '0']:\n",
    "                        d_err = 1.0 if curr_y_d[i] != curr_p_d[i] else 0.0\n",
    "                    else:\n",
    "                        d_err = None \n",
    "                        \n",
    "                    # C. Depth Error (VERY Strict Masking: Only 7, 8, 9)\n",
    "                    if curr_y_dp[i] in valid_depth_ids:\n",
    "                        dp_err = 1.0 if curr_y_dp[i] != curr_p_dp[i] else 0.0\n",
    "                    else:\n",
    "                        dp_err = None \n",
    "                        \n",
    "                    # D. Whole Shot\n",
    "                    miss = False\n",
    "                    if t_err == 1.0: miss = True\n",
    "                    if d_err is not None and d_err == 1.0: miss = True\n",
    "                    if dp_err is not None and dp_err == 1.0: miss = True\n",
    "                    \n",
    "                    ws_err = 1.0 if miss else 0.0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Surface': surf,\n",
    "                        'Type Error': t_err,\n",
    "                        'Direction Error': d_err,\n",
    "                        'Depth Error': dp_err,\n",
    "                        'Whole Shot Error': ws_err\n",
    "                    })\n",
    "\n",
    "    # --- 3. Output Generation ---\n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    depth_counts = df.groupby('Surface')['Depth Error'].count()\n",
    "    print(\"\\n[Diagnostics] Valid Depth Samples found per surface:\")\n",
    "    print(depth_counts)\n",
    "    \n",
    "    stats = df.groupby('Surface')[['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error']].mean() * 100\n",
    "    print(\"\\n--- Mean Error Rates (%) [Lower is Better] ---\")\n",
    "    print(stats.round(2))\n",
    "    \n",
    "    df_melt = df.melt(id_vars=['Surface'], \n",
    "                      value_vars=['Type Error', 'Direction Error', 'Depth Error', 'Whole Shot Error'], \n",
    "                      value_name='Error Rate')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df_melt, x='Surface', y='Error Rate', hue='variable', \n",
    "                order=['Clay', 'Hard', 'Grass'], palette='viridis', errorbar=('ci', 95))\n",
    "    \n",
    "    plt.title(f'Error Rates by Component (Full Test Scan, N={len(df)})')\n",
    "    plt.ylabel('Error Rate (0.0 - 1.0)')\n",
    "    plt.legend(title='Metric')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. RUNNER SNIPPET (ADAPTED for HybridRichLSTM) ---\n",
    "if 'dataset' in globals() and 'rich_hybrid_LSTM' in globals():\n",
    "    print(\"Recreating validation/test split for evaluation...\")\n",
    "    seed_everything(42)\n",
    "    \n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.8 * total_len)\n",
    "    val_len = int(0.15 * total_len)\n",
    "    test_len = total_len - train_len - val_len\n",
    "    \n",
    "    gen = torch.Generator().manual_seed(42)\n",
    "    _, _, test_ds = random_split(dataset, [train_len, val_len, test_len], generator=gen)\n",
    "    \n",
    "    test_indices = test_ds.indices\n",
    "    test_loader_eval = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "    run_full_evaluation(\n",
    "        model=rich_hybrid_LSTM, \n",
    "        dataset=dataset, \n",
    "        loader=test_loader_eval, \n",
    "        test_indices=test_indices,\n",
    "        live_samples=5000, \n",
    "        length_matches=2000,\n",
    "        freq_matches=2000\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: 'dataset' or 'hierarchical_model' not found. Run training and ensure the trained model is named 'hierarchical_model' or update the runner snippet.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8890117,
     "sourceId": 13948110,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8890122,
     "sourceId": 13948116,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8911585,
     "sourceId": 13979377,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8918673,
     "sourceId": 13994588,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8918690,
     "sourceId": 13994606,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8925998,
     "sourceId": 14011612,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9011635,
     "sourceId": 14140861,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9011642,
     "sourceId": 14140868,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 535197,
     "modelInstanceId": 520965,
     "sourceId": 686918,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 537997,
     "modelInstanceId": 523983,
     "sourceId": 691115,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 538043,
     "modelInstanceId": 524030,
     "sourceId": 691187,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 538060,
     "modelInstanceId": 524047,
     "sourceId": 691207,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
